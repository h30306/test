{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1643299917966,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "QrdNt6qkHO9t"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1643304720000,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "E5R9zsTzIwc5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "這邊的路徑需要先掛接自己的雲端硬碟，上傳原始資料，然後填入自己雲端的絕對路徑\n",
    "\"\"\"\n",
    "path = r'/content/drive/MyDrive/Philip Wong/Data'\n",
    "image_path = r'/content/drive/MyDrive/Philip Wong/Image'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbjHffT5H2Su"
   },
   "source": [
    "# **Data Cleansing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiPuTFe3IBDs"
   },
   "source": [
    "### OWID-COVID-DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2324,
     "status": "ok",
     "timestamp": 1643299857218,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "drGjwrXFHyAC",
    "outputId": "24aac908-5196-41d7-c51f-7e4fd29bb5c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iso_code  :  0\n",
      "continent  :  0\n",
      "location  :  0\n",
      "date  :  0\n",
      "total_cases  :  7411\n",
      "new_cases  :  7423\n",
      "new_cases_smoothed  :  8381\n",
      "total_deaths  :  18545\n",
      "new_deaths  :  18549\n",
      "new_deaths_smoothed  :  8381\n",
      "total_cases_per_million  :  7411\n",
      "new_cases_per_million  :  7423\n",
      "new_cases_smoothed_per_million  :  8381\n",
      "total_deaths_per_million  :  18545\n",
      "new_deaths_per_million  :  18549\n",
      "new_deaths_smoothed_per_million  :  8381\n",
      "reproduction_rate  :  21035\n",
      "icu_patients  :  112239\n",
      "icu_patients_per_million  :  112239\n",
      "hosp_patients  :  109787\n",
      "hosp_patients_per_million  :  109787\n",
      "weekly_icu_admissions  :  128017\n",
      "weekly_icu_admissions_per_million  :  128017\n",
      "weekly_hosp_admissions  :  127115\n",
      "weekly_hosp_admissions_per_million  :  127115\n",
      "new_tests  :  70985\n",
      "total_tests  :  70891\n",
      "total_tests_per_thousand  :  70891\n",
      "new_tests_per_thousand  :  70985\n",
      "new_tests_smoothed  :  59145\n",
      "new_tests_smoothed_per_thousand  :  59145\n",
      "positive_rate  :  63251\n",
      "tests_per_case  :  63912\n",
      "total_vaccinations  :  96970\n",
      "people_vaccinated  :  98589\n",
      "people_fully_vaccinated  :  101359\n",
      "total_boosters  :  122808\n",
      "new_vaccinations  :  103043\n",
      "new_vaccinations_smoothed  :  68410\n",
      "total_vaccinations_per_hundred  :  96970\n",
      "people_vaccinated_per_hundred  :  98589\n",
      "people_fully_vaccinated_per_hundred  :  101359\n",
      "total_boosters_per_hundred  :  122808\n",
      "new_vaccinations_smoothed_per_million  :  68410\n",
      "new_people_vaccinated_smoothed  :  69512\n",
      "new_people_vaccinated_smoothed_per_hundred  :  69512\n",
      "stringency_index  :  16780\n",
      "population  :  0\n",
      "population_density  :  4724\n",
      "median_age  :  10087\n",
      "aged_65_older  :  11411\n",
      "aged_70_older  :  10741\n",
      "gdp_per_capita  :  9952\n",
      "extreme_poverty  :  50022\n",
      "cardiovasc_death_rate  :  9905\n",
      "diabetes_prevalence  :  5862\n",
      "female_smokers  :  36468\n",
      "male_smokers  :  37818\n",
      "handwashing_facilities  :  69564\n",
      "hospital_beds_per_thousand  :  20893\n",
      "life_expectancy  :  671\n",
      "human_development_index  :  9779\n",
      "excess_mortality_cumulative_absolute  :  124468\n",
      "excess_mortality_cumulative  :  124468\n",
      "excess_mortality  :  124468\n",
      "excess_mortality_cumulative_per_million  :  124468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "df : covid stat data\n",
    "\"\"\"\n",
    "df = pd.read_csv(f\"{path}/owid-covid-data.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df[~df['iso_code'].str.contains('OWID')]   # 排除各地區的彙整資料\n",
    "df.drop(['tests_units'], 1, inplace=True)       # 刪除測試數據單位欄位\n",
    "\n",
    "for x in df.columns:                            # 觀察各變數的空值狀況\n",
    "    print(x, \" : \", df[x].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 42227,
     "status": "ok",
     "timestamp": 1643299905151,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "G3hJPkv9Hyyb"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "依據iso_code的不同，填補該國家的平均值\n",
    "\"\"\"\n",
    "cols = ['reproduction_rate', 'median_age', 'aged_65_older', 'aged_70_older', 'aged_70_older', 'gdp_per_capita', 'extreme_poverty', 'cardiovasc_death_rate',\n",
    "        'diabetes_prevalence', 'female_smokers', 'male_smokers', 'handwashing_facilities', 'hospital_beds_per_thousand', 'life_expectancy',\n",
    "        'human_development_index']\n",
    "\n",
    "def group_value(col):\n",
    "    return df.groupby(['iso_code'])[col].mean().to_dict()\n",
    "\n",
    "for name in cols:\n",
    "    value = group_value(name)\n",
    "    df[name] = df.apply(lambda r:value[r['iso_code']] if pd.isna(r[name]) else r[name], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1643299905152,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "05w2PtJ2H651"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "刪除記錄死亡率過高的欄位，因為空值佔比高達96%(124468/129343)\n",
    "\"\"\"\n",
    "df.drop(['excess_mortality_cumulative_absolute', 'excess_mortality_cumulative', 'excess_mortality', 'excess_mortality_cumulative_per_million'], 1, inplace=True)\n",
    "\n",
    "\"\"\"\n",
    "其餘欄位空值皆補0\n",
    "\"\"\"\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 589,
     "status": "ok",
     "timestamp": 1643299906769,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "Nm-7fatETgMj",
    "outputId": "dc5a067a-2d13-4d6b-dce0-9e0c901846f6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-02010a9b-b7e4-4eca-962a-11536a917cb9\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso_code</th>\n",
       "      <th>continent</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>new_cases_smoothed</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>new_deaths_smoothed</th>\n",
       "      <th>total_cases_per_million</th>\n",
       "      <th>new_cases_per_million</th>\n",
       "      <th>new_cases_smoothed_per_million</th>\n",
       "      <th>total_deaths_per_million</th>\n",
       "      <th>new_deaths_per_million</th>\n",
       "      <th>new_deaths_smoothed_per_million</th>\n",
       "      <th>reproduction_rate</th>\n",
       "      <th>icu_patients</th>\n",
       "      <th>icu_patients_per_million</th>\n",
       "      <th>hosp_patients</th>\n",
       "      <th>hosp_patients_per_million</th>\n",
       "      <th>weekly_icu_admissions</th>\n",
       "      <th>weekly_icu_admissions_per_million</th>\n",
       "      <th>weekly_hosp_admissions</th>\n",
       "      <th>weekly_hosp_admissions_per_million</th>\n",
       "      <th>new_tests</th>\n",
       "      <th>total_tests</th>\n",
       "      <th>total_tests_per_thousand</th>\n",
       "      <th>new_tests_per_thousand</th>\n",
       "      <th>new_tests_smoothed</th>\n",
       "      <th>new_tests_smoothed_per_thousand</th>\n",
       "      <th>positive_rate</th>\n",
       "      <th>tests_per_case</th>\n",
       "      <th>total_vaccinations</th>\n",
       "      <th>people_vaccinated</th>\n",
       "      <th>people_fully_vaccinated</th>\n",
       "      <th>total_boosters</th>\n",
       "      <th>new_vaccinations</th>\n",
       "      <th>new_vaccinations_smoothed</th>\n",
       "      <th>total_vaccinations_per_hundred</th>\n",
       "      <th>people_vaccinated_per_hundred</th>\n",
       "      <th>people_fully_vaccinated_per_hundred</th>\n",
       "      <th>total_boosters_per_hundred</th>\n",
       "      <th>new_vaccinations_smoothed_per_million</th>\n",
       "      <th>new_people_vaccinated_smoothed</th>\n",
       "      <th>new_people_vaccinated_smoothed_per_hundred</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>population</th>\n",
       "      <th>population_density</th>\n",
       "      <th>median_age</th>\n",
       "      <th>aged_65_older</th>\n",
       "      <th>aged_70_older</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>extreme_poverty</th>\n",
       "      <th>cardiovasc_death_rate</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>female_smokers</th>\n",
       "      <th>male_smokers</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>life_expectancy</th>\n",
       "      <th>human_development_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-28</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139143</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>139046.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>688.571</td>\n",
       "      <td>4710.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571</td>\n",
       "      <td>9213.121</td>\n",
       "      <td>0.000</td>\n",
       "      <td>45.624</td>\n",
       "      <td>312.082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8380.0</td>\n",
       "      <td>1496379.0</td>\n",
       "      <td>99.149</td>\n",
       "      <td>0.555</td>\n",
       "      <td>6171.0</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.1116</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6775471.0</td>\n",
       "      <td>3883107.0</td>\n",
       "      <td>2892364.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15882.0</td>\n",
       "      <td>26995.0</td>\n",
       "      <td>44.89</td>\n",
       "      <td>25.73</td>\n",
       "      <td>19.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1789.0</td>\n",
       "      <td>14438.0</td>\n",
       "      <td>0.096</td>\n",
       "      <td>49.07</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139144</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>141601.0</td>\n",
       "      <td>2555.0</td>\n",
       "      <td>996.571</td>\n",
       "      <td>4713.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.857</td>\n",
       "      <td>9382.414</td>\n",
       "      <td>169.293</td>\n",
       "      <td>66.032</td>\n",
       "      <td>312.281</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.057</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11535.0</td>\n",
       "      <td>1507914.0</td>\n",
       "      <td>99.914</td>\n",
       "      <td>0.764</td>\n",
       "      <td>7150.0</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>7.2</td>\n",
       "      <td>6808392.0</td>\n",
       "      <td>3897441.0</td>\n",
       "      <td>2910951.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32921.0</td>\n",
       "      <td>28064.0</td>\n",
       "      <td>45.11</td>\n",
       "      <td>25.82</td>\n",
       "      <td>19.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>14577.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139145</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-08</td>\n",
       "      <td>150628.0</td>\n",
       "      <td>9027.0</td>\n",
       "      <td>2184.429</td>\n",
       "      <td>4720.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.857</td>\n",
       "      <td>9980.539</td>\n",
       "      <td>598.125</td>\n",
       "      <td>144.739</td>\n",
       "      <td>312.745</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.123</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14166.0</td>\n",
       "      <td>1522080.0</td>\n",
       "      <td>100.852</td>\n",
       "      <td>0.939</td>\n",
       "      <td>8296.0</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.2633</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6834846.0</td>\n",
       "      <td>3908712.0</td>\n",
       "      <td>2926134.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26454.0</td>\n",
       "      <td>27772.0</td>\n",
       "      <td>45.29</td>\n",
       "      <td>25.90</td>\n",
       "      <td>19.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1840.0</td>\n",
       "      <td>14116.0</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139146</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-09</td>\n",
       "      <td>155817.0</td>\n",
       "      <td>5189.0</td>\n",
       "      <td>2776.857</td>\n",
       "      <td>4723.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.286</td>\n",
       "      <td>10324.360</td>\n",
       "      <td>343.821</td>\n",
       "      <td>183.993</td>\n",
       "      <td>312.944</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.151</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12899.0</td>\n",
       "      <td>1534979.0</td>\n",
       "      <td>101.707</td>\n",
       "      <td>0.855</td>\n",
       "      <td>8954.0</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.3101</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139147</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>155817.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2625.143</td>\n",
       "      <td>4723.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.143</td>\n",
       "      <td>10324.360</td>\n",
       "      <td>0.000</td>\n",
       "      <td>173.941</td>\n",
       "      <td>312.944</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.142</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129343 rows × 62 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02010a9b-b7e4-4eca-962a-11536a917cb9')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-02010a9b-b7e4-4eca-962a-11536a917cb9 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-02010a9b-b7e4-4eca-962a-11536a917cb9');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       iso_code continent  ... life_expectancy human_development_index\n",
       "0           AFG      Asia  ...           64.83                   0.511\n",
       "1           AFG      Asia  ...           64.83                   0.511\n",
       "2           AFG      Asia  ...           64.83                   0.511\n",
       "3           AFG      Asia  ...           64.83                   0.511\n",
       "4           AFG      Asia  ...           64.83                   0.511\n",
       "...         ...       ...  ...             ...                     ...\n",
       "139143      ZWE    Africa  ...           61.49                   0.571\n",
       "139144      ZWE    Africa  ...           61.49                   0.571\n",
       "139145      ZWE    Africa  ...           61.49                   0.571\n",
       "139146      ZWE    Africa  ...           61.49                   0.571\n",
       "139147      ZWE    Africa  ...           61.49                   0.571\n",
       "\n",
       "[129343 rows x 62 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkUtu4CdIGnL"
   },
   "source": [
    "### SARS-COV-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 678,
     "status": "ok",
     "timestamp": 1643299939261,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "D2c057rIH7or"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sars : 變種病毒統計資料\n",
    "\"\"\"\n",
    "sars = pd.read_csv(f\"{path}/SARS-CoV-2.csv\")\n",
    "\n",
    "\"\"\"\n",
    "拆解年份與第幾週\n",
    "\"\"\"\n",
    "def year_week(d):\n",
    "    d = d.split('-')\n",
    "    y, w = d[0], d[1]\n",
    "    return y, w.zfill(2)\n",
    "\n",
    "sars['year'], sars['week'] = zip(*sars['year_week'].apply(year_week))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1643299955668,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "nagzIeRgT1KS",
    "outputId": "3d365b1f-0917-44b5-e279-25d4a3876efe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-2b6fd721-542e-41ec-9667-1aa03106033b\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>variant</th>\n",
       "      <th>number_detections_variant</th>\n",
       "      <th>year</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>Austria</td>\n",
       "      <td>B.1.617.2</td>\n",
       "      <td>24406</td>\n",
       "      <td>2021</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>Austria</td>\n",
       "      <td>B.1.617.2</td>\n",
       "      <td>19995</td>\n",
       "      <td>2021</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>Austria</td>\n",
       "      <td>B.1.617.2</td>\n",
       "      <td>19238</td>\n",
       "      <td>2021</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16132</th>\n",
       "      <td>France</td>\n",
       "      <td>B.1.617.2</td>\n",
       "      <td>18610</td>\n",
       "      <td>2021</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1689</th>\n",
       "      <td>Austria</td>\n",
       "      <td>B.1.617.2</td>\n",
       "      <td>13985</td>\n",
       "      <td>2021</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18818</th>\n",
       "      <td>Greece</td>\n",
       "      <td>B.1.640</td>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18819</th>\n",
       "      <td>Greece</td>\n",
       "      <td>C.1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18820</th>\n",
       "      <td>Greece</td>\n",
       "      <td>C.37</td>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18821</th>\n",
       "      <td>Greece</td>\n",
       "      <td>P.1</td>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24630</th>\n",
       "      <td>Italy</td>\n",
       "      <td>B.1.621</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47380 rows × 5 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b6fd721-542e-41ec-9667-1aa03106033b')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-2b6fd721-542e-41ec-9667-1aa03106033b button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-2b6fd721-542e-41ec-9667-1aa03106033b');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      location    variant  number_detections_variant  year week\n",
       "1697   Austria  B.1.617.2                      24406  2021   45\n",
       "1694   Austria  B.1.617.2                      19995  2021   44\n",
       "1692   Austria  B.1.617.2                      19238  2021   43\n",
       "16132   France  B.1.617.2                      18610  2021   29\n",
       "1689   Austria  B.1.617.2                      13985  2021   42\n",
       "...        ...        ...                        ...   ...  ...\n",
       "18818   Greece    B.1.640                          0  2021   23\n",
       "18819   Greece      C.1.2                          0  2021   23\n",
       "18820   Greece       C.37                          0  2021   23\n",
       "18821   Greece        P.1                          0  2021   23\n",
       "24630    Italy    B.1.621                          0  2020   48\n",
       "\n",
       "[47380 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1643299944873,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "lTc-lUY9IJek"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "保留需要用到的欄位，並將`country`改名為`location`，以便後續與OWID-COVID-DATA進行合併\n",
    "\"\"\"\n",
    "sars = sars[['country', 'variant', 'number_detections_variant', 'year', 'week']]\n",
    "sars.rename(columns={'country':'location'}, inplace=True)\n",
    "\n",
    "\"\"\"\n",
    "刪除重複資料\n",
    "\"\"\"\n",
    "sars.sort_values(['number_detections_variant'], ascending=False, inplace=True)\n",
    "sars.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "\"\"\"\n",
    "計算各國家各週各變種病毒的案例\n",
    "\"\"\"\n",
    "sars1 = sars.groupby(['location', 'year', 'week', 'variant'])['number_detections_variant'].sum().unstack().reset_index()\n",
    "sars1 = sars1.fillna(0)\n",
    "\n",
    "\"\"\"\n",
    "幫OWID-COVID-DATA新增`year`與`week`共兩個變數，以利合併(left join)\n",
    "\"\"\"\n",
    "df['year'] = df['date'].dt.year\n",
    "df['week'] = df['date'].dt.week\n",
    "df['year'] = df['year'].astype(str)\n",
    "df['week'] = df['week'].apply(lambda x:str(x).zfill(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSpBiQMSIOc0"
   },
   "source": [
    "### 合併OWID-COVID-DATA與SARS-COV-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1643299965713,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "Zc_jCBpqIQRv"
   },
   "outputs": [],
   "source": [
    "df1 = pd.merge(df, sars1, on=['location', 'year', 'week'], how='left')\n",
    "\n",
    "\"\"\"\n",
    "由於變種病毒僅記錄30個國家的資訊，因此其餘國家皆補為0\n",
    "\"\"\"\n",
    "df1 = df1.fillna(0)\n",
    "\n",
    "\"\"\"\n",
    "將`year`與`week`兩個欄位刪除\n",
    "\"\"\"\n",
    "df1.drop(['year', 'week'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 565,
     "status": "ok",
     "timestamp": 1643299971747,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "5AD4Ovn8T5SS",
    "outputId": "75211b85-ec7a-4b18-e5c7-e2cbfa678611"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-97b1bb82-851e-48f6-90e5-8aa6b0a16c6e\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso_code</th>\n",
       "      <th>continent</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>new_cases_smoothed</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>new_deaths_smoothed</th>\n",
       "      <th>total_cases_per_million</th>\n",
       "      <th>new_cases_per_million</th>\n",
       "      <th>new_cases_smoothed_per_million</th>\n",
       "      <th>total_deaths_per_million</th>\n",
       "      <th>new_deaths_per_million</th>\n",
       "      <th>new_deaths_smoothed_per_million</th>\n",
       "      <th>reproduction_rate</th>\n",
       "      <th>icu_patients</th>\n",
       "      <th>icu_patients_per_million</th>\n",
       "      <th>hosp_patients</th>\n",
       "      <th>hosp_patients_per_million</th>\n",
       "      <th>weekly_icu_admissions</th>\n",
       "      <th>weekly_icu_admissions_per_million</th>\n",
       "      <th>weekly_hosp_admissions</th>\n",
       "      <th>weekly_hosp_admissions_per_million</th>\n",
       "      <th>new_tests</th>\n",
       "      <th>total_tests</th>\n",
       "      <th>total_tests_per_thousand</th>\n",
       "      <th>new_tests_per_thousand</th>\n",
       "      <th>new_tests_smoothed</th>\n",
       "      <th>new_tests_smoothed_per_thousand</th>\n",
       "      <th>positive_rate</th>\n",
       "      <th>tests_per_case</th>\n",
       "      <th>total_vaccinations</th>\n",
       "      <th>people_vaccinated</th>\n",
       "      <th>people_fully_vaccinated</th>\n",
       "      <th>total_boosters</th>\n",
       "      <th>new_vaccinations</th>\n",
       "      <th>new_vaccinations_smoothed</th>\n",
       "      <th>total_vaccinations_per_hundred</th>\n",
       "      <th>...</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>population</th>\n",
       "      <th>population_density</th>\n",
       "      <th>median_age</th>\n",
       "      <th>aged_65_older</th>\n",
       "      <th>aged_70_older</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>extreme_poverty</th>\n",
       "      <th>cardiovasc_death_rate</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>female_smokers</th>\n",
       "      <th>male_smokers</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>life_expectancy</th>\n",
       "      <th>human_development_index</th>\n",
       "      <th>AT.1</th>\n",
       "      <th>AY.4.2</th>\n",
       "      <th>B.1.1.529</th>\n",
       "      <th>B.1.1.7</th>\n",
       "      <th>B.1.1.7+E484K</th>\n",
       "      <th>B.1.351</th>\n",
       "      <th>B.1.427/B.1.429</th>\n",
       "      <th>B.1.525</th>\n",
       "      <th>B.1.526</th>\n",
       "      <th>B.1.616</th>\n",
       "      <th>B.1.617</th>\n",
       "      <th>B.1.617.1</th>\n",
       "      <th>B.1.617.2</th>\n",
       "      <th>B.1.617.3</th>\n",
       "      <th>B.1.620</th>\n",
       "      <th>B.1.621</th>\n",
       "      <th>B.1.640</th>\n",
       "      <th>C.1.2</th>\n",
       "      <th>C.37</th>\n",
       "      <th>Other</th>\n",
       "      <th>P.1</th>\n",
       "      <th>P.3</th>\n",
       "      <th>SGTF</th>\n",
       "      <th>UNK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-28</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129338</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>139046.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>688.571</td>\n",
       "      <td>4710.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571</td>\n",
       "      <td>9213.121</td>\n",
       "      <td>0.000</td>\n",
       "      <td>45.624</td>\n",
       "      <td>312.082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8380.0</td>\n",
       "      <td>1496379.0</td>\n",
       "      <td>99.149</td>\n",
       "      <td>0.555</td>\n",
       "      <td>6171.0</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.1116</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6775471.0</td>\n",
       "      <td>3883107.0</td>\n",
       "      <td>2892364.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15882.0</td>\n",
       "      <td>26995.0</td>\n",
       "      <td>44.89</td>\n",
       "      <td>...</td>\n",
       "      <td>49.07</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129339</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>141601.0</td>\n",
       "      <td>2555.0</td>\n",
       "      <td>996.571</td>\n",
       "      <td>4713.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.857</td>\n",
       "      <td>9382.414</td>\n",
       "      <td>169.293</td>\n",
       "      <td>66.032</td>\n",
       "      <td>312.281</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.057</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11535.0</td>\n",
       "      <td>1507914.0</td>\n",
       "      <td>99.914</td>\n",
       "      <td>0.764</td>\n",
       "      <td>7150.0</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>7.2</td>\n",
       "      <td>6808392.0</td>\n",
       "      <td>3897441.0</td>\n",
       "      <td>2910951.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32921.0</td>\n",
       "      <td>28064.0</td>\n",
       "      <td>45.11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129340</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-08</td>\n",
       "      <td>150628.0</td>\n",
       "      <td>9027.0</td>\n",
       "      <td>2184.429</td>\n",
       "      <td>4720.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.857</td>\n",
       "      <td>9980.539</td>\n",
       "      <td>598.125</td>\n",
       "      <td>144.739</td>\n",
       "      <td>312.745</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.123</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14166.0</td>\n",
       "      <td>1522080.0</td>\n",
       "      <td>100.852</td>\n",
       "      <td>0.939</td>\n",
       "      <td>8296.0</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.2633</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6834846.0</td>\n",
       "      <td>3908712.0</td>\n",
       "      <td>2926134.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26454.0</td>\n",
       "      <td>27772.0</td>\n",
       "      <td>45.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129341</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-09</td>\n",
       "      <td>155817.0</td>\n",
       "      <td>5189.0</td>\n",
       "      <td>2776.857</td>\n",
       "      <td>4723.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.286</td>\n",
       "      <td>10324.360</td>\n",
       "      <td>343.821</td>\n",
       "      <td>183.993</td>\n",
       "      <td>312.944</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.151</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12899.0</td>\n",
       "      <td>1534979.0</td>\n",
       "      <td>101.707</td>\n",
       "      <td>0.855</td>\n",
       "      <td>8954.0</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.3101</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129342</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>155817.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2625.143</td>\n",
       "      <td>4723.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.143</td>\n",
       "      <td>10324.360</td>\n",
       "      <td>0.000</td>\n",
       "      <td>173.941</td>\n",
       "      <td>312.944</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.142</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129343 rows × 86 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97b1bb82-851e-48f6-90e5-8aa6b0a16c6e')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-97b1bb82-851e-48f6-90e5-8aa6b0a16c6e button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-97b1bb82-851e-48f6-90e5-8aa6b0a16c6e');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       iso_code continent     location       date  ...  P.1  P.3  SGTF  UNK\n",
       "0           AFG      Asia  Afghanistan 2020-02-24  ...  0.0  0.0   0.0  0.0\n",
       "1           AFG      Asia  Afghanistan 2020-02-25  ...  0.0  0.0   0.0  0.0\n",
       "2           AFG      Asia  Afghanistan 2020-02-26  ...  0.0  0.0   0.0  0.0\n",
       "3           AFG      Asia  Afghanistan 2020-02-27  ...  0.0  0.0   0.0  0.0\n",
       "4           AFG      Asia  Afghanistan 2020-02-28  ...  0.0  0.0   0.0  0.0\n",
       "...         ...       ...          ...        ...  ...  ...  ...   ...  ...\n",
       "129338      ZWE    Africa     Zimbabwe 2021-12-06  ...  0.0  0.0   0.0  0.0\n",
       "129339      ZWE    Africa     Zimbabwe 2021-12-07  ...  0.0  0.0   0.0  0.0\n",
       "129340      ZWE    Africa     Zimbabwe 2021-12-08  ...  0.0  0.0   0.0  0.0\n",
       "129341      ZWE    Africa     Zimbabwe 2021-12-09  ...  0.0  0.0   0.0  0.0\n",
       "129342      ZWE    Africa     Zimbabwe 2021-12-10  ...  0.0  0.0   0.0  0.0\n",
       "\n",
       "[129343 rows x 86 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lbc1xecEIU0x"
   },
   "source": [
    "### RESPONSE-DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 669,
     "status": "ok",
     "timestamp": 1643299992951,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "Ui3kp6p_IVyy",
    "outputId": "d1fd7dc2-c4fc-464d-8030-3c0eceec7ee2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-7fe4f8d2-4546-4fe5-aaa7-49761c1d50e8\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>date_start</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Austria</td>\n",
       "      <td>2020-03-10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austria</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austria</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Austria</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Austria</td>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2020-10-23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2020-11-05</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2020-11-06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2020-12-20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>869 rows × 3 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7fe4f8d2-4546-4fe5-aaa7-49761c1d50e8')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-7fe4f8d2-4546-4fe5-aaa7-49761c1d50e8 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-7fe4f8d2-4546-4fe5-aaa7-49761c1d50e8');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "           location  date_start  count\n",
       "0           Austria  2020-03-10      3\n",
       "1           Austria  2020-03-11      3\n",
       "2           Austria  2020-03-16     12\n",
       "3           Austria  2020-03-31      2\n",
       "4           Austria  2020-04-06      1\n",
       "..              ...         ...    ...\n",
       "864  United Kingdom  2020-09-24      2\n",
       "865  United Kingdom  2020-10-23      1\n",
       "866  United Kingdom  2020-11-05      7\n",
       "867  United Kingdom  2020-11-06      1\n",
       "868  United Kingdom  2020-12-20      1\n",
       "\n",
       "[869 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "rep : 防疫政策數據\n",
    "\"\"\"\n",
    "rep = pd.read_csv(f\"{path}/response_graphs_data_2021-12-09.csv\")\n",
    "\n",
    "\"\"\"\n",
    "將`Country`改名為`location`，以便後續合併\n",
    "\"\"\"\n",
    "rep.rename(columns={'Country':'location'}, inplace=True)\n",
    "\n",
    "\"\"\"\n",
    "確認各國家是否於同一天頒布兩個以上的政策，以此判斷後續資料清洗應以何種方式合併\n",
    "\"\"\"\n",
    "rep.groupby(['location', 'date_start'])['Response_measure'].count().to_frame('count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1643299998934,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "GWO4mrzrIYJa"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "確認有國家一天頒布兩個以上的政策，且並無存在政策只出現過一次的，因此以OneHotEnocoding的方式進行資料合併 (此資料為建模用)\n",
    "首先創建一個表包含2020-01-01至2020-12-10所有國家的空表格\n",
    "\"\"\"\n",
    "temp = [d.strftime('%Y-%m-%d') for d in pd.date_range('2020-01-01', '2021-12-10')]\n",
    "data = []\n",
    "for x in list(set(rep['location'])):\n",
    "    for d in temp:\n",
    "        data.append((x, d))\n",
    "new_rep = pd.DataFrame(data, columns=['location', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 17860,
     "status": "ok",
     "timestamp": 1643300019673,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "m7Y9Yd1WIZ-V"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "為每一種政策新增一個空的欄位，若是開始政策填為1；沒有頒布政策為0；結束政策為-1\n",
    "\"\"\"\n",
    "for x in list(set(rep['Response_measure'])):\n",
    "    new_rep[x] = np.nan\n",
    "\n",
    "for i in range(len(rep)):\n",
    "    l, r, s, e = rep.iloc[i, 0], rep.iloc[i, 1], rep.iloc[i, 2], rep.iloc[i, 3]\n",
    "    new_rep[r][(new_rep['location']==l) & (new_rep['date']==s)] = 1\n",
    "    if not pd.isna(e):\n",
    "        new_rep[r][(new_rep['location'] == l) & (new_rep['date'] == e)] = -1\n",
    "\n",
    "new_rep = new_rep.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 387,
     "status": "ok",
     "timestamp": 1643300020973,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "HV1cMJ-2UCaO",
    "outputId": "0d8989a5-8231-4b39-d1b1-c04f3876b48d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-395cf299-4ace-40be-8c35-0735de1bdb03\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>StayHomeOrder</th>\n",
       "      <th>MasksMandatoryAllSpacesPartial</th>\n",
       "      <th>MasksVoluntaryClosedSpaces</th>\n",
       "      <th>StayHomeGen</th>\n",
       "      <th>QuarantineForInternationalTravellersPartial</th>\n",
       "      <th>ClosPubAnyPartial</th>\n",
       "      <th>NonEssentialShopsPartial</th>\n",
       "      <th>ClosPubAny</th>\n",
       "      <th>MasksMandatoryClosedSpaces</th>\n",
       "      <th>MassGather50Partial</th>\n",
       "      <th>EntertainmentVenues</th>\n",
       "      <th>MassGather50</th>\n",
       "      <th>RegionalStayHomeOrderPartial</th>\n",
       "      <th>OutdoorOver1000</th>\n",
       "      <th>HotelsOtherAccommodationPartial</th>\n",
       "      <th>ClosSec</th>\n",
       "      <th>MasksMandatoryAllSpaces</th>\n",
       "      <th>MassGatherAll</th>\n",
       "      <th>AdaptationOfWorkplace</th>\n",
       "      <th>MasksVoluntaryAllSpaces</th>\n",
       "      <th>ClosureOfPublicTransport</th>\n",
       "      <th>WorkplaceClosuresPartial</th>\n",
       "      <th>MassGatherAllPartial</th>\n",
       "      <th>TeleworkingPartial</th>\n",
       "      <th>ClosureOfPublicTransportPartial</th>\n",
       "      <th>ClosPrim</th>\n",
       "      <th>BanOnAllEventsPartial</th>\n",
       "      <th>ClosHigh</th>\n",
       "      <th>OutdoorOver50</th>\n",
       "      <th>RestaurantsCafesPartial</th>\n",
       "      <th>Teleworking</th>\n",
       "      <th>MasksMandatoryClosedSpacesPartial</th>\n",
       "      <th>StayHomeRiskG</th>\n",
       "      <th>RestaurantsCafes</th>\n",
       "      <th>QuarantineForInternationalTravellers</th>\n",
       "      <th>BanOnAllEvents</th>\n",
       "      <th>PrivateGatheringRestrictions</th>\n",
       "      <th>StayHomeOrderPartial</th>\n",
       "      <th>RegionalStayHomeOrder</th>\n",
       "      <th>MasksVoluntaryAllSpacesPartial</th>\n",
       "      <th>AdaptationOfWorkplacePartial</th>\n",
       "      <th>ClosDaycarePartial</th>\n",
       "      <th>IndoorOver500</th>\n",
       "      <th>SocialCirclePartial</th>\n",
       "      <th>NonEssentialShops</th>\n",
       "      <th>ClosHighPartial</th>\n",
       "      <th>PlaceOfWorship</th>\n",
       "      <th>SocialCircle</th>\n",
       "      <th>StayHomeGenPartial</th>\n",
       "      <th>ClosSecPartial</th>\n",
       "      <th>EntertainmentVenuesPartial</th>\n",
       "      <th>ClosPrimPartial</th>\n",
       "      <th>HotelsOtherAccommodation</th>\n",
       "      <th>MasksVoluntaryClosedSpacesPartial</th>\n",
       "      <th>OutdoorOver100</th>\n",
       "      <th>WorkplaceClosures</th>\n",
       "      <th>IndoorOver1000</th>\n",
       "      <th>StayHomeRiskGPartial</th>\n",
       "      <th>PlaceOfWorshipPartial</th>\n",
       "      <th>IndoorOver50</th>\n",
       "      <th>OutdoorOver500</th>\n",
       "      <th>GymsSportsCentres</th>\n",
       "      <th>ClosDaycare</th>\n",
       "      <th>PrivateGatheringRestrictionsPartial</th>\n",
       "      <th>GymsSportsCentresPartial</th>\n",
       "      <th>IndoorOver100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22715</th>\n",
       "      <td>France</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22716</th>\n",
       "      <td>France</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22717</th>\n",
       "      <td>France</td>\n",
       "      <td>2021-12-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22718</th>\n",
       "      <td>France</td>\n",
       "      <td>2021-12-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22719</th>\n",
       "      <td>France</td>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22720 rows × 68 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-395cf299-4ace-40be-8c35-0735de1bdb03')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-395cf299-4ace-40be-8c35-0735de1bdb03 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-395cf299-4ace-40be-8c35-0735de1bdb03');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      location        date  ...  GymsSportsCentresPartial  IndoorOver100\n",
       "0      Iceland  2020-01-01  ...                       0.0            0.0\n",
       "1      Iceland  2020-01-02  ...                       0.0            0.0\n",
       "2      Iceland  2020-01-03  ...                       0.0            0.0\n",
       "3      Iceland  2020-01-04  ...                       0.0            0.0\n",
       "4      Iceland  2020-01-05  ...                       0.0            0.0\n",
       "...        ...         ...  ...                       ...            ...\n",
       "22715   France  2021-12-06  ...                       0.0            0.0\n",
       "22716   France  2021-12-07  ...                       0.0            0.0\n",
       "22717   France  2021-12-08  ...                       0.0            0.0\n",
       "22718   France  2021-12-09  ...                       0.0            0.0\n",
       "22719   France  2021-12-10  ...                       0.0            0.0\n",
       "\n",
       "[22720 rows x 68 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xfk7ic6_IfyQ"
   },
   "source": [
    "### 將RESPONSE-DATA合併至df1中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnA7fplAIhHo"
   },
   "outputs": [],
   "source": [
    "new_rep['date'] = pd.to_datetime(new_rep['date'])\n",
    "df2 = pd.merge(df1, new_rep, on=['date', 'location'], how='left')\n",
    "df2 = df2.fillna(0)\n",
    "\n",
    "\"\"\"\n",
    "儲存建模資料\n",
    "\"\"\"\n",
    "df2.to_csv(f\"{path}/owid-covid-data-for-model.csv\", index=0, header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1643300090409,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "H4Yz8R2YInlB"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "由於後續將使用視覺化工具 - Tableau 進行資料分析與探勘，但Tableau不適合使用OneHotEncoding的資料進行分析，因此再另外清整一份專門用於資料分析的資料檔\n",
    "\"\"\"\n",
    "df_a = pd.merge(df, sars, on=['location', 'year', 'week'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1643300095233,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "wYAAReHdUXSx",
    "outputId": "cbf1fb84-754f-4e23-9356-6105fc75d22a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-4c26fcaf-e537-4e69-9aa3-001784b1375c\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso_code</th>\n",
       "      <th>continent</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>new_cases_smoothed</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>new_deaths_smoothed</th>\n",
       "      <th>total_cases_per_million</th>\n",
       "      <th>new_cases_per_million</th>\n",
       "      <th>new_cases_smoothed_per_million</th>\n",
       "      <th>total_deaths_per_million</th>\n",
       "      <th>new_deaths_per_million</th>\n",
       "      <th>new_deaths_smoothed_per_million</th>\n",
       "      <th>reproduction_rate</th>\n",
       "      <th>icu_patients</th>\n",
       "      <th>icu_patients_per_million</th>\n",
       "      <th>hosp_patients</th>\n",
       "      <th>hosp_patients_per_million</th>\n",
       "      <th>weekly_icu_admissions</th>\n",
       "      <th>weekly_icu_admissions_per_million</th>\n",
       "      <th>weekly_hosp_admissions</th>\n",
       "      <th>weekly_hosp_admissions_per_million</th>\n",
       "      <th>new_tests</th>\n",
       "      <th>total_tests</th>\n",
       "      <th>total_tests_per_thousand</th>\n",
       "      <th>new_tests_per_thousand</th>\n",
       "      <th>new_tests_smoothed</th>\n",
       "      <th>new_tests_smoothed_per_thousand</th>\n",
       "      <th>positive_rate</th>\n",
       "      <th>tests_per_case</th>\n",
       "      <th>total_vaccinations</th>\n",
       "      <th>people_vaccinated</th>\n",
       "      <th>people_fully_vaccinated</th>\n",
       "      <th>total_boosters</th>\n",
       "      <th>new_vaccinations</th>\n",
       "      <th>new_vaccinations_smoothed</th>\n",
       "      <th>total_vaccinations_per_hundred</th>\n",
       "      <th>people_vaccinated_per_hundred</th>\n",
       "      <th>people_fully_vaccinated_per_hundred</th>\n",
       "      <th>total_boosters_per_hundred</th>\n",
       "      <th>new_vaccinations_smoothed_per_million</th>\n",
       "      <th>new_people_vaccinated_smoothed</th>\n",
       "      <th>new_people_vaccinated_smoothed_per_hundred</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>population</th>\n",
       "      <th>population_density</th>\n",
       "      <th>median_age</th>\n",
       "      <th>aged_65_older</th>\n",
       "      <th>aged_70_older</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>extreme_poverty</th>\n",
       "      <th>cardiovasc_death_rate</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>female_smokers</th>\n",
       "      <th>male_smokers</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>life_expectancy</th>\n",
       "      <th>human_development_index</th>\n",
       "      <th>year</th>\n",
       "      <th>week</th>\n",
       "      <th>variant</th>\n",
       "      <th>number_detections_variant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>2020</td>\n",
       "      <td>09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>2020</td>\n",
       "      <td>09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>2020</td>\n",
       "      <td>09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>2020</td>\n",
       "      <td>09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-28</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>2020</td>\n",
       "      <td>09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445872</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>139046.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>688.571</td>\n",
       "      <td>4710.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571</td>\n",
       "      <td>9213.121</td>\n",
       "      <td>0.000</td>\n",
       "      <td>45.624</td>\n",
       "      <td>312.082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8380.0</td>\n",
       "      <td>1496379.0</td>\n",
       "      <td>99.149</td>\n",
       "      <td>0.555</td>\n",
       "      <td>6171.0</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.1116</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6775471.0</td>\n",
       "      <td>3883107.0</td>\n",
       "      <td>2892364.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15882.0</td>\n",
       "      <td>26995.0</td>\n",
       "      <td>44.89</td>\n",
       "      <td>25.73</td>\n",
       "      <td>19.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1789.0</td>\n",
       "      <td>14438.0</td>\n",
       "      <td>0.096</td>\n",
       "      <td>49.07</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>2021</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445873</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>141601.0</td>\n",
       "      <td>2555.0</td>\n",
       "      <td>996.571</td>\n",
       "      <td>4713.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.857</td>\n",
       "      <td>9382.414</td>\n",
       "      <td>169.293</td>\n",
       "      <td>66.032</td>\n",
       "      <td>312.281</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.057</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11535.0</td>\n",
       "      <td>1507914.0</td>\n",
       "      <td>99.914</td>\n",
       "      <td>0.764</td>\n",
       "      <td>7150.0</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>7.2</td>\n",
       "      <td>6808392.0</td>\n",
       "      <td>3897441.0</td>\n",
       "      <td>2910951.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32921.0</td>\n",
       "      <td>28064.0</td>\n",
       "      <td>45.11</td>\n",
       "      <td>25.82</td>\n",
       "      <td>19.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>14577.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>2021</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445874</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-08</td>\n",
       "      <td>150628.0</td>\n",
       "      <td>9027.0</td>\n",
       "      <td>2184.429</td>\n",
       "      <td>4720.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.857</td>\n",
       "      <td>9980.539</td>\n",
       "      <td>598.125</td>\n",
       "      <td>144.739</td>\n",
       "      <td>312.745</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.123</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14166.0</td>\n",
       "      <td>1522080.0</td>\n",
       "      <td>100.852</td>\n",
       "      <td>0.939</td>\n",
       "      <td>8296.0</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.2633</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6834846.0</td>\n",
       "      <td>3908712.0</td>\n",
       "      <td>2926134.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26454.0</td>\n",
       "      <td>27772.0</td>\n",
       "      <td>45.29</td>\n",
       "      <td>25.90</td>\n",
       "      <td>19.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1840.0</td>\n",
       "      <td>14116.0</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>2021</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445875</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-09</td>\n",
       "      <td>155817.0</td>\n",
       "      <td>5189.0</td>\n",
       "      <td>2776.857</td>\n",
       "      <td>4723.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.286</td>\n",
       "      <td>10324.360</td>\n",
       "      <td>343.821</td>\n",
       "      <td>183.993</td>\n",
       "      <td>312.944</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.151</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12899.0</td>\n",
       "      <td>1534979.0</td>\n",
       "      <td>101.707</td>\n",
       "      <td>0.855</td>\n",
       "      <td>8954.0</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.3101</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>2021</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445876</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>155817.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2625.143</td>\n",
       "      <td>4723.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.143</td>\n",
       "      <td>10324.360</td>\n",
       "      <td>0.000</td>\n",
       "      <td>173.941</td>\n",
       "      <td>312.944</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.142</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>2021</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>445877 rows × 66 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c26fcaf-e537-4e69-9aa3-001784b1375c')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-4c26fcaf-e537-4e69-9aa3-001784b1375c button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-4c26fcaf-e537-4e69-9aa3-001784b1375c');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       iso_code continent     location  ... week  variant  number_detections_variant\n",
       "0           AFG      Asia  Afghanistan  ...   09      NaN                        NaN\n",
       "1           AFG      Asia  Afghanistan  ...   09      NaN                        NaN\n",
       "2           AFG      Asia  Afghanistan  ...   09      NaN                        NaN\n",
       "3           AFG      Asia  Afghanistan  ...   09      NaN                        NaN\n",
       "4           AFG      Asia  Afghanistan  ...   09      NaN                        NaN\n",
       "...         ...       ...          ...  ...  ...      ...                        ...\n",
       "445872      ZWE    Africa     Zimbabwe  ...   49      NaN                        NaN\n",
       "445873      ZWE    Africa     Zimbabwe  ...   49      NaN                        NaN\n",
       "445874      ZWE    Africa     Zimbabwe  ...   49      NaN                        NaN\n",
       "445875      ZWE    Africa     Zimbabwe  ...   49      NaN                        NaN\n",
       "445876      ZWE    Africa     Zimbabwe  ...   49      NaN                        NaN\n",
       "\n",
       "[445877 rows x 66 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJfcgex7Ipo7"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "合併頒布政策開始的時間\n",
    "\"\"\"\n",
    "rep_1 = rep[['location', 'Response_measure', 'date_start']].copy()\n",
    "rep_1.rename(columns={'date_start':'date', 'Response_measure':'response_measure_start'}, inplace=True)\n",
    "rep_1['date'] = pd.to_datetime(rep_1['date'])\n",
    "\n",
    "df_b = pd.merge(df_a, rep_1, on=['date', 'location'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1643300116971,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "Vvk6M4EUUcvz",
    "outputId": "bf394cc5-3cdd-4179-9291-12c54a2e93d4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-87a82801-1f10-4b47-99ec-6c7298527841\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso_code</th>\n",
       "      <th>continent</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>new_cases_smoothed</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>new_deaths_smoothed</th>\n",
       "      <th>total_cases_per_million</th>\n",
       "      <th>new_cases_per_million</th>\n",
       "      <th>new_cases_smoothed_per_million</th>\n",
       "      <th>total_deaths_per_million</th>\n",
       "      <th>new_deaths_per_million</th>\n",
       "      <th>new_deaths_smoothed_per_million</th>\n",
       "      <th>reproduction_rate</th>\n",
       "      <th>icu_patients</th>\n",
       "      <th>icu_patients_per_million</th>\n",
       "      <th>hosp_patients</th>\n",
       "      <th>hosp_patients_per_million</th>\n",
       "      <th>weekly_icu_admissions</th>\n",
       "      <th>weekly_icu_admissions_per_million</th>\n",
       "      <th>weekly_hosp_admissions</th>\n",
       "      <th>weekly_hosp_admissions_per_million</th>\n",
       "      <th>new_tests</th>\n",
       "      <th>total_tests</th>\n",
       "      <th>total_tests_per_thousand</th>\n",
       "      <th>new_tests_per_thousand</th>\n",
       "      <th>new_tests_smoothed</th>\n",
       "      <th>new_tests_smoothed_per_thousand</th>\n",
       "      <th>positive_rate</th>\n",
       "      <th>tests_per_case</th>\n",
       "      <th>total_vaccinations</th>\n",
       "      <th>people_vaccinated</th>\n",
       "      <th>people_fully_vaccinated</th>\n",
       "      <th>total_boosters</th>\n",
       "      <th>new_vaccinations</th>\n",
       "      <th>new_vaccinations_smoothed</th>\n",
       "      <th>total_vaccinations_per_hundred</th>\n",
       "      <th>people_vaccinated_per_hundred</th>\n",
       "      <th>people_fully_vaccinated_per_hundred</th>\n",
       "      <th>total_boosters_per_hundred</th>\n",
       "      <th>new_vaccinations_smoothed_per_million</th>\n",
       "      <th>new_people_vaccinated_smoothed</th>\n",
       "      <th>new_people_vaccinated_smoothed_per_hundred</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>population</th>\n",
       "      <th>population_density</th>\n",
       "      <th>median_age</th>\n",
       "      <th>aged_65_older</th>\n",
       "      <th>aged_70_older</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>extreme_poverty</th>\n",
       "      <th>cardiovasc_death_rate</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>female_smokers</th>\n",
       "      <th>male_smokers</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>life_expectancy</th>\n",
       "      <th>human_development_index</th>\n",
       "      <th>year</th>\n",
       "      <th>week</th>\n",
       "      <th>variant</th>\n",
       "      <th>number_detections_variant</th>\n",
       "      <th>response_measure_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>2020</td>\n",
       "      <td>09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>2020</td>\n",
       "      <td>09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>2020</td>\n",
       "      <td>09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>2020</td>\n",
       "      <td>09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-28</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>2020</td>\n",
       "      <td>09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460329</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>139046.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>688.571</td>\n",
       "      <td>4710.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571</td>\n",
       "      <td>9213.121</td>\n",
       "      <td>0.000</td>\n",
       "      <td>45.624</td>\n",
       "      <td>312.082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8380.0</td>\n",
       "      <td>1496379.0</td>\n",
       "      <td>99.149</td>\n",
       "      <td>0.555</td>\n",
       "      <td>6171.0</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.1116</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6775471.0</td>\n",
       "      <td>3883107.0</td>\n",
       "      <td>2892364.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15882.0</td>\n",
       "      <td>26995.0</td>\n",
       "      <td>44.89</td>\n",
       "      <td>25.73</td>\n",
       "      <td>19.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1789.0</td>\n",
       "      <td>14438.0</td>\n",
       "      <td>0.096</td>\n",
       "      <td>49.07</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>2021</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460330</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>141601.0</td>\n",
       "      <td>2555.0</td>\n",
       "      <td>996.571</td>\n",
       "      <td>4713.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.857</td>\n",
       "      <td>9382.414</td>\n",
       "      <td>169.293</td>\n",
       "      <td>66.032</td>\n",
       "      <td>312.281</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.057</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11535.0</td>\n",
       "      <td>1507914.0</td>\n",
       "      <td>99.914</td>\n",
       "      <td>0.764</td>\n",
       "      <td>7150.0</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>7.2</td>\n",
       "      <td>6808392.0</td>\n",
       "      <td>3897441.0</td>\n",
       "      <td>2910951.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32921.0</td>\n",
       "      <td>28064.0</td>\n",
       "      <td>45.11</td>\n",
       "      <td>25.82</td>\n",
       "      <td>19.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>14577.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>2021</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460331</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-08</td>\n",
       "      <td>150628.0</td>\n",
       "      <td>9027.0</td>\n",
       "      <td>2184.429</td>\n",
       "      <td>4720.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.857</td>\n",
       "      <td>9980.539</td>\n",
       "      <td>598.125</td>\n",
       "      <td>144.739</td>\n",
       "      <td>312.745</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.123</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14166.0</td>\n",
       "      <td>1522080.0</td>\n",
       "      <td>100.852</td>\n",
       "      <td>0.939</td>\n",
       "      <td>8296.0</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.2633</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6834846.0</td>\n",
       "      <td>3908712.0</td>\n",
       "      <td>2926134.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26454.0</td>\n",
       "      <td>27772.0</td>\n",
       "      <td>45.29</td>\n",
       "      <td>25.90</td>\n",
       "      <td>19.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1840.0</td>\n",
       "      <td>14116.0</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>2021</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460332</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-09</td>\n",
       "      <td>155817.0</td>\n",
       "      <td>5189.0</td>\n",
       "      <td>2776.857</td>\n",
       "      <td>4723.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.286</td>\n",
       "      <td>10324.360</td>\n",
       "      <td>343.821</td>\n",
       "      <td>183.993</td>\n",
       "      <td>312.944</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.151</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12899.0</td>\n",
       "      <td>1534979.0</td>\n",
       "      <td>101.707</td>\n",
       "      <td>0.855</td>\n",
       "      <td>8954.0</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.3101</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>2021</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460333</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>155817.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2625.143</td>\n",
       "      <td>4723.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.143</td>\n",
       "      <td>10324.360</td>\n",
       "      <td>0.000</td>\n",
       "      <td>173.941</td>\n",
       "      <td>312.944</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.142</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>2021</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>460334 rows × 67 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87a82801-1f10-4b47-99ec-6c7298527841')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-87a82801-1f10-4b47-99ec-6c7298527841 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-87a82801-1f10-4b47-99ec-6c7298527841');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       iso_code continent  ... number_detections_variant response_measure_start\n",
       "0           AFG      Asia  ...                       NaN                    NaN\n",
       "1           AFG      Asia  ...                       NaN                    NaN\n",
       "2           AFG      Asia  ...                       NaN                    NaN\n",
       "3           AFG      Asia  ...                       NaN                    NaN\n",
       "4           AFG      Asia  ...                       NaN                    NaN\n",
       "...         ...       ...  ...                       ...                    ...\n",
       "460329      ZWE    Africa  ...                       NaN                    NaN\n",
       "460330      ZWE    Africa  ...                       NaN                    NaN\n",
       "460331      ZWE    Africa  ...                       NaN                    NaN\n",
       "460332      ZWE    Africa  ...                       NaN                    NaN\n",
       "460333      ZWE    Africa  ...                       NaN                    NaN\n",
       "\n",
       "[460334 rows x 67 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNZvcZ6LUgg4"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "合併頒布政策結束的時間\n",
    "\"\"\"\n",
    "rep_2 = rep[['location', 'Response_measure', 'date_end']].copy()\n",
    "rep_2.rename(columns={'date_end':'date', 'Response_measure':'response_measure_end'}, inplace=True)\n",
    "rep_2['date'] = pd.to_datetime(rep_2['date'])\n",
    "\n",
    "df_c = pd.merge(df_b, rep_2, on=['date', 'location'], how='left')\n",
    "\n",
    "\"\"\"\n",
    "儲存資料分析使用資料\n",
    "\"\"\"\n",
    "df_c.to_csv(f\"{path}/owid-covid-data-for-mining.csv\", index=0, header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1643300152567,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "5725f27NUkeY",
    "outputId": "c0e94533-d39a-4e63-f358-0945d02425b2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-42199970-76f4-4967-b261-b409c2a8111a\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso_code</th>\n",
       "      <th>continent</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>new_cases_smoothed</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>new_deaths_smoothed</th>\n",
       "      <th>total_cases_per_million</th>\n",
       "      <th>new_cases_per_million</th>\n",
       "      <th>new_cases_smoothed_per_million</th>\n",
       "      <th>total_deaths_per_million</th>\n",
       "      <th>new_deaths_per_million</th>\n",
       "      <th>new_deaths_smoothed_per_million</th>\n",
       "      <th>reproduction_rate</th>\n",
       "      <th>icu_patients</th>\n",
       "      <th>icu_patients_per_million</th>\n",
       "      <th>hosp_patients</th>\n",
       "      <th>hosp_patients_per_million</th>\n",
       "      <th>weekly_icu_admissions</th>\n",
       "      <th>weekly_icu_admissions_per_million</th>\n",
       "      <th>weekly_hosp_admissions</th>\n",
       "      <th>weekly_hosp_admissions_per_million</th>\n",
       "      <th>new_tests</th>\n",
       "      <th>total_tests</th>\n",
       "      <th>total_tests_per_thousand</th>\n",
       "      <th>new_tests_per_thousand</th>\n",
       "      <th>new_tests_smoothed</th>\n",
       "      <th>new_tests_smoothed_per_thousand</th>\n",
       "      <th>positive_rate</th>\n",
       "      <th>tests_per_case</th>\n",
       "      <th>total_vaccinations</th>\n",
       "      <th>people_vaccinated</th>\n",
       "      <th>people_fully_vaccinated</th>\n",
       "      <th>total_boosters</th>\n",
       "      <th>new_vaccinations</th>\n",
       "      <th>new_vaccinations_smoothed</th>\n",
       "      <th>total_vaccinations_per_hundred</th>\n",
       "      <th>people_vaccinated_per_hundred</th>\n",
       "      <th>people_fully_vaccinated_per_hundred</th>\n",
       "      <th>total_boosters_per_hundred</th>\n",
       "      <th>new_vaccinations_smoothed_per_million</th>\n",
       "      <th>new_people_vaccinated_smoothed</th>\n",
       "      <th>new_people_vaccinated_smoothed_per_hundred</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>population</th>\n",
       "      <th>population_density</th>\n",
       "      <th>median_age</th>\n",
       "      <th>aged_65_older</th>\n",
       "      <th>aged_70_older</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>extreme_poverty</th>\n",
       "      <th>cardiovasc_death_rate</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>female_smokers</th>\n",
       "      <th>male_smokers</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>life_expectancy</th>\n",
       "      <th>human_development_index</th>\n",
       "      <th>year</th>\n",
       "      <th>week</th>\n",
       "      <th>variant</th>\n",
       "      <th>number_detections_variant</th>\n",
       "      <th>response_measure_start</th>\n",
       "      <th>response_measure_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>2020</td>\n",
       "      <td>09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>2020</td>\n",
       "      <td>09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>2020</td>\n",
       "      <td>09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>2020</td>\n",
       "      <td>09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-28</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.045981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.33</td>\n",
       "      <td>39835428.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>2020</td>\n",
       "      <td>09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474339</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>139046.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>688.571</td>\n",
       "      <td>4710.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571</td>\n",
       "      <td>9213.121</td>\n",
       "      <td>0.000</td>\n",
       "      <td>45.624</td>\n",
       "      <td>312.082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8380.0</td>\n",
       "      <td>1496379.0</td>\n",
       "      <td>99.149</td>\n",
       "      <td>0.555</td>\n",
       "      <td>6171.0</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.1116</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6775471.0</td>\n",
       "      <td>3883107.0</td>\n",
       "      <td>2892364.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15882.0</td>\n",
       "      <td>26995.0</td>\n",
       "      <td>44.89</td>\n",
       "      <td>25.73</td>\n",
       "      <td>19.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1789.0</td>\n",
       "      <td>14438.0</td>\n",
       "      <td>0.096</td>\n",
       "      <td>49.07</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>2021</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474340</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>141601.0</td>\n",
       "      <td>2555.0</td>\n",
       "      <td>996.571</td>\n",
       "      <td>4713.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.857</td>\n",
       "      <td>9382.414</td>\n",
       "      <td>169.293</td>\n",
       "      <td>66.032</td>\n",
       "      <td>312.281</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.057</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11535.0</td>\n",
       "      <td>1507914.0</td>\n",
       "      <td>99.914</td>\n",
       "      <td>0.764</td>\n",
       "      <td>7150.0</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>7.2</td>\n",
       "      <td>6808392.0</td>\n",
       "      <td>3897441.0</td>\n",
       "      <td>2910951.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32921.0</td>\n",
       "      <td>28064.0</td>\n",
       "      <td>45.11</td>\n",
       "      <td>25.82</td>\n",
       "      <td>19.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>14577.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>2021</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474341</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-08</td>\n",
       "      <td>150628.0</td>\n",
       "      <td>9027.0</td>\n",
       "      <td>2184.429</td>\n",
       "      <td>4720.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.857</td>\n",
       "      <td>9980.539</td>\n",
       "      <td>598.125</td>\n",
       "      <td>144.739</td>\n",
       "      <td>312.745</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.123</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14166.0</td>\n",
       "      <td>1522080.0</td>\n",
       "      <td>100.852</td>\n",
       "      <td>0.939</td>\n",
       "      <td>8296.0</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.2633</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6834846.0</td>\n",
       "      <td>3908712.0</td>\n",
       "      <td>2926134.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26454.0</td>\n",
       "      <td>27772.0</td>\n",
       "      <td>45.29</td>\n",
       "      <td>25.90</td>\n",
       "      <td>19.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1840.0</td>\n",
       "      <td>14116.0</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>2021</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474342</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-09</td>\n",
       "      <td>155817.0</td>\n",
       "      <td>5189.0</td>\n",
       "      <td>2776.857</td>\n",
       "      <td>4723.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.286</td>\n",
       "      <td>10324.360</td>\n",
       "      <td>343.821</td>\n",
       "      <td>183.993</td>\n",
       "      <td>312.944</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.151</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12899.0</td>\n",
       "      <td>1534979.0</td>\n",
       "      <td>101.707</td>\n",
       "      <td>0.855</td>\n",
       "      <td>8954.0</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.3101</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>2021</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474343</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>155817.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2625.143</td>\n",
       "      <td>4723.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.143</td>\n",
       "      <td>10324.360</td>\n",
       "      <td>0.000</td>\n",
       "      <td>173.941</td>\n",
       "      <td>312.944</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.142</td>\n",
       "      <td>1.104255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15092171.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571</td>\n",
       "      <td>2021</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>474344 rows × 68 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42199970-76f4-4967-b261-b409c2a8111a')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-42199970-76f4-4967-b261-b409c2a8111a button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-42199970-76f4-4967-b261-b409c2a8111a');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       iso_code continent  ... response_measure_start response_measure_end\n",
       "0           AFG      Asia  ...                    NaN                  NaN\n",
       "1           AFG      Asia  ...                    NaN                  NaN\n",
       "2           AFG      Asia  ...                    NaN                  NaN\n",
       "3           AFG      Asia  ...                    NaN                  NaN\n",
       "4           AFG      Asia  ...                    NaN                  NaN\n",
       "...         ...       ...  ...                    ...                  ...\n",
       "474339      ZWE    Africa  ...                    NaN                  NaN\n",
       "474340      ZWE    Africa  ...                    NaN                  NaN\n",
       "474341      ZWE    Africa  ...                    NaN                  NaN\n",
       "474342      ZWE    Africa  ...                    NaN                  NaN\n",
       "474343      ZWE    Africa  ...                    NaN                  NaN\n",
       "\n",
       "[474344 rows x 68 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRoAu1kiJJ1M"
   },
   "source": [
    "# **Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 809,
     "status": "ok",
     "timestamp": 1643301459497,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "IFgUBjcFJQhi",
    "outputId": "abfb1f14-dfea-4ab8-ad7a-adc300560acf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 3562,
     "status": "ok",
     "timestamp": 1643300178326,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "VEgcN1NPJTaT"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "將各國家每日的新增案例數變更為隔日的新增案例數\n",
    "\"\"\"\n",
    "df = pd.read_csv(f\"{path}/owid-covid-data-for-model.csv\")\n",
    "df.sort_values(['location', 'date'], ascending=False, inplace=True)\n",
    "df['ind'] = df.groupby(['location'])['date'].cumcount()\n",
    "df['new_cases'] = df['new_cases'].shift(1)\n",
    "df = df[df['ind']!=0].drop(['ind'], 1)\n",
    "\n",
    "\"\"\"\n",
    "選取美國、英國、土耳其、德國、加拿大、南非、韓國、日本共八個國家，計算各國各數值型變數與隔日新增案例數的相關係數\n",
    "\"\"\"\n",
    "location = ['United States', 'United Kingdom', 'Turkey', 'Germany', 'Canada', 'South Africa', 'South Korea', 'Japan']\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "for l in location:\n",
    "    temp = df[df['location']==l].corr()['new_cases'].reset_index()\n",
    "    temp['location'] = l\n",
    "    temp.columns = ['columns', 'new_cases', 'location']\n",
    "    df1 = pd.concat([df1, temp], 0)\n",
    "\n",
    "df1.to_excel(f'{path}/corr.xlsx', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1643300223896,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "VepQ-lkzUtSi",
    "outputId": "c512c7b5-9e72-4194-bfb1-a10a672e2146"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-5870e5bb-8b5d-495b-960e-cc00f4fa6d59\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>total_cases</td>\n",
       "      <td>0.316994</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new_cases</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new_cases_smoothed</td>\n",
       "      <td>0.899618</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>total_deaths</td>\n",
       "      <td>0.306254</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new_deaths</td>\n",
       "      <td>0.601206</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>BanOnAllEvents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>BanOnAllEventsPartial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>OutdoorOver500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>PlaceOfWorship</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>GymsSportsCentres</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1184 rows × 3 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5870e5bb-8b5d-495b-960e-cc00f4fa6d59')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-5870e5bb-8b5d-495b-960e-cc00f4fa6d59 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-5870e5bb-8b5d-495b-960e-cc00f4fa6d59');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                   columns  new_cases       location\n",
       "0              total_cases   0.316994  United States\n",
       "1                new_cases   1.000000  United States\n",
       "2       new_cases_smoothed   0.899618  United States\n",
       "3             total_deaths   0.306254  United States\n",
       "4               new_deaths   0.601206  United States\n",
       "..                     ...        ...            ...\n",
       "143         BanOnAllEvents        NaN          Japan\n",
       "144  BanOnAllEventsPartial        NaN          Japan\n",
       "145         OutdoorOver500        NaN          Japan\n",
       "146         PlaceOfWorship        NaN          Japan\n",
       "147      GymsSportsCentres        NaN          Japan\n",
       "\n",
       "[1184 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "隔日新增案例數與其他數值型變數的相關係數表\n",
    "\"\"\"\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 2756,
     "status": "ok",
     "timestamp": 1643300236146,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "jg6gBy40JVnZ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "將各國家每日的新增死亡數變更為隔日的新增死亡數\n",
    "\"\"\"\n",
    "df = pd.read_csv(f\"{path}/owid-covid-data-for-model.csv\")\n",
    "df.sort_values(['location', 'date'], ascending=False, inplace=True)\n",
    "df['ind'] = df.groupby(['location'])['date'].cumcount()\n",
    "df['new_deaths'] = df['new_deaths'].shift(1)\n",
    "df = df[df['ind']!=0].drop(['ind'], 1)\n",
    "\n",
    "\"\"\"\n",
    "選取美國、英國、土耳其、德國、加拿大、南非、韓國、日本共八個國家，計算各國各數值型變數與隔日新增死亡數的相關係數\n",
    "\"\"\"\n",
    "location = ['United States', 'United Kingdom', 'Turkey', 'Germany', 'Canada', 'South Africa', 'South Korea', 'Japan']\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "for l in location:\n",
    "    temp = df[df['location']==l].corr()['new_deaths'].reset_index()\n",
    "    temp['location'] = l\n",
    "    temp.columns = ['columns', 'new_deaths', 'location']\n",
    "    df1 = pd.concat([df1, temp], 0)\n",
    "\n",
    "df1.to_excel(f'{path}/corr_deaths.xlsx', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1643300244157,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "-OYiQsVcU6vc",
    "outputId": "b4c5a88b-69ba-4d03-f215-fd3b20bac303"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-cf27770e-cad0-4f30-919c-27d9d727814f\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>total_cases</td>\n",
       "      <td>0.156649</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new_cases</td>\n",
       "      <td>0.617549</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new_cases_smoothed</td>\n",
       "      <td>0.663449</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>total_deaths</td>\n",
       "      <td>0.137513</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new_deaths</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>BanOnAllEvents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>BanOnAllEventsPartial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>OutdoorOver500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>PlaceOfWorship</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>GymsSportsCentres</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1184 rows × 3 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf27770e-cad0-4f30-919c-27d9d727814f')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-cf27770e-cad0-4f30-919c-27d9d727814f button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-cf27770e-cad0-4f30-919c-27d9d727814f');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                   columns  new_deaths       location\n",
       "0              total_cases    0.156649  United States\n",
       "1                new_cases    0.617549  United States\n",
       "2       new_cases_smoothed    0.663449  United States\n",
       "3             total_deaths    0.137513  United States\n",
       "4               new_deaths    1.000000  United States\n",
       "..                     ...         ...            ...\n",
       "143         BanOnAllEvents         NaN          Japan\n",
       "144  BanOnAllEventsPartial         NaN          Japan\n",
       "145         OutdoorOver500         NaN          Japan\n",
       "146         PlaceOfWorship         NaN          Japan\n",
       "147      GymsSportsCentres         NaN          Japan\n",
       "\n",
       "[1184 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "隔日新增死亡案例數與其他數值型變數的相關係數表\n",
    "\"\"\"\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 3005,
     "status": "ok",
     "timestamp": 1643300254851,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "QVVZPrTKJVpv"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "將各國家每日的新增重症案例數變更為隔日的新增重症人數\n",
    "\"\"\"\n",
    "df = pd.read_csv(f\"{path}/owid-covid-data-for-model.csv\")\n",
    "df.sort_values(['location', 'date'], ascending=False, inplace=True)\n",
    "df['ind'] = df.groupby(['location'])['date'].cumcount()\n",
    "df['icu_patients_1'] = df['icu_patients'].shift(1)\n",
    "df['icu_patients'] = df['icu_patients_1'] - df['icu_patients']\n",
    "df.drop(['icu_patients_1'], 1, inplace=True)\n",
    "df = df[df['ind']!=0].drop(['ind'], 1)\n",
    "\n",
    "\"\"\"\n",
    "選取美國、英國、土耳其、德國、加拿大、南非、韓國、日本共八個國家，計算各國各數值型變數與隔日新增重症人數的相關係數\n",
    "\"\"\"\n",
    "location = ['United States', 'United Kingdom', 'Turkey', 'Germany', 'Canada', 'South Africa', 'South Korea', 'Japan']\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "for l in location:\n",
    "    temp = df[df['location']==l].corr()['icu_patients'].reset_index()\n",
    "    temp['location'] = l\n",
    "    temp.columns = ['columns', 'icu_patients', 'location']\n",
    "    df1 = pd.concat([df1, temp], 0)\n",
    "\n",
    "df1.to_excel(f'{path}/corr_icu.xlsx', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1643300265463,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "67w94dA9U-qL",
    "outputId": "d19fabd8-de59-4bee-d3b4-660afee70419"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-0c4dfa2a-f856-41e6-a070-2fab9b9d8e4f\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>icu_patients</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>total_cases</td>\n",
       "      <td>-0.103619</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new_cases</td>\n",
       "      <td>-0.000919</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new_cases_smoothed</td>\n",
       "      <td>0.007234</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>total_deaths</td>\n",
       "      <td>-0.092518</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new_deaths</td>\n",
       "      <td>-0.141816</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>BanOnAllEvents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>BanOnAllEventsPartial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>OutdoorOver500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>PlaceOfWorship</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>GymsSportsCentres</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1184 rows × 3 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c4dfa2a-f856-41e6-a070-2fab9b9d8e4f')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-0c4dfa2a-f856-41e6-a070-2fab9b9d8e4f button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-0c4dfa2a-f856-41e6-a070-2fab9b9d8e4f');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                   columns  icu_patients       location\n",
       "0              total_cases     -0.103619  United States\n",
       "1                new_cases     -0.000919  United States\n",
       "2       new_cases_smoothed      0.007234  United States\n",
       "3             total_deaths     -0.092518  United States\n",
       "4               new_deaths     -0.141816  United States\n",
       "..                     ...           ...            ...\n",
       "143         BanOnAllEvents           NaN          Japan\n",
       "144  BanOnAllEventsPartial           NaN          Japan\n",
       "145         OutdoorOver500           NaN          Japan\n",
       "146         PlaceOfWorship           NaN          Japan\n",
       "147      GymsSportsCentres           NaN          Japan\n",
       "\n",
       "[1184 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "隔日重症人數與其他數值型變數的相關係數表\n",
    "\"\"\"\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NasfemvmZdDl"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "時間序列分解\n",
    "\"\"\"\n",
    "data = []\n",
    "for l in location:\n",
    "    df3 = df[df['location']==l]\n",
    "    for x in df3.columns[4:]:\n",
    "        temp = df3[x]\n",
    "        result = seasonal_decompose(temp, freq=7)\n",
    "        result.plot()\n",
    "        temp = pd.DataFrame(result.seasonal)\n",
    "        temp = temp[temp[x].notnull()]\n",
    "        if len(temp[x].unique()) != 1 :\n",
    "            data.append((l, x))\n",
    "        x = x.replace('/', ' ')\n",
    "        plt.title(f'{l}')\n",
    "        plt.savefig(r\"{}/Time Series Decomposition/{}/{}.png\".format(image_path, l, x), dpi=300)\n",
    "\n",
    "dec = pd.DataFrame(data, columns=['location', 'columns'])\n",
    "dec.to_excel(f'{path}/seasonal_columns.xlsx', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1643301511338,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "60QOZLQiVQQD",
    "outputId": "d6f0dd36-7bf6-44eb-f154-b0b057ca9d4b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-45a5cb63-a16f-45ac-886f-2faecf8b8fe0\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>columns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>total_cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>new_cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United States</td>\n",
       "      <td>new_cases_smoothed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United States</td>\n",
       "      <td>total_deaths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United States</td>\n",
       "      <td>new_deaths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>Japan</td>\n",
       "      <td>people_fully_vaccinated_per_hundred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>Japan</td>\n",
       "      <td>new_vaccinations_smoothed_per_million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>Japan</td>\n",
       "      <td>new_people_vaccinated_smoothed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>Japan</td>\n",
       "      <td>new_people_vaccinated_smoothed_per_hundred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>Japan</td>\n",
       "      <td>stringency_index</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>391 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45a5cb63-a16f-45ac-886f-2faecf8b8fe0')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-45a5cb63-a16f-45ac-886f-2faecf8b8fe0 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-45a5cb63-a16f-45ac-886f-2faecf8b8fe0');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "          location                                     columns\n",
       "0    United States                                 total_cases\n",
       "1    United States                                   new_cases\n",
       "2    United States                          new_cases_smoothed\n",
       "3    United States                                total_deaths\n",
       "4    United States                                  new_deaths\n",
       "..             ...                                         ...\n",
       "386          Japan         people_fully_vaccinated_per_hundred\n",
       "387          Japan       new_vaccinations_smoothed_per_million\n",
       "388          Japan              new_people_vaccinated_smoothed\n",
       "389          Japan  new_people_vaccinated_smoothed_per_hundred\n",
       "390          Japan                            stringency_index\n",
       "\n",
       "[391 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "各國家資料變數中具有時間季節性列表\n",
    "\"\"\"\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5h19h06JVu2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "擷取最終建模的資料：隔日新增案例\n",
    "\"\"\"\n",
    "location = ['United States', 'United Kingdom', 'Turkey', 'Germany', 'Canada', 'South Africa', 'South Korea', 'Japan']\n",
    "df = pd.read_csv(f\"{path}/owid-covid-data-for-model.csv\")\n",
    "\n",
    "cor = pd.read_excel(f\"{path}/corr.xlsx\")\n",
    "cor = cor[(cor['new_cases'] >= 0.5) | (cor['new_cases'] < 0)] # 篩選出與目標變數相關性大於等於0.5或小於0的變數\n",
    "dropout = [x for x in cor['columns'].unique() if 'new_cases' in x and 'new_cases' != x]\n",
    "\n",
    "seasonal = pd.read_excel(f'{path}/seasonal_columns.xlsx')\n",
    "\n",
    "for l in location:\n",
    "    c1 = list(cor['columns'][cor['location']==l].unique())\n",
    "    c2 = list(seasonal['columns'][seasonal['location']==l].unique())\n",
    "    c3 = list(set(c1 + c2 + ['new_cases']))\n",
    "    c3 = [x for x in c3 if x not in dropout]\n",
    "    # df[c3][df['location']==l].to_csv(r\"{}/Model/new_cases/{}.csv\".format(path, l), index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZlLNxhOJVxS"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "擷取最終建模的資料：隔日死亡案例\n",
    "\"\"\"\n",
    "location = ['United States', 'United Kingdom', 'Turkey', 'Germany', 'Canada', 'South Africa', 'South Korea', 'Japan']\n",
    "df = pd.read_csv(f\"{path}/owid-covid-data-for-model.csv\")\n",
    "\n",
    "cor = pd.read_excel(f\"{path}/corr_deaths.xlsx\")\n",
    "cor = cor[(cor['new_deaths'] >= 0.5) | (cor['new_deaths'] < 0)] # 篩選出與目標變數相關性大於等於0.5或小於0的變數\n",
    "dropout = [x for x in cor['columns'].unique() if 'new_deaths' in x and 'new_deaths' != x]\n",
    "\n",
    "seasonal = pd.read_excel(f'{path}/seasonal_columns.xlsx')\n",
    "\n",
    "for l in location:\n",
    "    c1 = list(cor['columns'][cor['location']==l].unique())\n",
    "    c2 = list(seasonal['columns'][seasonal['location']==l].unique())\n",
    "    c3 = list(set(c1 + c2 + ['new_deaths']))\n",
    "    c3 = [x for x in c3 if x not in dropout]\n",
    "    df[c3][df['location']==l].to_csv(r\"{}/Model/new_deaths/{}.csv\".format(path, l), index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUPuxc6yJVzx"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "擷取最終建模的資料：隔日重症案例\n",
    "\"\"\"\n",
    "location = ['United States', 'United Kingdom', 'Turkey', 'Germany', 'Canada', 'South Africa', 'South Korea', 'Japan']\n",
    "df = pd.read_csv(f\"{path}/owid-covid-data-for-model.csv\")\n",
    "df.sort_values(['location', 'date'], inplace=True)\n",
    "df['ind'] = df.groupby(['location'])['date'].cumcount()\n",
    "df['icu_patients_1'] = df['icu_patients'].shift(1)\n",
    "df['icu_patients'] = df['icu_patients'] - df['icu_patients_1']\n",
    "df.drop(['icu_patients_1'], 1, inplace=True)\n",
    "df = df[df['ind']!=0].drop(['ind'], 1)\n",
    "\n",
    "cor = pd.read_excel(f\"{path}/corr_icu.xlsx\")\n",
    "cor = cor[(cor['icu_patients'] >= 0.5) | (cor['icu_patients'] < 0)] # 篩選出與目標變數相關性大於等於0.5或小於0的變數\n",
    "dropout = [x for x in cor['columns'].unique() if 'icu' in x and 'icu_patients' != x]\n",
    "\n",
    "seasonal = pd.read_excel(f'{path}/seasonal_columns.xlsx')\n",
    "\n",
    "for l in location:\n",
    "    c1 = list(cor['columns'][cor['location']==l].unique())\n",
    "    c2 = list(seasonal['columns'][seasonal['location']==l].unique())\n",
    "    c3 = list(set(c1 + c2 + ['icu_patients']))\n",
    "    c3 = [x for x in c3 if x not in dropout]\n",
    "    print(len(df[c3][df['location']==l]), len(df[c3][df['location']==l].columns))\n",
    "    df[c3][df['location']==l].to_csv(r\"{}/Model/icu_patients/{}.csv\".format(path, l), index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBLz_EBhK5oE"
   },
   "source": [
    "# **Model Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2453,
     "status": "ok",
     "timestamp": 1643301571872,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "_JEYlXc6K7fs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "target = ['new_cases', 'new_deaths', 'icu_patients']\n",
    "location = ['United States', 'United Kingdom', 'Turkey', 'Germany', 'Canada', 'South Africa', 'South Korea', 'Japan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAJ-_c2xLOtu"
   },
   "source": [
    "### 多特徵之LSTM實驗過程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1643301578009,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "HITbDAJOLQWn"
   },
   "outputs": [],
   "source": [
    "def buildTrain(train, target, pastDay=7, futureDay=1):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureDay-pastDay):\n",
    "        X_train.append(np.array(train.iloc[i:i+pastDay]))\n",
    "        Y_train.append(np.array(train.iloc[i+pastDay:i+pastDay+futureDay][target]))\n",
    "    return np.array(X_train), np.array(Y_train)\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    np.random.seed(10)\n",
    "    randomList = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randomList)\n",
    "    return X[randomList], Y[randomList]\n",
    "\n",
    "def splitData(X, Y, rate):\n",
    "    X_train = X[int(X.shape[0]*rate):]\n",
    "    Y_train = Y[int(Y.shape[0]*rate):]\n",
    "    X_val = X[:int(X.shape[0]*rate)]\n",
    "    Y_val = Y[:int(Y.shape[0]*rate)]\n",
    "    return X_train, Y_train, X_val, Y_val\n",
    "\n",
    "def buildOneToOneModel(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, input_length=shape[1], input_dim=shape[2], return_sequences=False))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(8))\n",
    "    model.add(Dense(4))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 167756,
     "status": "ok",
     "timestamp": 1643302146777,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "h6h0Q3j4LTz2",
    "outputId": "4a3244e3-e0bd-4d0a-86bb-46aad37aa007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 10)                1960      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,865\n",
      "Trainable params: 182,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 6s 6s/step - loss: 7725279744.0000 - val_loss: 10201066496.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7724830208.0000 - val_loss: 10200434688.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7724327424.0000 - val_loss: 10199636992.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7723694080.0000 - val_loss: 10198586368.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7722860032.0000 - val_loss: 10197186560.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7721747456.0000 - val_loss: 10195324928.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7720269312.0000 - val_loss: 10192864256.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7718315008.0000 - val_loss: 10189635584.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7715750400.0000 - val_loss: 10185433088.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7712412160.0000 - val_loss: 10180006912.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7708101632.0000 - val_loss: 10173048832.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7702577152.0000 - val_loss: 10164187136.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7695540224.0000 - val_loss: 10152967168.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7686633472.0000 - val_loss: 10138845184.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7675423744.0000 - val_loss: 10121157632.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7661387776.0000 - val_loss: 10099103744.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7643892736.0000 - val_loss: 10071728128.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7622182912.0000 - val_loss: 10037892096.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7595364352.0000 - val_loss: 9996251136.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7562379264.0000 - val_loss: 9945223168.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7521987072.0000 - val_loss: 9882957824.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7472745472.0000 - val_loss: 9807301632.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7412982272.0000 - val_loss: 9715775488.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7340786176.0000 - val_loss: 9605543936.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7253985792.0000 - val_loss: 9473401856.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7150160896.0000 - val_loss: 9315783680.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7026654208.0000 - val_loss: 9128784896.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6880620544.0000 - val_loss: 8908244992.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6709123584.0000 - val_loss: 8649875456.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6509289472.0000 - val_loss: 8349513728.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6278569984.0000 - val_loss: 8003497984.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 6015135232.0000 - val_loss: 7609262592.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5718483968.0000 - val_loss: 7166249472.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5390347776.0000 - val_loss: 6677280768.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5036024832.0000 - val_loss: 6150599680.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4666328064.0000 - val_loss: 5602858496.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4300321792.0000 - val_loss: 5063261184.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3968871936.0000 - val_loss: 4578456064.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3718016768.0000 - val_loss: 4208500224.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3595962624.0000 - val_loss: 4229433600.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3637894400.0000 - val_loss: 4088095744.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3720492544.0000 - val_loss: 4102655232.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3917800192.0000 - val_loss: 4163864320.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4082615040.0000 - val_loss: 4171815168.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4108438272.0000 - val_loss: 3888399360.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3704154368.0000 - val_loss: 4096375552.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3707854080.0000 - val_loss: 4124669696.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3639623168.0000 - val_loss: 4558299648.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3720834816.0000 - val_loss: 4654111744.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3756408576.0000 - val_loss: 4717430272.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3783973120.0000 - val_loss: 4742430720.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3795061504.0000 - val_loss: 4730027008.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3788399872.0000 - val_loss: 4684732928.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3766282496.0000 - val_loss: 4613445120.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3733264896.0000 - val_loss: 4524690432.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3695177984.0000 - val_loss: 4427998208.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3658112768.0000 - val_loss: 4331776512.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3625586688.0000 - val_loss: 4834320896.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3843623936.0000 - val_loss: 4690765312.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3768423936.0000 - val_loss: 4545348096.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3705308928.0000 - val_loss: 4410438144.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3660305664.0000 - val_loss: 4298515968.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3640183040.0000 - val_loss: 4217307648.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3645834752.0000 - val_loss: 4167137792.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3670016512.0000 - val_loss: 4140899328.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3698628352.0000 - val_loss: 4128201216.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3716617216.0000 - val_loss: 4121514752.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3715340032.0000 - val_loss: 4119569408.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3695948800.0000 - val_loss: 4125830400.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3666900224.0000 - val_loss: 4144270336.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3638473728.0000 - val_loss: 4175857664.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3618148096.0000 - val_loss: 4217450752.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3608690688.0000 - val_loss: 4262815488.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3608687360.0000 - val_loss: 4304599040.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3614310144.0000 - val_loss: 4336180224.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3621177088.0000 - val_loss: 4352859648.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3625676544.0000 - val_loss: 4352335360.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3625612800.0000 - val_loss: 4334530048.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3620283904.0000 - val_loss: 4300784128.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3610101504.0000 - val_loss: 4434714112.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3663758848.0000 - val_loss: 4355919872.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3632446976.0000 - val_loss: 4266198784.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3600231936.0000 - val_loss: 4179011072.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3561440000.0000 - val_loss: 3986769152.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3348452096.0000 - val_loss: 7256963072.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5460872192.0000 - val_loss: 11605266432.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8852575232.0000 - val_loss: 6715935232.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5061978112.0000 - val_loss: 6355947008.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4806595072.0000 - val_loss: 5933159936.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 4477207040.0000 - val_loss: 9690260480.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7323275776.0000 - val_loss: 9014774784.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6793911296.0000 - val_loss: 8262035968.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6213339648.0000 - val_loss: 7461922304.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5610202624.0000 - val_loss: 6642808320.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5013568000.0000 - val_loss: 5839671296.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4459782656.0000 - val_loss: 5101877248.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3998408192.0000 - val_loss: 4496134656.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3692633856.0000 - val_loss: 4095234560.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3604601088.0000 - val_loss: 3939380736.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3752271616.0000 - val_loss: 3978142720.0000\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1 (LSTM)               (None, 10)                1960      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,865\n",
      "Trainable params: 182,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 8384397824.0000 - val_loss: 4649111040.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8384119808.0000 - val_loss: 4648863744.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8383803904.0000 - val_loss: 4648549376.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8383403008.0000 - val_loss: 4648132608.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8382872064.0000 - val_loss: 4647577600.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8382162432.0000 - val_loss: 4646842368.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8381223936.0000 - val_loss: 4645875712.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8379990016.0000 - val_loss: 4644614656.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8378381312.0000 - val_loss: 4642985472.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8376301056.0000 - val_loss: 4640893440.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8373630464.0000 - val_loss: 4638225408.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8370224640.0000 - val_loss: 4634844160.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8365907456.0000 - val_loss: 4630584320.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8360466432.0000 - val_loss: 4625246720.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8353648128.0000 - val_loss: 4618592768.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8345146368.0000 - val_loss: 4610335232.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8334591488.0000 - val_loss: 4600130560.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8321541632.0000 - val_loss: 4587569664.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8305470464.0000 - val_loss: 4572167168.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8285751808.0000 - val_loss: 4553341440.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8261638144.0000 - val_loss: 4526182912.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8226783232.0000 - val_loss: 4497411584.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8189840384.0000 - val_loss: 4462582784.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8145035776.0000 - val_loss: 4420554240.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8090864128.0000 - val_loss: 4370047488.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8025611264.0000 - val_loss: 4309630976.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7947325440.0000 - val_loss: 4237722880.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7853809664.0000 - val_loss: 4152606720.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7742620160.0000 - val_loss: 4052476160.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7611082240.0000 - val_loss: 3935501312.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7456337408.0000 - val_loss: 3799952128.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7275430400.0000 - val_loss: 3644383488.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7065462784.0000 - val_loss: 3467916288.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 6823815168.0000 - val_loss: 3332806656.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6635978240.0000 - val_loss: 3054272512.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6238709760.0000 - val_loss: 2823111936.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5895818752.0000 - val_loss: 2585152768.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5524267520.0000 - val_loss: 2353913344.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5133347328.0000 - val_loss: 2150827008.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4739603456.0000 - val_loss: 2008169856.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4369937408.0000 - val_loss: 1971737472.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 4065002240.0000 - val_loss: 2092081664.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3873108224.0000 - val_loss: 2257465600.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3873819392.0000 - val_loss: 2674333952.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3956127232.0000 - val_loss: 3192005632.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4176368384.0000 - val_loss: 3584293632.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4379380224.0000 - val_loss: 3689229056.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4434404864.0000 - val_loss: 3518069248.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4338147328.0000 - val_loss: 3165147904.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4155471616.0000 - val_loss: 2040634496.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4428500480.0000 - val_loss: 2052954240.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4491193856.0000 - val_loss: 2756987904.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 5744366592.0000 - val_loss: 2701934336.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5674158592.0000 - val_loss: 2651958016.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5591639552.0000 - val_loss: 2582008832.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5474733568.0000 - val_loss: 2496803584.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5328201216.0000 - val_loss: 2401825280.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5157050368.0000 - val_loss: 2303517696.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4966899200.0000 - val_loss: 2209854464.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4764686336.0000 - val_loss: 2130804864.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4559330304.0000 - val_loss: 2078526080.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 4362220032.0000 - val_loss: 2066957056.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4187261952.0000 - val_loss: 2110276224.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4050043392.0000 - val_loss: 2219516160.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3965438208.0000 - val_loss: 2396809984.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3943083520.0000 - val_loss: 2628081920.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3981051648.0000 - val_loss: 2878207232.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4060665088.0000 - val_loss: 3095826944.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4148305920.0000 - val_loss: 3231231232.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4207909888.0000 - val_loss: 3257858304.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4217693696.0000 - val_loss: 3181266944.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4178265856.0000 - val_loss: 3031046144.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4107406080.0000 - val_loss: 2470627328.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3976941056.0000 - val_loss: 2355574016.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3945570560.0000 - val_loss: 2277803520.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3943651072.0000 - val_loss: 2219396608.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3949156352.0000 - val_loss: 2177846528.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3956552704.0000 - val_loss: 2149940736.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3962097408.0000 - val_loss: 2132663680.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3963489792.0000 - val_loss: 2123812096.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3959713280.0000 - val_loss: 2122081792.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3950824448.0000 - val_loss: 2126873088.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3937695232.0000 - val_loss: 2137996416.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3921731840.0000 - val_loss: 2155351552.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3904568320.0000 - val_loss: 2178600960.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3887754496.0000 - val_loss: 2206861312.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3872466432.0000 - val_loss: 2238446080.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3859412480.0000 - val_loss: 2270708224.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3848871936.0000 - val_loss: 2299510272.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3840385536.0000 - val_loss: 2318392064.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3832734208.0000 - val_loss: 2317367808.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3823605760.0000 - val_loss: 2290606080.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3808460544.0000 - val_loss: 2155435264.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3663600128.0000 - val_loss: 2620140544.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5578522624.0000 - val_loss: 2536478976.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5444977664.0000 - val_loss: 2389847808.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5240829440.0000 - val_loss: 9481125888.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 14198403072.0000 - val_loss: 3413332480.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6737151488.0000 - val_loss: 3213306880.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6474725888.0000 - val_loss: 3020788480.0000\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 10)                1960      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,865\n",
      "Trainable params: 182,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 7833455104.0000 - val_loss: 9938801664.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7833101312.0000 - val_loss: 9938362368.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7832721920.0000 - val_loss: 9937818624.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 7832250880.0000 - val_loss: 9937110016.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 7831636992.0000 - val_loss: 9936174080.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 7830828032.0000 - val_loss: 9934938112.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7829757952.0000 - val_loss: 9933315072.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7828351488.0000 - val_loss: 9931195392.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7826517504.0000 - val_loss: 9928452096.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 7824143872.0000 - val_loss: 9924930560.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7821095424.0000 - val_loss: 9920438272.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7817206784.0000 - val_loss: 9914743808.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7812277760.0000 - val_loss: 9907564544.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 7806067200.0000 - val_loss: 9898565632.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7798280192.0000 - val_loss: 9887343616.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7788570624.0000 - val_loss: 9873416192.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7776522240.0000 - val_loss: 9856205824.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7761636352.0000 - val_loss: 9835027456.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7743321600.0000 - val_loss: 9809068032.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7720881152.0000 - val_loss: 9777373184.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 7693488640.0000 - val_loss: 9738820608.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7660184064.0000 - val_loss: 9692105728.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7619846656.0000 - val_loss: 9635716096.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7571185152.0000 - val_loss: 9567909888.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7512714240.0000 - val_loss: 9486692352.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7442739712.0000 - val_loss: 9389800448.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7359350784.0000 - val_loss: 9274692608.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 7260414464.0000 - val_loss: 9138548736.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 7143588352.0000 - val_loss: 8978309120.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7006358016.0000 - val_loss: 8790729728.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6846111744.0000 - val_loss: 8572489728.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 6660250624.0000 - val_loss: 8320364032.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 6446364672.0000 - val_loss: 8031476736.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 6202513408.0000 - val_loss: 7703709696.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 5927631360.0000 - val_loss: 7336278016.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 5622120960.0000 - val_loss: 6930599424.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 5288726528.0000 - val_loss: 6491533312.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 4933775872.0000 - val_loss: 6029188096.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 4568916480.0000 - val_loss: 5561385984.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4213406976.0000 - val_loss: 5116818432.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3896674816.0000 - val_loss: 4733617152.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3652562944.0000 - val_loss: 4721744896.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3648161536.0000 - val_loss: 4795604992.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3692062464.0000 - val_loss: 4544930304.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3590426880.0000 - val_loss: 4474876416.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3671232000.0000 - val_loss: 4551681024.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3867140352.0000 - val_loss: 4633121792.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4014227968.0000 - val_loss: 4624187904.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 4016825856.0000 - val_loss: 4539452928.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3901820160.0000 - val_loss: 4442371072.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3746530304.0000 - val_loss: 4386342400.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3615204608.0000 - val_loss: 4486778368.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3547145728.0000 - val_loss: 4585703424.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3578452992.0000 - val_loss: 4685762560.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3625531392.0000 - val_loss: 4733312512.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3619377408.0000 - val_loss: 6444088320.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4891915776.0000 - val_loss: 9220097024.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7214792704.0000 - val_loss: 8791543808.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 6846732288.0000 - val_loss: 8231255040.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 6369284608.0000 - val_loss: 7599073792.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5837040128.0000 - val_loss: 6934122496.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5286974976.0000 - val_loss: 6269966336.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4751936000.0000 - val_loss: 5644247552.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4269177344.0000 - val_loss: 5104343040.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3884948992.0000 - val_loss: 4705950208.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3651813120.0000 - val_loss: 4498125312.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3611910400.0000 - val_loss: 4488815616.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3760280832.0000 - val_loss: 4606364672.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 4005732096.0000 - val_loss: 4717212160.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4193360640.0000 - val_loss: 4682174976.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 4159936768.0000 - val_loss: 5240559616.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3976913408.0000 - val_loss: 5175111168.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3926764032.0000 - val_loss: 5062309888.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3850965248.0000 - val_loss: 4925806592.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3764694528.0000 - val_loss: 4789359616.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3686636800.0000 - val_loss: 4672933376.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3631976704.0000 - val_loss: 4589721088.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3608820480.0000 - val_loss: 4543158272.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3615276800.0000 - val_loss: 4526132224.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3639283200.0000 - val_loss: 4524690944.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3663327488.0000 - val_loss: 4525442048.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3672751872.0000 - val_loss: 4521825792.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3662350080.0000 - val_loss: 4514816512.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3636901888.0000 - val_loss: 4508144640.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3606453504.0000 - val_loss: 4500981760.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3580433920.0000 - val_loss: 4483220992.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3564120832.0000 - val_loss: 4452582400.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3557759488.0000 - val_loss: 4441940992.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3549171712.0000 - val_loss: 14692148224.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 11991193600.0000 - val_loss: 4471392256.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3540588288.0000 - val_loss: 4465047552.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3548795904.0000 - val_loss: 4466091520.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3552429824.0000 - val_loss: 4471976448.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3553311232.0000 - val_loss: 4477803520.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3553060352.0000 - val_loss: 4481865216.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3552958720.0000 - val_loss: 4484528640.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3553612544.0000 - val_loss: 4486505984.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3555134464.0000 - val_loss: 4488340992.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3557319936.0000 - val_loss: 4490383360.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3559797504.0000 - val_loss: 4492837376.0000\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 10)                1960      \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,865\n",
      "Trainable params: 182,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 5s 5s/step - loss: 8220933632.0000 - val_loss: 6868711424.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8220814848.0000 - val_loss: 6868593152.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 8220686848.0000 - val_loss: 6868445696.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8220528640.0000 - val_loss: 6868248064.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8220314112.0000 - val_loss: 6867981824.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8220028416.0000 - val_loss: 6867625472.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8219644416.0000 - val_loss: 6867152896.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8219135488.0000 - val_loss: 6866530816.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8218464768.0000 - val_loss: 6865718272.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8217589248.0000 - val_loss: 6864664576.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8216453632.0000 - val_loss: 6863310336.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8214993920.0000 - val_loss: 6861579264.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 8213128192.0000 - val_loss: 6859378688.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 8210757120.0000 - val_loss: 6856597504.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 8207760896.0000 - val_loss: 6853101568.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8203992064.0000 - val_loss: 6848722432.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 8199272448.0000 - val_loss: 6843261952.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 8193387520.0000 - val_loss: 6836479488.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 8186077696.0000 - val_loss: 6828090368.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 8177033216.0000 - val_loss: 6817746944.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 8165881856.0000 - val_loss: 6805037056.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8152175616.0000 - val_loss: 6789471232.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8135389696.0000 - val_loss: 6770469888.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8114892288.0000 - val_loss: 6747344384.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8089940992.0000 - val_loss: 6719295488.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 8059664896.0000 - val_loss: 6685387264.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8023051776.0000 - val_loss: 6644533248.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 7978915840.0000 - val_loss: 6595481088.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7925891072.0000 - val_loss: 6536802304.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 7862411264.0000 - val_loss: 6466874880.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7786694144.0000 - val_loss: 6383888896.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 7696733184.0000 - val_loss: 6285845504.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7590300672.0000 - val_loss: 6170587648.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 7464966144.0000 - val_loss: 6035847680.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 7318128640.0000 - val_loss: 5879332864.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7147101696.0000 - val_loss: 5698868224.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 6949236224.0000 - val_loss: 5492613632.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 6722116608.0000 - val_loss: 5259394048.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 6463874560.0000 - val_loss: 4999183872.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 6173639168.0000 - val_loss: 4713812480.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5852204544.0000 - val_loss: 4407966720.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 5502995456.0000 - val_loss: 4090615552.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 5133433856.0000 - val_loss: 3776962816.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 4756853248.0000 - val_loss: 3491010048.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4395055104.0000 - val_loss: 3268417024.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4081317888.0000 - val_loss: 3157928192.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3862466816.0000 - val_loss: 3214400512.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3793988864.0000 - val_loss: 3463450880.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3910081280.0000 - val_loss: 3822374912.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4151640832.0000 - val_loss: 4094076160.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 4350944768.0000 - val_loss: 4152540160.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 4391323648.0000 - val_loss: 4024303872.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 4288878080.0000 - val_loss: 3801266176.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 4119891200.0000 - val_loss: 3568220160.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3955209216.0000 - val_loss: 3377641472.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3836595200.0000 - val_loss: 3249270272.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3776370688.0000 - val_loss: 3179717632.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3766631680.0000 - val_loss: 3154036480.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3790210560.0000 - val_loss: 3154790400.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3829020160.0000 - val_loss: 3167150592.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3868494080.0000 - val_loss: 3180701440.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3898893824.0000 - val_loss: 3189320192.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3914919424.0000 - val_loss: 3190171392.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3914722560.0000 - val_loss: 3182092544.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3898913536.0000 - val_loss: 3163452416.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3869384448.0000 - val_loss: 3426149632.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 4274098944.0000 - val_loss: 3347909632.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 4179546112.0000 - val_loss: 3244536576.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4063863040.0000 - val_loss: 3869054976.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4855604224.0000 - val_loss: 3700147200.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4651371008.0000 - val_loss: 3524423424.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 4426926592.0000 - val_loss: 3359217664.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 4200996608.0000 - val_loss: 3228237312.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3997041152.0000 - val_loss: 3158396160.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3842232832.0000 - val_loss: 3174237440.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3763024896.0000 - val_loss: 3285907456.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3774612480.0000 - val_loss: 3472070656.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3865170432.0000 - val_loss: 3671914496.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3987004160.0000 - val_loss: 3808538368.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4075202048.0000 - val_loss: 3834715392.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 4087720704.0000 - val_loss: 3755873024.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 4026989824.0000 - val_loss: 3614214656.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3926935296.0000 - val_loss: 3458595072.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3826853120.0000 - val_loss: 3322706176.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3753288448.0000 - val_loss: 3216408064.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3714702848.0000 - val_loss: 2939202816.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3550855680.0000 - val_loss: 3742646528.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4688179712.0000 - val_loss: 3769368064.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4726341120.0000 - val_loss: 3747612416.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4700313600.0000 - val_loss: 3684545280.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 4621014528.0000 - val_loss: 3590842880.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4500469248.0000 - val_loss: 3478770944.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4351462400.0000 - val_loss: 3362315264.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4188106240.0000 - val_loss: 3257334528.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4026315264.0000 - val_loss: 3181025536.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3883619328.0000 - val_loss: 3149977344.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3777669888.0000 - val_loss: 3176154624.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3722805504.0000 - val_loss: 3260879104.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 3724623616.0000 - val_loss: 3388909312.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3774315776.0000 - val_loss: 3527618560.0000\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 10)                1960      \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,865\n",
      "Trainable params: 182,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 7798354432.0000 - val_loss: 11331851264.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7797943296.0000 - val_loss: 11331307520.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7797492224.0000 - val_loss: 11330627584.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7796928512.0000 - val_loss: 11329732608.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7796186624.0000 - val_loss: 11328542720.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7795202048.0000 - val_loss: 11326965760.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7793892352.0000 - val_loss: 11324881920.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7792166912.0000 - val_loss: 11322160128.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7789910016.0000 - val_loss: 11318629376.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7786984960.0000 - val_loss: 11314085888.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7783218688.0000 - val_loss: 11308278784.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7778406400.0000 - val_loss: 11300897792.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7772291584.0000 - val_loss: 11291577344.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7764569088.0000 - val_loss: 11279863808.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7754865664.0000 - val_loss: 11265211392.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7742731776.0000 - val_loss: 11246961664.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7727617024.0000 - val_loss: 11224205312.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7708773376.0000 - val_loss: 11183347712.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7674991616.0000 - val_loss: 11146194944.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7644279808.0000 - val_loss: 11100590080.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 7606609408.0000 - val_loss: 11044757504.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7560528384.0000 - val_loss: 11079963648.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7589362176.0000 - val_loss: 10894081024.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7436380672.0000 - val_loss: 10794792960.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7354750976.0000 - val_loss: 10675634176.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7256971264.0000 - val_loss: 10533126144.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7140309504.0000 - val_loss: 10363468800.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7001831936.0000 - val_loss: 10162580480.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6838464000.0000 - val_loss: 9926197248.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 6647110144.0000 - val_loss: 9650055168.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 6424866816.0000 - val_loss: 9330217984.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6169360384.0000 - val_loss: 8963571712.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5879261696.0000 - val_loss: 8548577792.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5555042816.0000 - val_loss: 8086431232.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5200097792.0000 - val_loss: 7582572032.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4822320128.0000 - val_loss: 7040097280.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4431007744.0000 - val_loss: 6510305280.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4068784896.0000 - val_loss: 6004780544.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3758086144.0000 - val_loss: 5588611072.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3558866944.0000 - val_loss: 5334865920.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3531047424.0000 - val_loss: 5290663936.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3688875264.0000 - val_loss: 5501195776.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3545626624.0000 - val_loss: 5359101440.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3529958656.0000 - val_loss: 5279913472.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3564053248.0000 - val_loss: 5250234880.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3618008064.0000 - val_loss: 5241888256.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3650162688.0000 - val_loss: 5229703168.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3634014208.0000 - val_loss: 5110006272.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3497535488.0000 - val_loss: 5303290368.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3473979392.0000 - val_loss: 5133179904.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3241554688.0000 - val_loss: 5388161024.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3453941248.0000 - val_loss: 5442499072.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3471553536.0000 - val_loss: 5458585088.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3472729344.0000 - val_loss: 5457682432.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3469490176.0000 - val_loss: 5441940480.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3462707456.0000 - val_loss: 5414599680.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3453891328.0000 - val_loss: 5380384256.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3445235200.0000 - val_loss: 5344464896.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3438726400.0000 - val_loss: 5311368704.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3435288320.0000 - val_loss: 5284193280.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3434374912.0000 - val_loss: 5264388608.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3434302976.0000 - val_loss: 5252151296.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3433225472.0000 - val_loss: 5247067136.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3430162432.0000 - val_loss: 5248488960.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3425413120.0000 - val_loss: 5255418880.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3420149248.0000 - val_loss: 5266121728.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3415566080.0000 - val_loss: 5277920768.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3412157184.0000 - val_loss: 5287323136.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3409400832.0000 - val_loss: 5289752064.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3405413376.0000 - val_loss: 5260612608.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3383246592.0000 - val_loss: 6624635904.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4119576064.0000 - val_loss: 6422195200.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3982826240.0000 - val_loss: 8901514240.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 5824971776.0000 - val_loss: 8287468032.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5345768448.0000 - val_loss: 7562131456.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4796637184.0000 - val_loss: 6793374208.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4244705024.0000 - val_loss: 6066030592.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3774334976.0000 - val_loss: 5493703680.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3493470720.0000 - val_loss: 5197984256.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3503688704.0000 - val_loss: 5214657536.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3794092032.0000 - val_loss: 5373755392.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4128616960.0000 - val_loss: 5431827456.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4229354496.0000 - val_loss: 5342095360.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4069220352.0000 - val_loss: 5216931328.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3802228736.0000 - val_loss: 5165951488.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3577668864.0000 - val_loss: 5225824768.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3462667520.0000 - val_loss: 5371219968.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3451760384.0000 - val_loss: 5552106496.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3505296640.0000 - val_loss: 5722545152.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3580821760.0000 - val_loss: 5852187136.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3647439104.0000 - val_loss: 5926286848.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3688011520.0000 - val_loss: 5941732352.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3696475648.0000 - val_loss: 5903188480.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3674532352.0000 - val_loss: 5820455424.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3629094656.0000 - val_loss: 5706860544.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3570498560.0000 - val_loss: 5578002432.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3510924288.0000 - val_loss: 5450219008.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3462491392.0000 - val_loss: 5338329600.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3434688512.0000 - val_loss: 5252739072.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3431404288.0000 - val_loss: 5196888064.0000\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_5 (LSTM)               (None, 10)                1960      \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,865\n",
      "Trainable params: 182,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 8091459584.0000 - val_loss: 11227309056.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8091133440.0000 - val_loss: 11226861568.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8090762752.0000 - val_loss: 11226293248.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8090293760.0000 - val_loss: 11225551872.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8089679872.0000 - val_loss: 11224568832.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8088866816.0000 - val_loss: 11223263232.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8087787520.0000 - val_loss: 11221541888.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8086363136.0000 - val_loss: 11219284992.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8084496896.0000 - val_loss: 11216350208.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8082070016.0000 - val_loss: 11212564480.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8078939136.0000 - val_loss: 11207718912.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8074930688.0000 - val_loss: 11201555456.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8069835264.0000 - val_loss: 11193769984.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8063397888.0000 - val_loss: 11183986688.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8055310336.0000 - val_loss: 11171760128.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8045204992.0000 - val_loss: 11156547584.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8032634880.0000 - val_loss: 11137709056.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 8017071616.0000 - val_loss: 11114473472.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7997884928.0000 - val_loss: 11085933568.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7974325760.0000 - val_loss: 11051002880.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7945508352.0000 - val_loss: 11008403456.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7910389248.0000 - val_loss: 10956633088.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 7867741696.0000 - val_loss: 10893944832.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 7816150528.0000 - val_loss: 10818314240.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 7753986560.0000 - val_loss: 10727417856.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 7679389184.0000 - val_loss: 10618612736.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7590256640.0000 - val_loss: 10488910848.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7484249600.0000 - val_loss: 10334982144.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 7358797312.0000 - val_loss: 10153178112.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7211140608.0000 - val_loss: 9939594240.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7038425600.0000 - val_loss: 9690199040.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 6837842432.0000 - val_loss: 9401014272.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6606849024.0000 - val_loss: 9173765120.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6427067904.0000 - val_loss: 8809897984.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6141225472.0000 - val_loss: 8399051776.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5822925824.0000 - val_loss: 7941912064.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5475353600.0000 - val_loss: 8169485824.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5655373312.0000 - val_loss: 6915540992.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4729643008.0000 - val_loss: 6374663680.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4366145024.0000 - val_loss: 5851210752.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 4049078016.0000 - val_loss: 5392495616.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3826774272.0000 - val_loss: 5061973504.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3757545984.0000 - val_loss: 4917968384.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3878349824.0000 - val_loss: 4946322432.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4126579200.0000 - val_loss: 5021899264.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4325202432.0000 - val_loss: 5036794368.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4357714432.0000 - val_loss: 4989073408.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4246839808.0000 - val_loss: 4929605120.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4073353216.0000 - val_loss: 4905057280.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3909225728.0000 - val_loss: 4936640000.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3795054592.0000 - val_loss: 5020736512.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3740985856.0000 - val_loss: 5138650112.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3737015808.0000 - val_loss: 5267308544.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3764557056.0000 - val_loss: 6260095488.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4392774656.0000 - val_loss: 6246486528.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4328693248.0000 - val_loss: 6288918016.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4345793536.0000 - val_loss: 7046862336.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4853638144.0000 - val_loss: 6973913088.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4801122304.0000 - val_loss: 6846880256.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4709952000.0000 - val_loss: 7515297280.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5188276736.0000 - val_loss: 7283237376.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5022730752.0000 - val_loss: 7006584832.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4828972032.0000 - val_loss: 7824202240.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5423791616.0000 - val_loss: 7485221376.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5179051520.0000 - val_loss: 7114754048.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4921871360.0000 - val_loss: 6722411520.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4661972992.0000 - val_loss: 6322878464.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4414642176.0000 - val_loss: 5936167424.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4199673856.0000 - val_loss: 5587334656.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4040041728.0000 - val_loss: 5303580672.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3957530112.0000 - val_loss: 5106972160.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3963684096.0000 - val_loss: 5002886656.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4047121408.0000 - val_loss: 4970731008.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4165927424.0000 - val_loss: 4970046464.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4259969024.0000 - val_loss: 4964751872.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 4282756608.0000 - val_loss: 4943805952.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4224342528.0000 - val_loss: 4752691200.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3838112000.0000 - val_loss: 5598556672.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4056005376.0000 - val_loss: 5668041728.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4069168384.0000 - val_loss: 5707055616.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4074076928.0000 - val_loss: 5714481152.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4063559424.0000 - val_loss: 5692260352.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3977584384.0000 - val_loss: 5640784384.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3944037376.0000 - val_loss: 5566024704.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3903950848.0000 - val_loss: 5475127808.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3859747584.0000 - val_loss: 5375862272.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3816645632.0000 - val_loss: 5276066304.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3779809792.0000 - val_loss: 5182692352.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3753287936.0000 - val_loss: 5101051392.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3739081216.0000 - val_loss: 5034125824.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3736383744.0000 - val_loss: 4982399488.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3741232384.0000 - val_loss: 4944549888.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3746443520.0000 - val_loss: 4603699200.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3319223296.0000 - val_loss: 6065590784.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4184938752.0000 - val_loss: 5957690368.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4116251904.0000 - val_loss: 5815785984.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4033061632.0000 - val_loss: 5653165568.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3944990720.0000 - val_loss: 5484323840.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3863322624.0000 - val_loss: 5323898880.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3798919680.0000 - val_loss: 5185100800.0000\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 10)                1960      \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,865\n",
      "Trainable params: 182,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2122815.0000 - val_loss: 2766166.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2122725.2500 - val_loss: 2766004.7500\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2122590.5000 - val_loss: 2765766.5000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2122391.2500 - val_loss: 2765419.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2122101.5000 - val_loss: 2764923.2500\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2121687.7500 - val_loss: 2764228.2500\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2121107.2500 - val_loss: 2763269.2500\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2120307.7500 - val_loss: 2761966.2500\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2119220.0000 - val_loss: 2760215.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2117758.7500 - val_loss: 2757887.5000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2115816.7500 - val_loss: 2754823.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2113261.0000 - val_loss: 2750822.5000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2109924.7500 - val_loss: 2745641.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2105604.7500 - val_loss: 2738976.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2100051.2500 - val_loss: 2730462.2500\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2092959.8750 - val_loss: 2719653.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2083962.6250 - val_loss: 2706011.7500\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2072617.5000 - val_loss: 2688894.7500\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2058396.0000 - val_loss: 2667536.2500\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2040673.6250 - val_loss: 2641033.7500\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2018716.8750 - val_loss: 2608334.5000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1991680.8750 - val_loss: 2568227.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1958603.5000 - val_loss: 2519335.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1918410.2500 - val_loss: 2460129.2500\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1869937.8750 - val_loss: 2388961.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1811976.8750 - val_loss: 2304127.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1743355.3750 - val_loss: 2203995.5000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1663082.2500 - val_loss: 2087217.6250\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1570582.6250 - val_loss: 1953068.5000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1466065.5000 - val_loss: 1801997.6250\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1351102.0000 - val_loss: 1636495.1250\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1229514.2500 - val_loss: 1462431.5000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1108711.8750 - val_loss: 1291080.5000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1001587.6250 - val_loss: 1141721.7500\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 928548.3750 - val_loss: 1042496.7500\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 916222.6875 - val_loss: 1017226.2500\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 977943.7500 - val_loss: 1042484.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1065642.7500 - val_loss: 1056096.6250\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1100856.6250 - val_loss: 1039251.9375\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1071229.0000 - val_loss: 1011865.5625\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1009370.5000 - val_loss: 996413.5625\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 948628.0000 - val_loss: 1003485.6250\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 907734.2500 - val_loss: 1031545.5000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 890745.2500 - val_loss: 1072139.5000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 892524.8125 - val_loss: 1115441.6250\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 904741.0625 - val_loss: 1153598.3750\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 919810.3125 - val_loss: 1181722.1250\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 932450.1250 - val_loss: 1197541.8750\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 939730.0625 - val_loss: 1200672.8750\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 940564.6250 - val_loss: 1191996.5000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 935179.4375 - val_loss: 1173259.3750\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 924717.5000 - val_loss: 1146852.5000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 910970.2500 - val_loss: 1115665.8750\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 896158.6875 - val_loss: 1082916.6250\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 882671.6250 - val_loss: 1051855.2500\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 872678.7500 - val_loss: 1025309.5000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 867610.3750 - val_loss: 1005121.6250\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 867606.1875 - val_loss: 991715.9375\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 871226.8750 - val_loss: 984125.8750\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 875782.6250 - val_loss: 980651.0625\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 878377.9375 - val_loss: 979833.5000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 877195.6875 - val_loss: 981106.9375\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 872218.0000 - val_loss: 984711.4375\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 864951.9375 - val_loss: 991070.5625\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 857472.9375 - val_loss: 1000155.8750\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 851453.7500 - val_loss: 1011218.2500\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 847648.5000 - val_loss: 1022924.4375\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 845898.0625 - val_loss: 1033718.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 845454.3750 - val_loss: 1042190.9375\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 845382.0000 - val_loss: 1047347.1250\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 844876.1250 - val_loss: 1048722.5000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 843443.3125 - val_loss: 1046389.5000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 840953.7500 - val_loss: 1040883.8750\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 837601.6250 - val_loss: 1033081.3125\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 833803.3125 - val_loss: 1024042.3750\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 830059.8750 - val_loss: 1014845.3125\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 826810.2500 - val_loss: 1006423.6875\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 824303.3750 - val_loss: 999444.8750\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 822529.6250 - val_loss: 994260.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 821237.5625 - val_loss: 990937.7500\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 820045.6250 - val_loss: 989364.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 818600.0625 - val_loss: 989350.9375\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 816715.8750 - val_loss: 990701.9375\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 814429.8125 - val_loss: 993214.5000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 811951.2500 - val_loss: 996639.1250\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 809547.8125 - val_loss: 1000640.8125\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 807428.8125 - val_loss: 1004798.0625\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 805677.8750 - val_loss: 1008647.4375\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 804249.0000 - val_loss: 1011752.8750\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 803013.0000 - val_loss: 1013779.4375\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 801820.8750 - val_loss: 1014544.0625\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 800559.3125 - val_loss: 1014034.5625\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 799182.6250 - val_loss: 1012396.1250\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 797714.3125 - val_loss: 1009891.6250\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 796225.6250 - val_loss: 1006844.3125\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 794800.1875 - val_loss: 1003579.5000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 793499.0000 - val_loss: 1000374.9375\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 792336.6875 - val_loss: 997429.5000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 791276.3750 - val_loss: 994853.3125\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 790245.9375 - val_loss: 992675.9375\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_7 (LSTM)               (None, 10)                1960      \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_76 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,865\n",
      "Trainable params: 182,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2242098.5000 - val_loss: 1801425.5000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2241671.0000 - val_loss: 1800834.6250\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2241033.7500 - val_loss: 1799965.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2240095.5000 - val_loss: 1798708.1250\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2238739.5000 - val_loss: 1796922.1250\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2236812.2500 - val_loss: 1794430.8750\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2234123.7500 - val_loss: 1791015.5000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2230437.5000 - val_loss: 1786399.3750\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2225454.2500 - val_loss: 1780235.1250\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2218797.5000 - val_loss: 1772092.6250\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2210002.0000 - val_loss: 1761445.6250\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2198497.0000 - val_loss: 1747657.7500\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2183592.5000 - val_loss: 1729969.1250\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2164461.5000 - val_loss: 1707476.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2140119.7500 - val_loss: 1679121.5000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2109412.5000 - val_loss: 1643701.1250\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2071014.2500 - val_loss: 1599881.5000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2023446.1250 - val_loss: 1546241.3750\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1965110.1250 - val_loss: 1481362.6250\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1894373.3750 - val_loss: 1403996.1250\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1809720.2500 - val_loss: 1313330.8750\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1710011.0000 - val_loss: 1209422.6250\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1594928.1250 - val_loss: 1093942.5000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1465713.2500 - val_loss: 971330.3750\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1326307.5000 - val_loss: 850611.1250\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1185172.1250 - val_loss: 748062.8750\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1058010.3750 - val_loss: 690270.8125\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 971069.6875 - val_loss: 712168.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 960145.0625 - val_loss: 824045.9375\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1040517.1875 - val_loss: 939407.2500\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1136019.6250 - val_loss: 961583.4375\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1153821.3750 - val_loss: 900922.9375\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1099955.0000 - val_loss: 809796.7500\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1021915.8750 - val_loss: 729602.6875\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 957427.3125 - val_loss: 678929.1250\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 922458.0000 - val_loss: 658012.3125\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 915512.1875 - val_loss: 657843.2500\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 926612.7500 - val_loss: 667831.4375\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 944782.4375 - val_loss: 679717.3125\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 961739.1250 - val_loss: 688468.9375\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 972603.3125 - val_loss: 691786.3750\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 975284.9375 - val_loss: 689333.3125\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 969659.5000 - val_loss: 682121.9375\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 956944.4375 - val_loss: 672093.6250\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 939303.4375 - val_loss: 661785.6250\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 919552.4375 - val_loss: 653934.2500\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 900818.6875 - val_loss: 650883.1250\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 886007.8750 - val_loss: 653758.0625\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 877025.6250 - val_loss: 661610.2500\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 873918.1875 - val_loss: 671054.5000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 874419.4375 - val_loss: 677058.9375\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 874544.1250 - val_loss: 674920.1250\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 870317.2500 - val_loss: 662322.5625\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 859671.7500 - val_loss: 640010.0625\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 843123.1250 - val_loss: 610721.6875\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 822808.3125 - val_loss: 577459.8750\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 800762.8750 - val_loss: 542471.4375\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 777569.1250 - val_loss: 507976.4375\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 751940.3125 - val_loss: 481578.5312\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 722724.9375 - val_loss: 500360.6250\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 713560.2500 - val_loss: 506857.4375\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 710459.1250 - val_loss: 444332.8125\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 649618.6250 - val_loss: 417476.9688\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 626001.9375 - val_loss: 420135.7812\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 621803.7500 - val_loss: 413891.1250\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 600876.4375 - val_loss: 380367.9375\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 545743.6250 - val_loss: 377183.7500\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 544376.6250 - val_loss: 357524.9688\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 501189.0938 - val_loss: 373951.9688\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 490949.0312 - val_loss: 392458.2500\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 491160.8125 - val_loss: 368774.8125\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 451654.0312 - val_loss: 387213.6562\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 482631.8750 - val_loss: 384963.8438\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 431601.6875 - val_loss: 427994.7188\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 452375.2188 - val_loss: 401156.4375\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 416392.9688 - val_loss: 380150.2500\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 419731.4688 - val_loss: 372185.5625\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 391316.7812 - val_loss: 399696.1875\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 391002.1875 - val_loss: 395983.0312\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 383461.8125 - val_loss: 362362.5938\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 366549.2812 - val_loss: 359854.6562\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 372743.1875 - val_loss: 365884.4062\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 352127.9688 - val_loss: 380606.4375\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 358034.7500 - val_loss: 357590.0312\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 340535.1250 - val_loss: 343704.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 343027.3125 - val_loss: 337234.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 331738.4375 - val_loss: 344216.0312\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 328633.4688 - val_loss: 338994.6562\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 325543.4062 - val_loss: 315854.9375\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 315957.0312 - val_loss: 307029.5312\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 318376.2500 - val_loss: 307327.8750\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 308427.3438 - val_loss: 312052.9375\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 310839.0312 - val_loss: 290702.5312\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 301556.1562 - val_loss: 280569.7188\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 304525.0312 - val_loss: 285166.1875\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 295948.9062 - val_loss: 288277.9062\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 296181.1562 - val_loss: 267418.6250\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 290024.7500 - val_loss: 263039.3750\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 287744.0625 - val_loss: 275867.4062\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 286243.3125 - val_loss: 264709.4688\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_8 (LSTM)               (None, 10)                1960      \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,865\n",
      "Trainable params: 182,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2139653.7500 - val_loss: 2812302.7500\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2138870.0000 - val_loss: 2811373.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2138070.5000 - val_loss: 2810230.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2137086.2500 - val_loss: 2808727.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2135791.2500 - val_loss: 2806713.5000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2134056.2500 - val_loss: 2804013.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2131728.5000 - val_loss: 2800413.7500\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2128625.2500 - val_loss: 2795653.7500\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2124521.2500 - val_loss: 2789412.7500\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2119140.7500 - val_loss: 2781299.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2112146.5000 - val_loss: 2770834.2500\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2103127.7500 - val_loss: 2757432.2500\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2091581.5000 - val_loss: 2740380.2500\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2076897.0000 - val_loss: 2718816.5000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2058339.0000 - val_loss: 2691715.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2035035.3750 - val_loss: 2657865.5000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2005963.7500 - val_loss: 2615857.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1969943.3750 - val_loss: 2564074.7500\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1925637.2500 - val_loss: 2500710.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1871574.2500 - val_loss: 2423806.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1806208.5000 - val_loss: 2331346.7500\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1728027.1250 - val_loss: 2221423.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1635729.2500 - val_loss: 2092568.2500\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1528571.5000 - val_loss: 1944301.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1406927.6250 - val_loss: 1777975.3750\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1273127.0000 - val_loss: 1598193.1250\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1132939.7500 - val_loss: 1414820.6250\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 997869.7500 - val_loss: 1244829.7500\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 887563.9375 - val_loss: 1109877.8750\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 827729.2500 - val_loss: 1054612.8750\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 854532.7500 - val_loss: 1114978.3750\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 964116.6250 - val_loss: 1145675.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1006889.6875 - val_loss: 1113720.5000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 960974.3125 - val_loss: 1062659.1250\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 880407.8750 - val_loss: 1030619.2500\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 811497.4375 - val_loss: 1031443.1250\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 774363.9375 - val_loss: 1059051.1250\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 767413.0000 - val_loss: 1098637.8750\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 778487.1250 - val_loss: 1135928.6250\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 794385.7500 - val_loss: 1161647.8750\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 805781.6250 - val_loss: 1179655.1250\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 814386.6250 - val_loss: 1169657.1250\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 803993.9375 - val_loss: 1140549.8750\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 780180.3125 - val_loss: 1096275.7500\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 745789.6875 - val_loss: 1039819.6250\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 702616.7500 - val_loss: 1086596.8750\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 743327.6250 - val_loss: 922014.5000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 621464.0625 - val_loss: 865616.8750\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 592622.1250 - val_loss: 847255.1250\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 597638.8125 - val_loss: 834043.2500\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 602600.1875 - val_loss: 804416.0625\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 582027.2500 - val_loss: 953017.1875\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 670416.1875 - val_loss: 848902.9375\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 605444.1250 - val_loss: 766061.3125\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 559598.7500 - val_loss: 746413.6250\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 541868.9375 - val_loss: 690431.9375\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 480993.5625 - val_loss: 726353.5000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 488077.5000 - val_loss: 750475.2500\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 500901.8438 - val_loss: 652871.5625\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 432946.3125 - val_loss: 646462.6875\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 431805.0938 - val_loss: 646016.5000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 428436.0000 - val_loss: 662584.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 432266.9062 - val_loss: 674340.3125\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 437242.5312 - val_loss: 629065.5000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 407829.0938 - val_loss: 610343.3125\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 397404.9062 - val_loss: 589867.7500\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 380517.8438 - val_loss: 566875.9375\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 358938.6562 - val_loss: 572848.3125\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 361600.7812 - val_loss: 573276.1250\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 363451.4375 - val_loss: 527905.7500\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 333704.8125 - val_loss: 515261.9375\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 331776.7188 - val_loss: 502287.2188\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 324182.6562 - val_loss: 482587.4688\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 302749.1562 - val_loss: 512022.3438\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 321290.3125 - val_loss: 457983.5625\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 289756.0938 - val_loss: 460451.0312\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 299906.8125 - val_loss: 437481.8438\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 275749.3438 - val_loss: 472869.4062\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 298982.6875 - val_loss: 426226.4375\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 273526.8750 - val_loss: 429649.4062\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 280885.2500 - val_loss: 422536.5938\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 264830.4375 - val_loss: 417009.1875\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 261348.2500 - val_loss: 412421.4688\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 269512.4375 - val_loss: 392211.6250\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 248495.3438 - val_loss: 425935.7812\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 270201.3125 - val_loss: 395212.7188\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 259110.7500 - val_loss: 379548.7500\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 245384.3594 - val_loss: 412176.7500\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 264169.0938 - val_loss: 365932.5312\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 235115.7969 - val_loss: 371109.5312\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 242508.2344 - val_loss: 366412.8750\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 231525.7344 - val_loss: 355311.5625\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 223688.2031 - val_loss: 353829.5938\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 228964.2812 - val_loss: 338936.6562\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 213405.8438 - val_loss: 357597.5625\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 224951.3750 - val_loss: 335584.5938\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 214467.5781 - val_loss: 326839.6562\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 207037.1406 - val_loss: 344061.8750\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 216011.4219 - val_loss: 317003.6562\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 200877.0469 - val_loss: 311847.8125\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_9 (LSTM)               (None, 10)                1960      \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_95 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_96 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,865\n",
      "Trainable params: 182,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2246900.7500 - val_loss: 1961384.8750\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2232268.5000 - val_loss: 1945762.8750\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2216269.2500 - val_loss: 1926561.2500\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2196596.2500 - val_loss: 1901943.5000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2171362.0000 - val_loss: 1870062.2500\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2138659.7500 - val_loss: 1828964.6250\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2096465.8750 - val_loss: 1776534.6250\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2042570.7500 - val_loss: 1710496.2500\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1974574.2500 - val_loss: 1628580.6250\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1890039.6250 - val_loss: 1528819.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1786767.0000 - val_loss: 1409998.5000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1663221.3750 - val_loss: 1272443.6250\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1519267.8750 - val_loss: 1119375.5000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1357468.3750 - val_loss: 959230.8125\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1185306.8750 - val_loss: 809403.6250\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1018814.8125 - val_loss: 701748.6875\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 888008.1250 - val_loss: 686697.0625\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 841524.8125 - val_loss: 806850.1875\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 924096.9375 - val_loss: 971654.9375\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1060481.6250 - val_loss: 1018285.8750\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1100620.7500 - val_loss: 948672.8750\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1041532.5625 - val_loss: 836806.3125\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 948793.8750 - val_loss: 740344.1250\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 873695.6875 - val_loss: 683101.5625\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 836133.5000 - val_loss: 662723.1250\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 831786.3750 - val_loss: 671322.8125\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 852666.1250 - val_loss: 684997.6875\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 873954.1875 - val_loss: 701505.8125\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 895842.6250 - val_loss: 711521.0625\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 908642.7500 - val_loss: 713183.5625\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 911009.3125 - val_loss: 706939.5000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 903681.8125 - val_loss: 694718.1250\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 888783.8125 - val_loss: 679499.6875\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 869448.1875 - val_loss: 664933.3125\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 849493.0625 - val_loss: 654784.6250\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 832933.6250 - val_loss: 652041.3750\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 823159.0000 - val_loss: 657699.5625\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 821774.0000 - val_loss: 669691.5000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 827522.9375 - val_loss: 682912.2500\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 836195.1250 - val_loss: 691115.6250\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 842277.3125 - val_loss: 690006.5000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 841783.2500 - val_loss: 679335.1250\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 834239.5000 - val_loss: 662387.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 822278.0625 - val_loss: 643489.3125\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 809359.6875 - val_loss: 625661.1250\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 797442.5000 - val_loss: 618794.8750\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 797399.0000 - val_loss: 608643.3125\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 790796.8750 - val_loss: 602144.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 789292.3750 - val_loss: 595749.3750\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 787216.3750 - val_loss: 585846.1250\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 781030.5000 - val_loss: 571316.1250\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 769753.1250 - val_loss: 551466.8125\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 752173.6250 - val_loss: 525710.8750\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 726221.5000 - val_loss: 494331.4062\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 689071.6250 - val_loss: 469884.9375\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 645168.1875 - val_loss: 538400.4375\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 683455.0625 - val_loss: 439428.5312\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 582146.8125 - val_loss: 385916.1562\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 543591.8750 - val_loss: 359041.4062\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 518562.7500 - val_loss: 334812.1875\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 473557.2500 - val_loss: 465393.3125\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 657591.2500 - val_loss: 344976.5938\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 480009.1562 - val_loss: 341167.4375\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 477252.7812 - val_loss: 337885.1562\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 469562.2812 - val_loss: 299818.4062\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 393999.3125 - val_loss: 457938.0625\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 530348.3750 - val_loss: 320226.8438\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 383942.9688 - val_loss: 296796.2188\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 397617.8438 - val_loss: 311012.7500\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 427930.3438 - val_loss: 257008.7969\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 342416.9062 - val_loss: 356352.1250\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 401971.5938 - val_loss: 358990.8750\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 389536.5000 - val_loss: 239050.9062\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 301070.1562 - val_loss: 242329.9844\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 345106.9688 - val_loss: 230455.3281\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 309213.9688 - val_loss: 287513.3438\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 319784.5312 - val_loss: 302677.3438\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 327003.5938 - val_loss: 239499.4375\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 296661.7812 - val_loss: 237336.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 317721.9375 - val_loss: 237230.0781\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 307150.2500 - val_loss: 258315.3281\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 301276.5938 - val_loss: 281398.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 311699.7812 - val_loss: 245654.4531\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 286863.8125 - val_loss: 226749.6562\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 289879.0000 - val_loss: 224606.2656\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 284086.0625 - val_loss: 248595.4375\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 281864.4375 - val_loss: 257312.3750\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 285605.9375 - val_loss: 221451.6250\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 275026.0938 - val_loss: 214932.2969\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 277732.8125 - val_loss: 226788.7500\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 263871.2500 - val_loss: 257455.2656\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 269221.5625 - val_loss: 230014.6094\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 257462.5469 - val_loss: 211583.8125\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 260759.1562 - val_loss: 214390.3281\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 254621.0312 - val_loss: 237724.0781\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 253890.7812 - val_loss: 228885.3594\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 247517.0938 - val_loss: 205213.6094\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 245020.0312 - val_loss: 206113.2812\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 239434.9688 - val_loss: 231080.8281\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 239843.0625 - val_loss: 208053.1562\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_10 (LSTM)              (None, 10)                1960      \n",
      "                                                                 \n",
      " dense_100 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_108 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,865\n",
      "Trainable params: 182,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2169350.7500 - val_loss: 2853761.7500\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2163764.7500 - val_loss: 2846881.2500\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2158181.0000 - val_loss: 2838510.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2151390.0000 - val_loss: 2827564.7500\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2142515.2500 - val_loss: 2812980.2500\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2130696.5000 - val_loss: 2793550.7500\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2114964.5000 - val_loss: 2767859.2500\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2094185.7500 - val_loss: 2734223.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2067021.2500 - val_loss: 2690660.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2031910.3750 - val_loss: 2634857.2500\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1987054.0000 - val_loss: 2564179.2500\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1930442.1250 - val_loss: 2475721.7500\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1859925.7500 - val_loss: 2366442.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1773372.3750 - val_loss: 2233422.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1668952.8750 - val_loss: 2074345.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1545649.8750 - val_loss: 1888356.2500\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1404131.8750 - val_loss: 1677505.7500\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1248205.0000 - val_loss: 1449189.2500\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1087204.2500 - val_loss: 1220257.3750\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 939851.7500 - val_loss: 1023186.8750\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 839535.5000 - val_loss: 910283.6250\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 834905.5000 - val_loss: 921038.0625\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 945363.3750 - val_loss: 977128.8750\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1055094.2500 - val_loss: 978533.8125\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1059346.2500 - val_loss: 937029.5625\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 987353.6250 - val_loss: 898836.5000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 900525.8750 - val_loss: 892470.1875\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 838653.2500 - val_loss: 921270.6250\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 813316.1875 - val_loss: 972792.7500\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 817429.9375 - val_loss: 1030665.6875\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 837353.9375 - val_loss: 1081855.6250\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 860843.8125 - val_loss: 1118630.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 879849.2500 - val_loss: 1137694.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 890303.0000 - val_loss: 1138723.8750\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 891026.3125 - val_loss: 1123256.6250\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 882763.3125 - val_loss: 1094083.5000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 867575.8125 - val_loss: 1054978.2500\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 848488.5000 - val_loss: 1010528.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 829181.1250 - val_loss: 965828.3125\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 813513.2500 - val_loss: 925831.9375\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 804699.9375 - val_loss: 894303.5000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 804160.6250 - val_loss: 872684.1250\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 810467.5000 - val_loss: 859670.1875\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 819325.9375 - val_loss: 852265.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 825339.2500 - val_loss: 847931.5000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 824867.0000 - val_loss: 846133.5000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 817857.8125 - val_loss: 848005.7500\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 807280.9375 - val_loss: 854730.1875\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 796935.1875 - val_loss: 866112.5000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 789412.1250 - val_loss: 880230.3125\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 785275.3750 - val_loss: 893988.2500\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 783363.9375 - val_loss: 903963.6875\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 781568.9375 - val_loss: 907041.9375\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 777493.8125 - val_loss: 900692.0625\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 768704.8750 - val_loss: 883003.5000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 752509.5000 - val_loss: 853248.4375\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 725833.2500 - val_loss: 824895.2500\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 694967.0000 - val_loss: 883514.2500\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 728282.4375 - val_loss: 726835.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 636549.5625 - val_loss: 648784.1250\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 604034.4375 - val_loss: 590590.6250\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 576252.5000 - val_loss: 543297.6875\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 535030.0625 - val_loss: 807872.6875\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 687073.5625 - val_loss: 529155.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 536477.8125 - val_loss: 465057.9375\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 547541.9375 - val_loss: 468393.3438\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 574118.0625 - val_loss: 428854.6875\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 511599.5312 - val_loss: 509624.6875\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 500809.8125 - val_loss: 523790.6562\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 489070.4062 - val_loss: 441118.9375\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 432993.1250 - val_loss: 386584.5312\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 402499.8438 - val_loss: 369134.0312\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 399745.1250 - val_loss: 363966.1250\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 396683.9062 - val_loss: 368831.8750\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 392956.9688 - val_loss: 406321.2188\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 416392.8750 - val_loss: 340124.1875\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 387605.9375 - val_loss: 342883.3125\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 403604.0625 - val_loss: 330693.8125\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 381694.5938 - val_loss: 377356.5312\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 402939.0625 - val_loss: 335356.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 376336.9375 - val_loss: 326737.8750\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 382471.3125 - val_loss: 321872.3750\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 375488.9375 - val_loss: 325505.7812\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 364912.0000 - val_loss: 338725.4375\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 371601.6562 - val_loss: 306615.5938\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 353609.2500 - val_loss: 305367.5625\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 361593.6250 - val_loss: 298318.8750\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 348084.1250 - val_loss: 311736.7188\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 350070.5938 - val_loss: 297132.6875\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 339355.6562 - val_loss: 286525.0938\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 336960.5000 - val_loss: 283284.4062\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 334499.9688 - val_loss: 286166.8750\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 328884.8125 - val_loss: 288314.9062\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 328178.8750 - val_loss: 271715.9375\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 319307.1562 - val_loss: 267586.4062\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 317548.7812 - val_loss: 263553.3438\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 308485.8750 - val_loss: 268253.7500\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 309805.9375 - val_loss: 252543.7344\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 300918.4375 - val_loss: 250106.6719\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 299397.6875 - val_loss: 247259.7812\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_11 (LSTM)              (None, 10)                1960      \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_115 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_116 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,865\n",
      "Trainable params: 182,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2231422.2500 - val_loss: 2970976.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2230033.7500 - val_loss: 2969143.2500\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2228451.2500 - val_loss: 2966809.7500\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2226436.7500 - val_loss: 2963724.7500\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2223774.0000 - val_loss: 2959620.5000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2220231.7500 - val_loss: 2954180.5000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2215537.2500 - val_loss: 2947021.7500\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2209362.0000 - val_loss: 2937683.2500\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2201308.2500 - val_loss: 2925602.2500\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2190894.2500 - val_loss: 2910100.7500\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2177538.5000 - val_loss: 2890363.2500\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2160545.0000 - val_loss: 2865408.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2139078.2500 - val_loss: 2834064.7500\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2112148.7500 - val_loss: 2794969.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2078608.3750 - val_loss: 2746557.5000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2037156.1250 - val_loss: 2687071.7500\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1986348.2500 - val_loss: 2614583.2500\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1924635.7500 - val_loss: 2527059.2500\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1850437.5000 - val_loss: 2422490.7500\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1762284.8750 - val_loss: 2299109.2500\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1659052.7500 - val_loss: 2155764.5000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1540353.0000 - val_loss: 1992541.7500\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1407168.6250 - val_loss: 1811773.7500\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1262868.7500 - val_loss: 1619615.5000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1114771.2500 - val_loss: 1428480.5000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 976487.3125 - val_loss: 1260463.7500\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 870988.1875 - val_loss: 1149981.6250\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 831931.6250 - val_loss: 1132983.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 888566.7500 - val_loss: 1190954.3750\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1003321.4375 - val_loss: 1231725.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1067250.2500 - val_loss: 1215652.7500\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1044692.1875 - val_loss: 1169814.5000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 973325.7500 - val_loss: 1130319.2500\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 898236.9375 - val_loss: 1117309.6250\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 846349.8125 - val_loss: 1132976.7500\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 824718.0000 - val_loss: 1168385.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 827497.5000 - val_loss: 1211398.8750\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 844116.8125 - val_loss: 1251762.2500\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 864778.6250 - val_loss: 1282827.5000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 882649.6875 - val_loss: 1301282.8750\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 893945.6250 - val_loss: 1306256.7500\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 897247.4375 - val_loss: 1298511.6250\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 892788.1250 - val_loss: 1279919.1250\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 881940.1250 - val_loss: 1253161.1250\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 866881.3750 - val_loss: 1221536.5000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 850335.3125 - val_loss: 1188730.7500\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 835259.5000 - val_loss: 1158428.3750\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 824371.3125 - val_loss: 1133713.1250\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 819485.0000 - val_loss: 1116335.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 820790.3750 - val_loss: 1106186.3750\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 826455.5625 - val_loss: 1101437.8750\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 833044.1250 - val_loss: 1099543.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 836879.6875 - val_loss: 1098617.3750\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 835715.3125 - val_loss: 1098245.3750\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 829612.5000 - val_loss: 1099193.8750\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 820490.2500 - val_loss: 1102389.8750\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 810809.4375 - val_loss: 1107933.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 802300.1875 - val_loss: 1114635.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 795311.9375 - val_loss: 1120079.6250\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 788795.8750 - val_loss: 1120959.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 780577.5000 - val_loss: 1113592.5000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 767585.4375 - val_loss: 1095667.1250\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 746923.4375 - val_loss: 1088591.5000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 735035.0000 - val_loss: 1080267.1250\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 729665.3750 - val_loss: 1028745.3125\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 698022.8750 - val_loss: 997084.2500\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 685326.0625 - val_loss: 969720.3125\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 676218.9375 - val_loss: 937953.4375\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 660531.6875 - val_loss: 900325.6250\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 633841.3750 - val_loss: 896775.5000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 625156.0625 - val_loss: 869598.5625\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 611220.1250 - val_loss: 828083.3750\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 597510.0625 - val_loss: 814084.8750\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 595298.7500 - val_loss: 793355.1875\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 571884.5000 - val_loss: 812597.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 567653.3125 - val_loss: 773594.3750\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 541626.6875 - val_loss: 752886.7500\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 530572.1250 - val_loss: 740957.9375\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 515071.5938 - val_loss: 732076.9375\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 495041.0000 - val_loss: 734962.5000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 490620.2500 - val_loss: 701797.5000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 475077.4375 - val_loss: 684561.8750\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 471872.2500 - val_loss: 671210.0625\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 466234.3438 - val_loss: 636238.7500\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 444083.8750 - val_loss: 680730.0625\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 469424.0938 - val_loss: 620741.4375\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 460608.7188 - val_loss: 636510.3750\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 484967.1875 - val_loss: 569857.3125\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 409789.2500 - val_loss: 602159.1250\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 415406.1562 - val_loss: 601729.5000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 415286.5000 - val_loss: 523416.4375\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 368114.2812 - val_loss: 546983.7500\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 404246.7500 - val_loss: 525831.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 382981.5938 - val_loss: 520762.5938\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 360097.5938 - val_loss: 567286.4375\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 389436.0312 - val_loss: 486846.6250\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 342849.3750 - val_loss: 486276.6250\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 356679.1562 - val_loss: 479623.1250\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 353368.1562 - val_loss: 484282.0312\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 347337.5938 - val_loss: 503014.7188\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 10)                2040      \n",
      "                                                                 \n",
      " dense_120 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_121 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_122 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_123 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_124 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_125 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_126 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,945\n",
      "Trainable params: 182,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 67068.0312 - val_loss: 1642252.6250\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 66680.9219 - val_loss: 1641849.6250\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 66231.5547 - val_loss: 1641373.1250\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 65659.1797 - val_loss: 1640853.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 64912.0352 - val_loss: 1640267.8750\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 63940.8203 - val_loss: 1639623.3750\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 62702.6406 - val_loss: 1638960.1250\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 61172.5195 - val_loss: 1638383.2500\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 59372.3555 - val_loss: 1638119.5000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 57443.3789 - val_loss: 1638639.8750\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 55746.4961 - val_loss: 1640740.7500\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 54646.6445 - val_loss: 1644543.8750\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 54560.5547 - val_loss: 1647833.7500\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 54881.7266 - val_loss: 1648202.3750\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 54418.2227 - val_loss: 1646354.1250\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 53251.0039 - val_loss: 1643901.1250\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 51956.8086 - val_loss: 1641937.5000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 50895.8516 - val_loss: 1640894.5000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 50437.7031 - val_loss: 1640473.5000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 50511.8516 - val_loss: 1640153.6250\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 50613.4609 - val_loss: 1639733.5000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 50556.3906 - val_loss: 1639147.3750\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 50327.1523 - val_loss: 1638387.2500\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 49995.0977 - val_loss: 1637518.8750\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 49713.4180 - val_loss: 1636669.8750\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 49576.4648 - val_loss: 1635870.1250\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 49388.7734 - val_loss: 1634937.7500\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 49070.3906 - val_loss: 1633692.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 48753.5586 - val_loss: 1632050.6250\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 48482.5508 - val_loss: 1630091.8750\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 48237.5742 - val_loss: 1627989.6250\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 47987.0703 - val_loss: 1625824.2500\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 47748.3789 - val_loss: 1623709.6250\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 47535.7617 - val_loss: 1621760.6250\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 47387.7852 - val_loss: 1620068.6250\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 47300.8789 - val_loss: 1618660.7500\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 47208.9336 - val_loss: 1617518.1250\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 47017.1367 - val_loss: 1616616.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 46752.5977 - val_loss: 1615947.2500\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 46513.3828 - val_loss: 1615450.8750\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 46335.7070 - val_loss: 1614888.8750\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 46178.9336 - val_loss: 1614226.6250\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 46010.5195 - val_loss: 1613742.1250\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 45834.0273 - val_loss: 1613394.6250\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 45662.1094 - val_loss: 1613238.6250\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 45482.8711 - val_loss: 1613444.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 45302.2734 - val_loss: 1613825.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 45122.6562 - val_loss: 1614200.5000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 44927.8516 - val_loss: 1614611.2500\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 44751.2227 - val_loss: 1615049.3750\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 44608.7266 - val_loss: 1615352.1250\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 44490.9102 - val_loss: 1615467.2500\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 44368.7188 - val_loss: 1615467.8750\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 44226.5508 - val_loss: 1615313.5000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 44075.1836 - val_loss: 1614909.5000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 43925.1875 - val_loss: 1614350.2500\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 43785.7500 - val_loss: 1613689.7500\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 43645.9219 - val_loss: 1612765.8750\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 43497.4023 - val_loss: 1611744.2500\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 43349.8516 - val_loss: 1610648.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 43211.0469 - val_loss: 1609338.3750\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 43079.3594 - val_loss: 1608176.6250\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 42956.6133 - val_loss: 1606777.6250\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 42817.9023 - val_loss: 1605669.1250\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 42683.5117 - val_loss: 1604683.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 42562.9336 - val_loss: 1603899.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 42456.5977 - val_loss: 1603545.3750\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 42359.7148 - val_loss: 1603259.1250\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 42230.6328 - val_loss: 1603248.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 42120.3242 - val_loss: 1603386.5000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 42037.1016 - val_loss: 1603467.2500\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 41944.1641 - val_loss: 1603612.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 41851.7539 - val_loss: 1603619.8750\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 41756.7422 - val_loss: 1603494.2500\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 41685.7031 - val_loss: 1603236.2500\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 41628.9297 - val_loss: 1602770.3750\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 41534.1875 - val_loss: 1602199.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 41448.8203 - val_loss: 1601582.3750\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 41386.4648 - val_loss: 1600992.7500\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 41310.9922 - val_loss: 1600501.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 41218.6367 - val_loss: 1600124.7500\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 41125.1406 - val_loss: 1599850.8750\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 41044.6758 - val_loss: 1599645.8750\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 40958.5938 - val_loss: 1599526.2500\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 40852.3203 - val_loss: 1599472.1250\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 40764.5820 - val_loss: 1599481.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 40674.1719 - val_loss: 1599542.8750\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 40558.9453 - val_loss: 1599552.2500\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 40436.5273 - val_loss: 1599503.7500\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 40319.4609 - val_loss: 1599390.6250\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 40182.4414 - val_loss: 1599147.5000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 40028.8008 - val_loss: 1598761.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 39854.4062 - val_loss: 1598260.8750\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 39642.3867 - val_loss: 1597658.5000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 39401.3828 - val_loss: 1597019.1250\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 39132.5391 - val_loss: 1596473.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 38839.7969 - val_loss: 1596122.1250\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 38559.2070 - val_loss: 1596141.6250\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 38343.3359 - val_loss: 1596278.6250\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 38172.7578 - val_loss: 1595636.5000\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_13 (LSTM)              (None, 10)                2040      \n",
      "                                                                 \n",
      " dense_130 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_131 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_132 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_133 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_134 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_135 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_136 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_137 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_138 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_139 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,945\n",
      "Trainable params: 182,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 243759.3438 - val_loss: 43635.3789\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 243682.5625 - val_loss: 43578.4570\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 243587.9219 - val_loss: 43503.8555\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 243463.7812 - val_loss: 43404.7227\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 243298.6406 - val_loss: 43272.7188\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 243078.7188 - val_loss: 43097.2305\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 242786.1719 - val_loss: 42865.5352\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 242399.0781 - val_loss: 42563.7305\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 241891.5781 - val_loss: 42178.7422\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 241234.7188 - val_loss: 41701.9805\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 240400.0312 - val_loss: 41130.4844\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 239368.9219 - val_loss: 40447.8008\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 238097.1094 - val_loss: 39639.6680\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 236523.2031 - val_loss: 38719.1328\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 234612.6406 - val_loss: 37740.0898\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 232375.9062 - val_loss: 36832.8398\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 229891.3281 - val_loss: 36233.9922\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 227294.2812 - val_loss: 36330.4219\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 224866.1875 - val_loss: 37736.5938\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 223145.2031 - val_loss: 41171.2852\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 222987.3125 - val_loss: 45978.8555\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 224559.6094 - val_loss: 48660.9414\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 225741.5469 - val_loss: 47912.5273\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 225218.7969 - val_loss: 45192.1523\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 223754.3438 - val_loss: 42063.1211\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 222350.8438 - val_loss: 39393.4219\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 221493.3750 - val_loss: 37436.4062\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 221185.7344 - val_loss: 36110.9219\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 221207.9531 - val_loss: 35230.7031\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 221324.0312 - val_loss: 34635.3281\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 221374.5156 - val_loss: 34235.1719\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 221281.8281 - val_loss: 34012.1055\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 221038.6562 - val_loss: 33984.3086\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 220696.2344 - val_loss: 34136.4453\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 220319.4844 - val_loss: 34384.8594\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 219892.8906 - val_loss: 34696.3398\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 219391.6406 - val_loss: 35112.9414\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 218937.1250 - val_loss: 35670.4883\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 218645.0469 - val_loss: 36358.2656\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 218523.3594 - val_loss: 36995.6094\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 218490.5469 - val_loss: 37319.1289\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 218417.9688 - val_loss: 37256.7969\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 218241.9062 - val_loss: 36866.1758\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 217994.6719 - val_loss: 36295.5312\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 217704.2812 - val_loss: 35695.3711\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 217409.7500 - val_loss: 35179.0820\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 217141.3750 - val_loss: 34815.3477\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 216917.9844 - val_loss: 34615.4453\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 216735.8750 - val_loss: 34543.1094\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 216567.2656 - val_loss: 34559.4414\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 216384.8906 - val_loss: 34646.7969\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 216171.6250 - val_loss: 34812.0273\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 215916.5625 - val_loss: 35088.2461\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 215627.1719 - val_loss: 35519.2227\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 215328.0000 - val_loss: 36122.0039\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 215036.6094 - val_loss: 36858.6836\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 214759.1875 - val_loss: 37638.9688\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 214495.9531 - val_loss: 38337.1719\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 214227.7500 - val_loss: 38870.8516\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 213934.4062 - val_loss: 39262.0352\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 213624.9219 - val_loss: 39548.6562\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 213290.0469 - val_loss: 39730.5469\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 212955.5156 - val_loss: 39762.5078\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 212605.7656 - val_loss: 39760.1953\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 212254.7656 - val_loss: 39827.3086\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 211881.2344 - val_loss: 39969.1914\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 211505.0469 - val_loss: 40230.5781\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 211109.0781 - val_loss: 40868.0469\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 210684.2812 - val_loss: 42071.5000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 210318.6562 - val_loss: 43283.8594\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 209880.3125 - val_loss: 44559.1211\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 209440.8281 - val_loss: 46057.7266\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 209085.0000 - val_loss: 47605.2148\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 208666.0312 - val_loss: 48998.2734\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 208244.1094 - val_loss: 50080.9180\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 207758.7656 - val_loss: 51085.7578\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 207292.2812 - val_loss: 52100.3438\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 206819.3594 - val_loss: 53089.6680\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 206347.4688 - val_loss: 54025.9297\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 205804.0469 - val_loss: 55255.1055\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 205289.9219 - val_loss: 56855.2773\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 204729.4531 - val_loss: 58809.2422\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 204174.1250 - val_loss: 60950.6211\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 203582.8125 - val_loss: 63403.8320\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 203014.2031 - val_loss: 66242.9297\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 202497.3906 - val_loss: 69357.2422\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 201709.3906 - val_loss: 72193.2031\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 201035.8438 - val_loss: 74723.3906\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 200416.5625 - val_loss: 77065.0547\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 199840.6406 - val_loss: 79615.5469\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 199643.2812 - val_loss: 83104.6641\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 198311.6562 - val_loss: 86539.9531\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 197874.0469 - val_loss: 87978.2578\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 197343.3125 - val_loss: 86928.1875\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 196396.3906 - val_loss: 85093.1016\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 195415.2656 - val_loss: 85679.3594\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 194745.6719 - val_loss: 88584.5391\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 193981.4531 - val_loss: 93432.0312\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 193210.1406 - val_loss: 99253.2969\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 192817.0312 - val_loss: 102274.4844\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_14 (LSTM)              (None, 10)                2040      \n",
      "                                                                 \n",
      " dense_140 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_141 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_142 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_143 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_144 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_145 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_146 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_147 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_148 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_149 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,945\n",
      "Trainable params: 182,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 240296.9062 - val_loss: 83501.1484\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 239997.2969 - val_loss: 83085.0625\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 239651.0625 - val_loss: 82555.0625\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 239208.2969 - val_loss: 81859.9297\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 238624.8438 - val_loss: 80946.6797\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 237855.3594 - val_loss: 79760.7969\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 236851.6875 - val_loss: 78253.4844\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 235559.7969 - val_loss: 76397.7656\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 233937.9844 - val_loss: 74177.5391\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 231989.5781 - val_loss: 71627.7344\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 229742.9375 - val_loss: 68890.6719\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 227322.6250 - val_loss: 66304.4609\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 225030.9375 - val_loss: 64518.9453\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 223455.6094 - val_loss: 64430.9062\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 223425.9688 - val_loss: 65885.3047\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 224733.2500 - val_loss: 66455.7344\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 225071.2500 - val_loss: 65485.1055\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 223928.7656 - val_loss: 63908.9648\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 222287.6875 - val_loss: 62346.7539\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 220921.4062 - val_loss: 63321.7773\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 220159.6406 - val_loss: 62368.4844\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 219982.5625 - val_loss: 62318.8711\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 219934.2812 - val_loss: 62342.9961\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 219791.7500 - val_loss: 62226.5664\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 219478.3438 - val_loss: 61926.3477\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 218966.2969 - val_loss: 61469.3242\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 218276.0156 - val_loss: 60922.0078\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 217476.6562 - val_loss: 60371.8828\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 216669.5156 - val_loss: 59914.3398\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 215947.9375 - val_loss: 59750.1016\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 215341.3906 - val_loss: 61322.4414\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 214696.8125 - val_loss: 67199.0156\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 214660.7656 - val_loss: 63105.1445\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 213875.8750 - val_loss: 60482.5312\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 213730.1875 - val_loss: 60333.1211\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 213162.7500 - val_loss: 62902.8203\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 212408.0625 - val_loss: 67047.6953\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 212485.9688 - val_loss: 64648.7148\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 211856.8438 - val_loss: 61277.6094\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 211653.3906 - val_loss: 59822.9336\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 211488.8438 - val_loss: 60367.0273\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 211012.0156 - val_loss: 63102.8516\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 210308.1406 - val_loss: 67113.7188\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 209937.0156 - val_loss: 68270.3594\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 209908.0938 - val_loss: 66841.2188\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 209132.5156 - val_loss: 65869.4531\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 209115.8125 - val_loss: 66719.1797\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 208987.8750 - val_loss: 68514.6953\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 208689.8906 - val_loss: 69474.9375\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 208568.7969 - val_loss: 69312.1484\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 208358.6875 - val_loss: 68150.9062\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 208061.1562 - val_loss: 66634.4453\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 207919.8906 - val_loss: 66157.6250\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 207746.9531 - val_loss: 67007.3438\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 207463.0781 - val_loss: 68323.0391\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 207169.0625 - val_loss: 69087.5938\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 206913.5625 - val_loss: 69231.4375\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 206604.5781 - val_loss: 69394.7188\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 206339.2344 - val_loss: 70101.9141\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 206218.8125 - val_loss: 72716.9062\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 206117.1094 - val_loss: 70140.5625\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 205834.2656 - val_loss: 69826.9141\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 205761.4219 - val_loss: 69582.5625\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 205576.3750 - val_loss: 69273.3672\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 205452.0312 - val_loss: 69275.9062\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 205213.4219 - val_loss: 69365.4453\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 204898.0000 - val_loss: 69240.5312\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 204655.7031 - val_loss: 69179.2734\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 204447.1406 - val_loss: 69535.1406\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 204230.0781 - val_loss: 70228.4375\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 203977.4375 - val_loss: 70995.8438\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 203715.6719 - val_loss: 71674.6953\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 203480.0625 - val_loss: 72252.2891\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 203232.4062 - val_loss: 72745.2578\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 202940.1094 - val_loss: 73071.3125\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 202627.6094 - val_loss: 73147.4922\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 202314.4844 - val_loss: 73050.0703\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 201991.2656 - val_loss: 72986.2656\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 201640.4219 - val_loss: 73230.7422\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 201278.1094 - val_loss: 74180.4844\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 200892.2188 - val_loss: 76635.7734\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 200446.5781 - val_loss: 81368.3438\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 200752.5469 - val_loss: 80107.7188\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 199781.5625 - val_loss: 77526.5703\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 200010.2344 - val_loss: 76836.0312\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 199654.9844 - val_loss: 75596.2031\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 199025.4531 - val_loss: 74793.5156\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 198800.2188 - val_loss: 75162.0312\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 198455.9375 - val_loss: 76461.7578\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 197738.8594 - val_loss: 78197.1797\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 197145.4219 - val_loss: 80430.3203\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 196774.0312 - val_loss: 85518.3516\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 197162.6094 - val_loss: 85030.3125\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 196244.2812 - val_loss: 82644.8828\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 196317.5938 - val_loss: 82245.7031\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 196069.5781 - val_loss: 81767.1328\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 195813.4531 - val_loss: 81779.1094\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 195749.2344 - val_loss: 82670.8125\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 195564.2031 - val_loss: 84334.9141\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 195169.8750 - val_loss: 86290.4531\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_15 (LSTM)              (None, 10)                2040      \n",
      "                                                                 \n",
      " dense_150 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_151 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_152 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_153 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_154 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_155 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_156 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_157 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_158 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_159 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,945\n",
      "Trainable params: 182,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 245866.5625 - val_loss: 43606.4180\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 245706.0469 - val_loss: 43445.7188\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 245525.6719 - val_loss: 43242.7656\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 245293.8438 - val_loss: 42975.7891\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 244983.3281 - val_loss: 42623.0938\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 244565.3125 - val_loss: 42162.6523\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 244007.9844 - val_loss: 41572.6367\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 243276.7812 - val_loss: 40833.9375\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 242335.6406 - val_loss: 39937.6562\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 241151.8281 - val_loss: 38905.8477\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 239710.0625 - val_loss: 37828.3984\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 238036.7500 - val_loss: 36851.0273\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 236207.5000 - val_loss: 36128.7656\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 234378.9688 - val_loss: 35950.2539\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 232748.1875 - val_loss: 36737.0078\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 231636.3125 - val_loss: 38583.3125\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 231226.6406 - val_loss: 40356.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 231016.0000 - val_loss: 40370.0234\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 230022.1875 - val_loss: 38676.2656\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 228353.3125 - val_loss: 36487.8125\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 226552.8906 - val_loss: 34694.7070\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 225168.6562 - val_loss: 33444.2266\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 224191.2031 - val_loss: 32626.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 223451.5938 - val_loss: 32095.0195\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 222769.0312 - val_loss: 31738.2910\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 222034.0156 - val_loss: 31491.3965\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 221221.2656 - val_loss: 31324.6934\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 220363.9531 - val_loss: 31216.4336\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 219507.2812 - val_loss: 31117.4902\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 218644.9531 - val_loss: 30931.6777\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 217724.2812 - val_loss: 30600.1621\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 216754.4531 - val_loss: 30183.6270\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 215833.7656 - val_loss: 29799.0898\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 215080.6562 - val_loss: 29490.4668\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 214535.4844 - val_loss: 29205.9062\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 214130.3438 - val_loss: 28894.7070\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 213774.3438 - val_loss: 28531.2773\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 213412.8438 - val_loss: 28112.4746\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 213030.5469 - val_loss: 27661.5430\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 212638.0781 - val_loss: 27216.9590\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 212255.2812 - val_loss: 26818.4238\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 211907.5156 - val_loss: 26510.0723\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 211619.2812 - val_loss: 26356.2148\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 211363.8594 - val_loss: 26405.4316\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 211046.3750 - val_loss: 26622.2637\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 210648.2812 - val_loss: 26932.2617\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 210231.0156 - val_loss: 27267.8281\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 209835.1719 - val_loss: 27579.9473\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 209463.7344 - val_loss: 27839.7910\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 209102.0625 - val_loss: 28037.0352\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 208733.0469 - val_loss: 28177.0879\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 208345.3125 - val_loss: 28276.7637\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 207934.5469 - val_loss: 28358.2168\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 207503.8906 - val_loss: 28441.8867\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 207061.8594 - val_loss: 28540.3516\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 206618.7344 - val_loss: 28655.6934\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 206182.0625 - val_loss: 28782.2812\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 205753.5156 - val_loss: 28914.0664\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 205329.7344 - val_loss: 29051.9199\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 204905.1719 - val_loss: 29205.5879\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 204474.9844 - val_loss: 29388.5039\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 204036.6562 - val_loss: 29608.7969\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 203590.5000 - val_loss: 29862.3887\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 203139.2500 - val_loss: 30132.4160\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 202686.6719 - val_loss: 30396.7871\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 202233.7812 - val_loss: 30640.7598\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 201777.7031 - val_loss: 30861.1270\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 201317.2969 - val_loss: 31051.6035\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 200858.0938 - val_loss: 31185.2129\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 200402.2812 - val_loss: 31220.8301\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 199938.6250 - val_loss: 31141.0605\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 199453.5469 - val_loss: 30984.2422\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 198949.3594 - val_loss: 30828.6504\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 198438.9688 - val_loss: 30742.6621\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 197925.1406 - val_loss: 30749.8223\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 197395.8906 - val_loss: 30839.2168\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 196843.8281 - val_loss: 30989.4746\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 196271.0000 - val_loss: 31181.5039\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 195669.5000 - val_loss: 31432.0098\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 195019.2188 - val_loss: 31806.1270\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 194309.5781 - val_loss: 32354.2598\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 193540.8906 - val_loss: 33028.5820\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 192691.7031 - val_loss: 33782.2656\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 191753.6250 - val_loss: 34509.1875\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 190725.3281 - val_loss: 34319.6484\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 189614.5312 - val_loss: 33527.3555\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 188535.3438 - val_loss: 35709.8828\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 187495.3438 - val_loss: 35169.0391\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 186304.5781 - val_loss: 35845.4766\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 185078.3750 - val_loss: 38456.5117\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 183791.4219 - val_loss: 38390.2188\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 182517.5156 - val_loss: 37123.2695\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 181183.9531 - val_loss: 37632.6289\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 179671.4219 - val_loss: 36504.6250\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 178110.7812 - val_loss: 34763.5547\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 176487.8594 - val_loss: 35005.5078\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 174752.0312 - val_loss: 35520.9961\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 172927.4688 - val_loss: 35308.9766\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 170933.7656 - val_loss: 38594.7109\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 168948.9062 - val_loss: 41955.0898\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_16 (LSTM)              (None, 10)                2040      \n",
      "                                                                 \n",
      " dense_160 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_161 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_162 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_163 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_164 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_165 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_166 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_167 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_168 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_169 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,945\n",
      "Trainable params: 182,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 247062.7344 - val_loss: 51579.0078\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 246639.3906 - val_loss: 51142.6992\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 246181.0781 - val_loss: 50612.6367\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 245618.2188 - val_loss: 49942.5156\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 244898.8438 - val_loss: 49090.0195\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 243973.6094 - val_loss: 48017.5664\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 242795.5000 - val_loss: 46700.5742\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 241327.8438 - val_loss: 45144.1055\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 239560.7188 - val_loss: 43412.0195\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 237539.3438 - val_loss: 41672.1445\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 235412.4531 - val_loss: 40229.8203\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 233487.0000 - val_loss: 39530.5117\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 232253.3125 - val_loss: 40061.1406\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 232267.1250 - val_loss: 41186.4922\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 233079.1875 - val_loss: 41179.1523\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 233079.6094 - val_loss: 39917.4961\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 232082.1562 - val_loss: 38243.0195\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 230828.7969 - val_loss: 36841.3203\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 229904.0781 - val_loss: 35957.0391\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 229468.2188 - val_loss: 35500.2188\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 229380.4531 - val_loss: 35259.0430\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 229413.9375 - val_loss: 35047.2227\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 229393.5000 - val_loss: 34759.8906\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 229236.3125 - val_loss: 34341.2656\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 228892.2656 - val_loss: 33872.3359\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 228296.0625 - val_loss: 33384.4727\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 227606.9219 - val_loss: 32942.5234\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 226935.4062 - val_loss: 32611.9160\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 226364.7969 - val_loss: 32410.5898\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 225939.6094 - val_loss: 32255.0762\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 225612.0781 - val_loss: 31961.7227\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 225226.6094 - val_loss: 31490.1602\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 224622.6719 - val_loss: 31263.1836\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 224244.4531 - val_loss: 30620.1230\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 223680.2969 - val_loss: 29430.6738\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 223172.2500 - val_loss: 28734.9570\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 222537.1406 - val_loss: 27648.4004\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 221838.9219 - val_loss: 27883.4004\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 220456.6875 - val_loss: 27313.1406\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 219319.8125 - val_loss: 27062.0371\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 218675.7812 - val_loss: 27748.6855\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 218623.3750 - val_loss: 29021.2949\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 218936.0469 - val_loss: 29302.1719\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 219058.3750 - val_loss: 28583.3496\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 218608.5469 - val_loss: 27134.4199\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 217958.6406 - val_loss: 26182.3203\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 217531.4531 - val_loss: 26224.3223\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 217268.5781 - val_loss: 26702.9531\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 217030.7188 - val_loss: 26979.0586\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 216615.3750 - val_loss: 27056.9902\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 216013.8750 - val_loss: 27341.1367\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 215426.3281 - val_loss: 28026.2305\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 214996.3906 - val_loss: 28689.9805\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 214793.1875 - val_loss: 29004.0762\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 214586.0938 - val_loss: 28865.9766\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 214181.2344 - val_loss: 28476.3730\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 213677.5625 - val_loss: 28460.7266\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 213265.7812 - val_loss: 28761.2305\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 213125.4531 - val_loss: 28310.1660\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 212640.6875 - val_loss: 27769.8340\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 212039.9375 - val_loss: 27405.4551\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 211524.4531 - val_loss: 27541.2344\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 211164.5625 - val_loss: 27613.1445\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 210766.7344 - val_loss: 27629.5703\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 210383.7656 - val_loss: 27896.0996\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 210029.7969 - val_loss: 28498.0039\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 209553.1719 - val_loss: 28962.5117\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 208912.7500 - val_loss: 29365.7070\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 208284.8906 - val_loss: 30118.3828\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 207901.4531 - val_loss: 31048.3496\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 207249.5312 - val_loss: 31991.4219\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 206733.8438 - val_loss: 32294.6914\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 206174.4531 - val_loss: 33037.9805\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 205505.9688 - val_loss: 33839.7188\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 204886.2344 - val_loss: 34081.5859\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 204153.0312 - val_loss: 34274.4570\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 203204.0938 - val_loss: 35422.6875\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 202085.0781 - val_loss: 36226.0430\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 201125.8750 - val_loss: 37089.0273\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 200196.6250 - val_loss: 38531.4492\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 199195.3125 - val_loss: 39973.2344\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 198100.0781 - val_loss: 41676.3789\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 196945.9688 - val_loss: 43107.2891\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 195664.6562 - val_loss: 43309.6953\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 194265.7500 - val_loss: 43186.0938\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 192778.3906 - val_loss: 45585.2969\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 191351.4219 - val_loss: 44465.2109\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 190309.9844 - val_loss: 42981.5391\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 189852.6406 - val_loss: 41725.0977\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 187455.7500 - val_loss: 47724.7266\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 186778.5156 - val_loss: 54822.4844\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 187227.5000 - val_loss: 44685.3867\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 184098.2812 - val_loss: 44772.5625\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 183607.8438 - val_loss: 47530.8555\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 180753.5312 - val_loss: 51007.5000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 180360.1094 - val_loss: 45655.2344\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 177783.6094 - val_loss: 41581.7539\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 176356.7812 - val_loss: 41017.3008\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 174251.5781 - val_loss: 43366.9492\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 172518.1875 - val_loss: 43432.1523\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_17 (LSTM)              (None, 10)                2040      \n",
      "                                                                 \n",
      " dense_170 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_171 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_172 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_173 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_174 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_175 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_176 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_177 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_178 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_179 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,945\n",
      "Trainable params: 182,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 251146.9688 - val_loss: 82955.6875\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 250920.9375 - val_loss: 82450.5312\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 250656.5312 - val_loss: 81805.5703\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 250317.9844 - val_loss: 80958.7188\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 249872.8906 - val_loss: 79842.7969\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 249287.5156 - val_loss: 78382.4531\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 248525.3438 - val_loss: 76492.4609\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 247548.1250 - val_loss: 74080.4688\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 246322.2344 - val_loss: 71056.3750\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 244830.1562 - val_loss: 67352.5078\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 243092.1562 - val_loss: 62963.7148\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 241205.4531 - val_loss: 58028.0781\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 239410.8594 - val_loss: 52985.7930\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 238188.3750 - val_loss: 48822.4023\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 238178.9062 - val_loss: 46660.2148\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 239158.0469 - val_loss: 46119.0117\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 239382.6250 - val_loss: 46265.0625\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 238459.4844 - val_loss: 47000.6016\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 237149.5312 - val_loss: 48333.5234\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 236097.4844 - val_loss: 50033.5312\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 235513.6719 - val_loss: 51730.2109\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 235285.4062 - val_loss: 53105.0312\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 235193.7188 - val_loss: 53979.3242\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 235057.8438 - val_loss: 54298.1797\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 234776.3750 - val_loss: 54086.5508\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 234318.2812 - val_loss: 53422.4453\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 233706.9688 - val_loss: 52433.4648\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 233012.0625 - val_loss: 51305.7930\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 232341.4219 - val_loss: 50282.7109\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 231813.7656 - val_loss: 49614.0312\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 231487.3125 - val_loss: 49442.1680\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 231280.0312 - val_loss: 49739.1328\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 231014.7031 - val_loss: 50400.4570\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 230585.6094 - val_loss: 51352.3633\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 230052.6719 - val_loss: 52527.1172\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 229557.4531 - val_loss: 53817.6289\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 229185.9375 - val_loss: 55072.7617\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 228922.9062 - val_loss: 56134.5586\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 228690.1250 - val_loss: 56902.1094\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 228413.5781 - val_loss: 57357.7734\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 228065.1719 - val_loss: 57551.8320\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 227664.4844 - val_loss: 57567.7070\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 227250.0156 - val_loss: 57488.6211\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 226846.0469 - val_loss: 57378.4844\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 226450.3906 - val_loss: 57274.1953\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 226047.0781 - val_loss: 57179.7344\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 225625.8125 - val_loss: 57063.5547\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 225184.4375 - val_loss: 56887.5312\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 224725.6875 - val_loss: 56650.9414\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 224265.8125 - val_loss: 56378.1875\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 223823.9688 - val_loss: 56063.7539\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 223397.4688 - val_loss: 55641.8984\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 222946.4688 - val_loss: 55122.2500\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 222449.2188 - val_loss: 54647.0508\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 221943.4531 - val_loss: 54351.7109\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 221454.1406 - val_loss: 54265.3789\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 220940.3906 - val_loss: 54343.3633\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 220353.2031 - val_loss: 54569.3984\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 219706.7344 - val_loss: 54944.4102\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 219043.3438 - val_loss: 55386.4805\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 218348.5156 - val_loss: 55806.5703\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 217567.2500 - val_loss: 56301.2852\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 216711.5000 - val_loss: 57076.8438\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 215820.2812 - val_loss: 58277.5195\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 214874.9062 - val_loss: 59911.8672\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 213834.7969 - val_loss: 61930.9180\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 212708.3906 - val_loss: 64181.7539\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 211486.6094 - val_loss: 66554.8906\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 210070.5938 - val_loss: 70157.3594\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 208151.1406 - val_loss: 73216.4922\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 205532.9062 - val_loss: 76136.4375\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 202849.4688 - val_loss: 79166.7656\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 200730.0156 - val_loss: 82078.7578\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 198426.2031 - val_loss: 84890.7578\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 195511.8750 - val_loss: 87426.6094\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 191935.4531 - val_loss: 88734.5312\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 187684.6094 - val_loss: 90074.2969\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 182924.8594 - val_loss: 94577.7031\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 177867.5000 - val_loss: 100006.6250\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 172162.5781 - val_loss: 113434.6953\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 165301.8281 - val_loss: 99789.7656\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 158337.6250 - val_loss: 143777.4062\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 152594.5156 - val_loss: 120858.4688\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 144663.3438 - val_loss: 104686.5078\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 140960.6406 - val_loss: 165107.0625\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 140647.9219 - val_loss: 94480.1953\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 128199.1719 - val_loss: 78864.3906\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 126081.6719 - val_loss: 115347.6953\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 120575.9141 - val_loss: 89835.5234\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 112308.4688 - val_loss: 65465.5820\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 112170.5703 - val_loss: 84740.2578\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 103041.5234 - val_loss: 86417.3125\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 97416.2422 - val_loss: 65415.4609\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 95986.7188 - val_loss: 69408.1641\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 85947.1172 - val_loss: 82803.0781\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 84763.4141 - val_loss: 67111.6484\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 79567.1641 - val_loss: 57308.4375\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 75722.2500 - val_loss: 66378.8750\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 71579.4141 - val_loss: 74784.5859\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 69136.1016 - val_loss: 58981.8203\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for t in target:\n",
    "    for l in location:\n",
    "        df = pd.read_csv(r\"{}/Model/{}/{}.csv\".format(path, t, l))\n",
    "        try:\n",
    "            for x in df.columns:\n",
    "                if len(df[x].unique()) == 1:\n",
    "                    df.drop(x, 1, inplace=True)\n",
    "                elif x != t:\n",
    "                    df[x] = MinMaxScaler().fit_transform(df[[x]])\n",
    "\n",
    "            train_size = round(len(df) * 0.8) # 計算訓練資料集應該有幾筆\n",
    "            train = df.iloc[:train_size, :] # 訓練資料集\n",
    "            test = df.iloc[train_size:, :] # 測試資料集\n",
    "\n",
    "            for e in [100, 300, 500, 1000, 2000]: # epochs\n",
    "                for d in [2, 5, 7, 10, 14, 30]: # pastDays : 囊括過去幾天的資料一起訓練\n",
    "\n",
    "                    X_train, Y_train = buildTrain(train, t, d, 1)\n",
    "\n",
    "                    ### shuffle the data, and random seed is 10\n",
    "                    X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "                    ### split training data and validation data\n",
    "                    X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1) # X_train : 訓練集的特徵變數, Y_train : 訓練集的目標變數, X_val : 驗證集的特徵變數, Y_val : 驗證集的目標變數\n",
    "\n",
    "                    Y_train = Y_train[:, np.newaxis]\n",
    "                    Y_val = Y_val[:, np.newaxis]\n",
    "\n",
    "                    model = buildOneToOneModel(X_train.shape)\n",
    "                    model.fit(X_train, Y_train, epochs=e, batch_size=512, validation_data=(X_val, Y_val))\n",
    "\n",
    "                    X_test, Y_test = buildTrain(test, t, d, 1) # X_test : 測試集的特徵變數, Y_test : 測試集的目標變數\n",
    "\n",
    "                    predict_test = model.predict(X_test) # 預測結果\n",
    "\n",
    "                    Y_test = Y_test.reshape(-1).tolist()\n",
    "                    predict_test = predict_test.reshape(-1).tolist()\n",
    "\n",
    "                    mae = mean_absolute_error(Y_test, predict_test) # MAE\n",
    "                    mse = mean_squared_error(Y_test, predict_test) # MSE\n",
    "                    data.append((t, l, e, d, mae, mse))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "result = pd.DataFrame(data, columns=['target', 'location', 'epochs', 'days', 'mae', 'mse'])\n",
    "result.to_excel(f'{path}/Model/LSTM_test.xlsx', index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iN2uW1KNLWc_"
   },
   "source": [
    "### 單特徵之LSTM實驗過程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1643302180012,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "MulI8fGXLYG1"
   },
   "outputs": [],
   "source": [
    "def buildTrain(train, target, pastDay=7, futureDay=1):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureDay-pastDay):\n",
    "        X_train.append(np.array(train.iloc[i:i+pastDay][[target]]))\n",
    "        Y_train.append(np.array(train.iloc[i+pastDay:i+pastDay+futureDay][target]))\n",
    "    return np.array(X_train), np.array(Y_train)\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    np.random.seed(10)\n",
    "    randomList = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randomList)\n",
    "    return X[randomList], Y[randomList]\n",
    "\n",
    "def splitData(X, Y, rate):\n",
    "    X_train = X[int(X.shape[0]*rate):]\n",
    "    Y_train = Y[int(Y.shape[0]*rate):]\n",
    "    X_val = X[:int(X.shape[0]*rate)]\n",
    "    Y_val = Y[:int(Y.shape[0]*rate)]\n",
    "    return X_train, Y_train, X_val, Y_val\n",
    "\n",
    "def buildOneToOneModel(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, input_length=shape[1], input_dim=shape[2], return_sequences=False))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(8))\n",
    "    model.add(Dense(4))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 179408,
     "status": "ok",
     "timestamp": 1643302382919,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "Jrlydm8-LeRs",
    "outputId": "ab956575-02c4-4468-cd57-20fff36d1746"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_18 (LSTM)              (None, 10)                480       \n",
      "                                                                 \n",
      " dense_180 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_181 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_182 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_183 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_184 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_185 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_186 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_187 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_188 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_189 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 7725186048.0000 - val_loss: 10201200640.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7724936704.0000 - val_loss: 10200846336.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7724654080.0000 - val_loss: 10200391680.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7724293632.0000 - val_loss: 10199787520.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7723813888.0000 - val_loss: 10198975488.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7723168256.0000 - val_loss: 10197891072.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7722307072.0000 - val_loss: 10196458496.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7721169408.0000 - val_loss: 10194585600.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7719682048.0000 - val_loss: 10192160768.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7717755392.0000 - val_loss: 10189041664.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7715277824.0000 - val_loss: 10185059328.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7712114688.0000 - val_loss: 10180011008.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7708106752.0000 - val_loss: 10173651968.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7703056384.0000 - val_loss: 10165679104.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7696724992.0000 - val_loss: 10155732992.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7688829440.0000 - val_loss: 10143377408.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7679021568.0000 - val_loss: 10128089088.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7666887680.0000 - val_loss: 10109242368.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7651935744.0000 - val_loss: 10086094848.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7633575936.0000 - val_loss: 10057768960.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7611118592.0000 - val_loss: 10023234560.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7583753728.0000 - val_loss: 9981277184.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7550526464.0000 - val_loss: 9930485760.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7510333440.0000 - val_loss: 9869216768.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7461893120.0000 - val_loss: 9795566592.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7403731456.0000 - val_loss: 9707358208.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7334167040.0000 - val_loss: 9602107392.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7251303936.0000 - val_loss: 9477024768.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7153031680.0000 - val_loss: 9329007616.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7037037056.0000 - val_loss: 9154660352.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 6900839424.0000 - val_loss: 8950360064.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6741863936.0000 - val_loss: 8712364032.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6557572608.0000 - val_loss: 8436995584.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6345647616.0000 - val_loss: 8120925696.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6104296960.0000 - val_loss: 7761618944.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5832691200.0000 - val_loss: 7357988864.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5531618304.0000 - val_loss: 6911367168.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 5204423680.0000 - val_loss: 6426932224.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4858365952.0000 - val_loss: 5915738112.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4506499072.0000 - val_loss: 5397559296.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 4170204160.0000 - val_loss: 4904541184.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3882136320.0000 - val_loss: 4484675072.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3688093184.0000 - val_loss: 4199817728.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3641247488.0000 - val_loss: 4100507904.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3769639168.0000 - val_loss: 4157762816.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4004075520.0000 - val_loss: 4241004800.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4180112640.0000 - val_loss: 4252382464.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4200979456.0000 - val_loss: 4197928448.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4093562880.0000 - val_loss: 4130463488.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3932402432.0000 - val_loss: 4096459776.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3782122240.0000 - val_loss: 4117119232.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3678628096.0000 - val_loss: 4189572608.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3630504448.0000 - val_loss: 4296341504.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3628414208.0000 - val_loss: 4415553536.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3655472640.0000 - val_loss: 4527940608.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3694769408.0000 - val_loss: 4619838464.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3733131264.0000 - val_loss: 4683392000.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3762059776.0000 - val_loss: 4715499520.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3777225728.0000 - val_loss: 4716530176.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3777528576.0000 - val_loss: 4689286656.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3764220160.0000 - val_loss: 4638282240.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3740209920.0000 - val_loss: 4569249280.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3709534720.0000 - val_loss: 4488732160.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3676861440.0000 - val_loss: 4403650560.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3646939136.0000 - val_loss: 4320718848.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3623913216.0000 - val_loss: 4245721856.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 3610523136.0000 - val_loss: 4182761984.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3607327488.0000 - val_loss: 4133710592.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3612242944.0000 - val_loss: 4098187776.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3620739328.0000 - val_loss: 4074147328.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3626371584.0000 - val_loss: 3929063168.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3423934720.0000 - val_loss: 4811538432.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3828375296.0000 - val_loss: 4756282368.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3797752320.0000 - val_loss: 4672249344.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3756424704.0000 - val_loss: 4570851328.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3711538176.0000 - val_loss: 4463815168.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3671128832.0000 - val_loss: 4362376192.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3642359552.0000 - val_loss: 4275952640.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3629825536.0000 - val_loss: 4210399232.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3633789184.0000 - val_loss: 4166678272.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 3649297920.0000 - val_loss: 4141160448.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3667466752.0000 - val_loss: 4128122112.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3679178240.0000 - val_loss: 4123138816.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3679219712.0000 - val_loss: 4124944640.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3668089344.0000 - val_loss: 4134735360.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3650650368.0000 - val_loss: 4153967872.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3633089024.0000 - val_loss: 4182393344.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3620167936.0000 - val_loss: 4217342976.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3613886464.0000 - val_loss: 4254237952.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3613592064.0000 - val_loss: 4287776512.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3616953856.0000 - val_loss: 4313140224.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3621136640.0000 - val_loss: 4326830080.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3623724544.0000 - val_loss: 4327043072.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3623230464.0000 - val_loss: 4313660928.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3619213568.0000 - val_loss: 4288016896.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3612092928.0000 - val_loss: 4252592128.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3602757888.0000 - val_loss: 4210684928.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3591910656.0000 - val_loss: 4490829312.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3694728192.0000 - val_loss: 4405428736.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3645029888.0000 - val_loss: 4302514688.0000\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_19 (LSTM)              (None, 10)                480       \n",
      "                                                                 \n",
      " dense_190 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_191 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_192 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_193 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_194 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_195 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_196 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_197 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_198 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_199 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 5s 5s/step - loss: 8384316416.0000 - val_loss: 4649069056.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8384067072.0000 - val_loss: 4648839680.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8383773696.0000 - val_loss: 4648549376.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8383403520.0000 - val_loss: 4648167936.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8382916096.0000 - val_loss: 4647663616.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8382272512.0000 - val_loss: 4646997504.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8381422080.0000 - val_loss: 4646125056.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8380308480.0000 - val_loss: 4644992000.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8378862592.0000 - val_loss: 4643533312.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 8377000960.0000 - val_loss: 4641669120.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8374620160.0000 - val_loss: 4639302144.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8371599360.0000 - val_loss: 4636314624.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8367784448.0000 - val_loss: 4632564224.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8362995712.0000 - val_loss: 4627879424.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8357010432.0000 - val_loss: 4622051328.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8349564416.0000 - val_loss: 4614831104.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8340338176.0000 - val_loss: 4605922816.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8328950272.0000 - val_loss: 4594973184.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8314943488.0000 - val_loss: 4581562368.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8297781760.0000 - val_loss: 4565196288.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8276822016.0000 - val_loss: 4545293824.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 8251311616.0000 - val_loss: 4521175040.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8220364288.0000 - val_loss: 4492047872.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8182941696.0000 - val_loss: 4456997888.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 8137837056.0000 - val_loss: 4414976512.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 8083657728.0000 - val_loss: 4364796416.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8018802176.0000 - val_loss: 4305124352.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7941454336.0000 - val_loss: 4234489344.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7849565184.0000 - val_loss: 4151298304.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7740860928.0000 - val_loss: 4053871360.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7612853760.0000 - val_loss: 3940506368.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7462888960.0000 - val_loss: 3809584128.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7288211456.0000 - val_loss: 3659726336.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7086102016.0000 - val_loss: 3490044928.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6854079488.0000 - val_loss: 3300497920.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6590204928.0000 - val_loss: 3092410880.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6293550592.0000 - val_loss: 2869228288.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5964881920.0000 - val_loss: 2637567232.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5607630336.0000 - val_loss: 2408689664.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5229303296.0000 - val_loss: 2200485376.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4843431936.0000 - val_loss: 2039975424.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4472179200.0000 - val_loss: 1965880448.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4149442304.0000 - val_loss: 2028993408.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3922994176.0000 - val_loss: 2282087680.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3849641984.0000 - val_loss: 2736766720.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3964969216.0000 - val_loss: 3273826048.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4209569792.0000 - val_loss: 3649019648.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4410887168.0000 - val_loss: 3713844992.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4445351424.0000 - val_loss: 2691351552.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4888471040.0000 - val_loss: 2093555200.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4429147648.0000 - val_loss: 2065393536.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4403856384.0000 - val_loss: 2038289792.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4348063232.0000 - val_loss: 2012346880.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4267379968.0000 - val_loss: 1992302336.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4170258432.0000 - val_loss: 1985578752.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4066231296.0000 - val_loss: 1998499328.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3964857088.0000 - val_loss: 2021174784.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3866987776.0000 - val_loss: 4409225728.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8027158528.0000 - val_loss: 4223536640.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7795332096.0000 - val_loss: 3998070016.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7500161024.0000 - val_loss: 3739878400.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7156577280.0000 - val_loss: 3459842560.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6775657984.0000 - val_loss: 3167929856.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6366373376.0000 - val_loss: 2874333696.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 5937232384.0000 - val_loss: 2591313152.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5498170880.0000 - val_loss: 2335170304.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5062682112.0000 - val_loss: 2128019712.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4649972736.0000 - val_loss: 1998466304.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4286519552.0000 - val_loss: 1979269120.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 4005686528.0000 - val_loss: 2098527616.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3842827008.0000 - val_loss: 2361909248.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3821528832.0000 - val_loss: 2732284416.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3916715008.0000 - val_loss: 4770369536.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8490179072.0000 - val_loss: 4397495808.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8006983168.0000 - val_loss: 3976655872.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7460758528.0000 - val_loss: 3546164480.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 6881668096.0000 - val_loss: 3127117568.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 6292235776.0000 - val_loss: 2739658752.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5712268288.0000 - val_loss: 2406154752.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5162522112.0000 - val_loss: 2153954560.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4668060672.0000 - val_loss: 2015311744.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4259835136.0000 - val_loss: 2023530496.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3972958720.0000 - val_loss: 2201958144.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3838921472.0000 - val_loss: 2540348672.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3868662528.0000 - val_loss: 2965249792.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4029173248.0000 - val_loss: 3352992000.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4226655232.0000 - val_loss: 7537603072.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 12042582016.0000 - val_loss: 3987539200.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4579845120.0000 - val_loss: 4109291008.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4674636288.0000 - val_loss: 3969819904.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4585265152.0000 - val_loss: 3657615104.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4394309120.0000 - val_loss: 3275678720.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 4180064256.0000 - val_loss: 2905771008.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3998370048.0000 - val_loss: 2594513152.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3875458304.0000 - val_loss: 2357716736.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3813684992.0000 - val_loss: 2191447808.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3801656832.0000 - val_loss: 2082531968.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3823085056.0000 - val_loss: 2015696768.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3862333952.0000 - val_loss: 1977321984.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3906926336.0000 - val_loss: 1956820352.0000\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_20 (LSTM)              (None, 10)                480       \n",
      "                                                                 \n",
      " dense_200 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_201 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_202 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_203 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_204 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_205 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_206 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_207 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_208 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_209 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 5s 5s/step - loss: 7833391104.0000 - val_loss: 9939090432.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 7833351680.0000 - val_loss: 9939038208.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 7833307648.0000 - val_loss: 9938972672.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 7833248768.0000 - val_loss: 9938885632.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 7833174528.0000 - val_loss: 9938770944.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 7833074176.0000 - val_loss: 9938620416.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7832945664.0000 - val_loss: 9938425856.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7832775680.0000 - val_loss: 9938168832.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7832555008.0000 - val_loss: 9937841152.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7832270848.0000 - val_loss: 9937421312.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7831907328.0000 - val_loss: 9936887808.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 7831443968.0000 - val_loss: 9936211968.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7830859776.0000 - val_loss: 9935358976.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7830121472.0000 - val_loss: 9934290944.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 7829197312.0000 - val_loss: 9932957696.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 7828043776.0000 - val_loss: 9931299840.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 7826608128.0000 - val_loss: 9929245696.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 7824830464.0000 - val_loss: 9926709248.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 7822635008.0000 - val_loss: 9923587072.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 7819932672.0000 - val_loss: 9919754240.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 7816615936.0000 - val_loss: 9915063296.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7812556800.0000 - val_loss: 9909340160.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 7807603200.0000 - val_loss: 9902372864.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7801575936.0000 - val_loss: 9893915648.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7794258432.0000 - val_loss: 9883674624.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7785399296.0000 - val_loss: 9871306752.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7774700032.0000 - val_loss: 9856405504.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7761811456.0000 - val_loss: 9838490624.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7746322944.0000 - val_loss: 9817008128.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 7727751168.0000 - val_loss: 9791305728.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 7705534976.0000 - val_loss: 9760627712.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7679029248.0000 - val_loss: 9724098560.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 7647480320.0000 - val_loss: 9680708608.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 7610022912.0000 - val_loss: 9629301760.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 7565669888.0000 - val_loss: 9568550912.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 7513291264.0000 - val_loss: 9496957952.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 7451614720.0000 - val_loss: 9412824064.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 7379187712.0000 - val_loss: 9314073600.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7293832192.0000 - val_loss: 9125741568.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7132787200.0000 - val_loss: 8977890304.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7006264832.0000 - val_loss: 8806610944.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6860443136.0000 - val_loss: 8604753920.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 6688241664.0000 - val_loss: 8373571072.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 6492215296.0000 - val_loss: 8108937728.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6268870656.0000 - val_loss: 7808705536.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6016983040.0000 - val_loss: 7471732736.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5736443904.0000 - val_loss: 7098568192.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5428917760.0000 - val_loss: 6692401664.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 5098790912.0000 - val_loss: 6260431872.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 4754481152.0000 - val_loss: 5815753216.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 4410233856.0000 - val_loss: 5379867648.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4088402688.0000 - val_loss: 4985561600.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3821856256.0000 - val_loss: 4678605824.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3654568960.0000 - val_loss: 4511931392.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3633335296.0000 - val_loss: 4515475456.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3773307648.0000 - val_loss: 4635606528.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3996596224.0000 - val_loss: 4741227520.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4153609472.0000 - val_loss: 4751881728.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 4165669888.0000 - val_loss: 4683041792.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 4061108992.0000 - val_loss: 4589394432.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3907931904.0000 - val_loss: 4518456832.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3764646912.0000 - val_loss: 4494405120.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3664130048.0000 - val_loss: 4519048192.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3614822912.0000 - val_loss: 4579891200.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3608881152.0000 - val_loss: 4659066880.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3631261952.0000 - val_loss: 4739758080.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3666475520.0000 - val_loss: 4809320448.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3702133248.0000 - val_loss: 4859900416.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3729983232.0000 - val_loss: 4887812608.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3745593088.0000 - val_loss: 4892550656.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3747556864.0000 - val_loss: 4875896320.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3736701184.0000 - val_loss: 4841251840.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3715441408.0000 - val_loss: 4793143296.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3687268096.0000 - val_loss: 4736825856.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3656304128.0000 - val_loss: 4677854720.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3626829824.0000 - val_loss: 4621580288.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3602729216.0000 - val_loss: 4572522496.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3586851584.0000 - val_loss: 4533712384.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3580380160.0000 - val_loss: 4506175488.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3582427904.0000 - val_loss: 4488805888.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3590099712.0000 - val_loss: 4478838784.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3599220736.0000 - val_loss: 4472801280.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3605586176.0000 - val_loss: 4467545600.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3606278912.0000 - val_loss: 4460704768.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3600449792.0000 - val_loss: 4450014208.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3589217024.0000 - val_loss: 4431178240.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3574698752.0000 - val_loss: 4396716544.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3557774848.0000 - val_loss: 4357986304.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3526167552.0000 - val_loss: 13271400448.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 10762456064.0000 - val_loss: 9451590656.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7419541504.0000 - val_loss: 9323437056.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7308573696.0000 - val_loss: 9166775296.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7173729280.0000 - val_loss: 8989930496.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 7022238720.0000 - val_loss: 8797885440.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6858307584.0000 - val_loss: 8592904192.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 6683892736.0000 - val_loss: 8375426560.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 6499481088.0000 - val_loss: 8144724480.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 6304633856.0000 - val_loss: 7899374592.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 6098396672.0000 - val_loss: 7637661184.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 5879647232.0000 - val_loss: 7357955584.0000\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_21 (LSTM)              (None, 10)                480       \n",
      "                                                                 \n",
      " dense_210 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_211 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_212 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_213 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_214 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_215 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_216 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_217 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_218 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_219 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 8220910592.0000 - val_loss: 6868789760.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8220898304.0000 - val_loss: 6868774400.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 8220882944.0000 - val_loss: 6868750336.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8220856320.0000 - val_loss: 6868715520.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8220819968.0000 - val_loss: 6868666368.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8220765696.0000 - val_loss: 6868595712.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 8220690432.0000 - val_loss: 6868497920.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8220585472.0000 - val_loss: 6868365824.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8220442624.0000 - val_loss: 6868187648.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8220251136.0000 - val_loss: 6867950080.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 8219994624.0000 - val_loss: 6867636224.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8219656192.0000 - val_loss: 6867225600.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8219213824.0000 - val_loss: 6866693120.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8218639360.0000 - val_loss: 6866007552.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8217900544.0000 - val_loss: 6865127424.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8216952320.0000 - val_loss: 6864008192.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8215746560.0000 - val_loss: 6862589952.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8214218240.0000 - val_loss: 6860803072.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8212293120.0000 - val_loss: 6858562560.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8209879040.0000 - val_loss: 6855768064.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8206867456.0000 - val_loss: 6852295680.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8203125248.0000 - val_loss: 6847998464.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8198493184.0000 - val_loss: 6842700288.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8192784896.0000 - val_loss: 6836192256.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8185769984.0000 - val_loss: 6828226560.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8177185280.0000 - val_loss: 6818510848.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 8166711808.0000 - val_loss: 6806697472.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8153976832.0000 - val_loss: 6792379392.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8138539008.0000 - val_loss: 6775076864.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8119880192.0000 - val_loss: 6754234368.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 8097397248.0000 - val_loss: 6729204224.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8070389760.0000 - val_loss: 6699238912.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8038048768.0000 - val_loss: 6663481856.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7999436800.0000 - val_loss: 6620953600.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7953490944.0000 - val_loss: 6570539008.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7898989056.0000 - val_loss: 6510991872.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7834560512.0000 - val_loss: 6440913920.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7758667776.0000 - val_loss: 6358773760.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7669604864.0000 - val_loss: 6262905856.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7565502464.0000 - val_loss: 6151538688.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7444350976.0000 - val_loss: 6022839296.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7304029184.0000 - val_loss: 5874983424.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7142370816.0000 - val_loss: 5706263552.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 6957253632.0000 - val_loss: 5515250176.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 6746752000.0000 - val_loss: 5301025792.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 6509345792.0000 - val_loss: 5063518208.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6244224000.0000 - val_loss: 4803959808.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5951720960.0000 - val_loss: 4525541888.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 5633918976.0000 - val_loss: 4234288640.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5295486464.0000 - val_loss: 3940245760.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4944808448.0000 - val_loss: 3659001344.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4595481088.0000 - val_loss: 3413487616.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4268132608.0000 - val_loss: 3235564032.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3992232960.0000 - val_loss: 3165522688.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3806365952.0000 - val_loss: 3243709952.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3752017664.0000 - val_loss: 3481365248.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3849275136.0000 - val_loss: 3807905792.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4049611520.0000 - val_loss: 4071518720.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 4229431296.0000 - val_loss: 4161345792.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4291167744.0000 - val_loss: 4079291904.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4230248960.0000 - val_loss: 3891875072.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4097803008.0000 - val_loss: 3672453120.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3951604480.0000 - val_loss: 3473976064.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3832294656.0000 - val_loss: 3323231744.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3758161920.0000 - val_loss: 3225519104.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3729309184.0000 - val_loss: 3172925440.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3735170304.0000 - val_loss: 3152130816.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3761535232.0000 - val_loss: 3149771520.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3795231744.0000 - val_loss: 3155152128.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3826309888.0000 - val_loss: 3161024512.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 3848475648.0000 - val_loss: 3163343616.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3858661632.0000 - val_loss: 3160651776.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3856323328.0000 - val_loss: 3153413120.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3842767360.0000 - val_loss: 3143427328.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3820593408.0000 - val_loss: 3133304576.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3793225472.0000 - val_loss: 3125958912.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3764472064.0000 - val_loss: 3124080384.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3738067712.0000 - val_loss: 3129559552.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3717158144.0000 - val_loss: 3142938368.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3703772928.0000 - val_loss: 3163001600.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3698388224.0000 - val_loss: 3186710784.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3699751168.0000 - val_loss: 3209647360.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3705129472.0000 - val_loss: 3226974208.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3711031296.0000 - val_loss: 3234659072.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3714200832.0000 - val_loss: 3230509312.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3712525824.0000 - val_loss: 3214622464.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 3705465344.0000 - val_loss: 3189182464.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 3693857792.0000 - val_loss: 3157908736.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3679284992.0000 - val_loss: 3125498368.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3663351040.0000 - val_loss: 3096864000.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3647467776.0000 - val_loss: 3075119872.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 3633388288.0000 - val_loss: 3044326400.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3600808704.0000 - val_loss: 5594658816.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 6832681472.0000 - val_loss: 5433101824.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6653012480.0000 - val_loss: 5181655552.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6372377088.0000 - val_loss: 4876263936.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6028965376.0000 - val_loss: 4540556800.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 5646917632.0000 - val_loss: 4194241536.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 5245479424.0000 - val_loss: 3857921536.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 4844136448.0000 - val_loss: 3556429312.0000\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_22 (LSTM)              (None, 10)                480       \n",
      "                                                                 \n",
      " dense_220 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_221 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_222 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_223 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_224 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_225 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_226 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_227 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_228 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_229 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 7798331904.0000 - val_loss: 11332057088.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7798115840.0000 - val_loss: 11331763200.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7797870592.0000 - val_loss: 11331389440.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7797561344.0000 - val_loss: 11330899968.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 7797155328.0000 - val_loss: 11330249728.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7796615168.0000 - val_loss: 11329386496.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7795900416.0000 - val_loss: 11328249856.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 7794957312.0000 - val_loss: 11326761984.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7793723392.0000 - val_loss: 11324825600.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7792119808.0000 - val_loss: 11322328064.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7790048768.0000 - val_loss: 11319128064.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 7787397120.0000 - val_loss: 11315056640.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7784020480.0000 - val_loss: 11309900800.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7779748352.0000 - val_loss: 11303414784.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7774372864.0000 - val_loss: 11295292416.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7767642624.0000 - val_loss: 11285170176.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7759257600.0000 - val_loss: 11272614912.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7748854272.0000 - val_loss: 11257096192.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7736002048.0000 - val_loss: 11237994496.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7720185856.0000 - val_loss: 11214568448.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7700797440.0000 - val_loss: 11185944576.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7677113344.0000 - val_loss: 11151079424.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 7648283648.0000 - val_loss: 11108759552.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7613308928.0000 - val_loss: 11114776576.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7618270720.0000 - val_loss: 11065058304.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7577108480.0000 - val_loss: 10921722880.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7459021824.0000 - val_loss: 10833588224.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7386486784.0000 - val_loss: 10728788992.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7300380160.0000 - val_loss: 10604439552.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7198420480.0000 - val_loss: 10457389056.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7078142464.0000 - val_loss: 10284213248.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 6936931840.0000 - val_loss: 10081286144.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 6772084224.0000 - val_loss: 9844866048.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6580931584.0000 - val_loss: 9571290112.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 6361034752.0000 - val_loss: 9257246720.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 6110486016.0000 - val_loss: 8900233216.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5828359680.0000 - val_loss: 8499202048.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5515365888.0000 - val_loss: 8055526912.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5174786560.0000 - val_loss: 7573501952.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 4813543936.0000 - val_loss: 7860870144.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 5035012608.0000 - val_loss: 7368200192.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 4673730048.0000 - val_loss: 6864157696.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4318528000.0000 - val_loss: 6367799808.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3990819072.0000 - val_loss: 5914499072.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3725231872.0000 - val_loss: 5554022912.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3565208320.0000 - val_loss: 5341572096.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3547341056.0000 - val_loss: 7569224192.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 4837116928.0000 - val_loss: 9219138560.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 6115322368.0000 - val_loss: 8872292352.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5845049856.0000 - val_loss: 8467152384.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5534436352.0000 - val_loss: 8010217472.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 5191436288.0000 - val_loss: 7512593920.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4828748288.0000 - val_loss: 6992076800.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4465646080.0000 - val_loss: 6475794944.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4130091008.0000 - val_loss: 6002371584.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3859877888.0000 - val_loss: 5621117952.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3699904768.0000 - val_loss: 5382262784.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3688864000.0000 - val_loss: 5308929536.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3826720768.0000 - val_loss: 5358002176.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 4036833280.0000 - val_loss: 5428393984.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4190707456.0000 - val_loss: 5443313664.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4207933952.0000 - val_loss: 5399543808.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4101319936.0000 - val_loss: 5338822144.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3934596352.0000 - val_loss: 5304475648.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3771290368.0000 - val_loss: 5319560704.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3650638080.0000 - val_loss: 5384615424.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3585042176.0000 - val_loss: 5484958720.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3567510272.0000 - val_loss: 5600007168.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3581317120.0000 - val_loss: 5709372416.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3607109120.0000 - val_loss: 8991229952.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 5924157952.0000 - val_loss: 8911374336.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 5854765056.0000 - val_loss: 8711959552.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 5691830272.0000 - val_loss: 8527963136.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5593200128.0000 - val_loss: 8045793280.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 5174643712.0000 - val_loss: 7641186816.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4866374656.0000 - val_loss: 7209553920.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4550076928.0000 - val_loss: 6773079040.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 4244095232.0000 - val_loss: 6354163200.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3968835584.0000 - val_loss: 5977726464.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3746614016.0000 - val_loss: 5669931008.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3599066368.0000 - val_loss: 5453118976.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3541203968.0000 - val_loss: 5337126400.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3572184064.0000 - val_loss: 5310056960.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3666533632.0000 - val_loss: 5336543232.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3774651136.0000 - val_loss: 5362938368.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3837807616.0000 - val_loss: 13043790848.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 9289506816.0000 - val_loss: 5335136256.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3887752448.0000 - val_loss: 5385943040.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3897236480.0000 - val_loss: 5357650944.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3828333056.0000 - val_loss: 5319132160.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3727945984.0000 - val_loss: 5299369984.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3630633472.0000 - val_loss: 5311182336.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3557956864.0000 - val_loss: 5354704384.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3517535744.0000 - val_loss: 5421849088.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3506719488.0000 - val_loss: 5501120512.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3517493248.0000 - val_loss: 5581238272.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3540470016.0000 - val_loss: 5653104640.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3567292160.0000 - val_loss: 11486568448.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7589936128.0000 - val_loss: 5764751360.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3616703744.0000 - val_loss: 5798462464.0000\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_23 (LSTM)              (None, 10)                480       \n",
      "                                                                 \n",
      " dense_230 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_231 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_232 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_233 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_234 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_235 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_236 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_237 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_238 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_239 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 8091425280.0000 - val_loss: 11227444224.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8091246080.0000 - val_loss: 11227190272.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8091035136.0000 - val_loss: 11226807296.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8090718208.0000 - val_loss: 11226352640.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8090342912.0000 - val_loss: 11225742336.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8089838592.0000 - val_loss: 11224934400.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8089168384.0000 - val_loss: 11223872512.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8088291328.0000 - val_loss: 11222490112.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8087147008.0000 - val_loss: 11220706304.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8085671936.0000 - val_loss: 11218423808.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8083784704.0000 - val_loss: 11215523840.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8081385472.0000 - val_loss: 11211858944.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8078353408.0000 - val_loss: 11207251968.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8074546176.0000 - val_loss: 11201498112.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8069786624.0000 - val_loss: 11194331136.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8063861760.0000 - val_loss: 11185447936.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8056518656.0000 - val_loss: 11174476800.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8047449088.0000 - val_loss: 11160980480.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8036294656.0000 - val_loss: 11144435712.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 8022626816.0000 - val_loss: 11124220928.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8005930496.0000 - val_loss: 11099603968.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7985606656.0000 - val_loss: 11069724672.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7960948224.0000 - val_loss: 11033569280.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 7931127296.0000 - val_loss: 10989955072.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 7895179264.0000 - val_loss: 10937505792.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7851982336.0000 - val_loss: 10874620928.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7800244736.0000 - val_loss: 10799465472.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7738485248.0000 - val_loss: 10709928960.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 7665016832.0000 - val_loss: 10603623424.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7577942016.0000 - val_loss: 10477873152.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7475158016.0000 - val_loss: 10542899200.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7526036992.0000 - val_loss: 10156465152.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7213621248.0000 - val_loss: 9954571264.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 7050279936.0000 - val_loss: 9720343552.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6861725696.0000 - val_loss: 9450090496.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 6645527552.0000 - val_loss: 9140416512.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 6399739392.0000 - val_loss: 8788554752.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 6123256832.0000 - val_loss: 8392925184.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 5816238592.0000 - val_loss: 7953928704.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 5481175040.0000 - val_loss: 7475102208.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 5124301824.0000 - val_loss: 6964813312.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 4756539392.0000 - val_loss: 6438609920.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4396116992.0000 - val_loss: 5922354688.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 4071186432.0000 - val_loss: 5455673856.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3821996544.0000 - val_loss: 5088190464.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3692059136.0000 - val_loss: 5017337856.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3712799232.0000 - val_loss: 4888249856.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3807831808.0000 - val_loss: 4895457792.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4000408832.0000 - val_loss: 4946961920.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 4156811776.0000 - val_loss: 4958499840.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4183981568.0000 - val_loss: 4923491840.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4093440512.0000 - val_loss: 4880970752.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3949016576.0000 - val_loss: 4869706240.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3811744768.0000 - val_loss: 4907722240.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3717345792.0000 - val_loss: 4991234048.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3674990848.0000 - val_loss: 5102710784.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3675539200.0000 - val_loss: 5220712960.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3702061312.0000 - val_loss: 5326591488.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3737832192.0000 - val_loss: 5407519744.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3769819392.0000 - val_loss: 5456643072.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3789968896.0000 - val_loss: 5472020992.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3794687232.0000 - val_loss: 5455342080.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3783857408.0000 - val_loss: 5410888704.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3759901696.0000 - val_loss: 5344780800.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3726996736.0000 - val_loss: 5264404480.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3690392832.0000 - val_loss: 5177826816.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3655684352.0000 - val_loss: 5093060608.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3627934208.0000 - val_loss: 5017119232.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3610657280.0000 - val_loss: 4954978816.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3604838400.0000 - val_loss: 4908777984.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3608403456.0000 - val_loss: 4877723648.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3616629248.0000 - val_loss: 4858996224.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3623692800.0000 - val_loss: 5676588032.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3979631616.0000 - val_loss: 5545441792.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3874052608.0000 - val_loss: 5460953600.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3824494336.0000 - val_loss: 5358544384.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3771013120.0000 - val_loss: 5246903808.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3718818816.0000 - val_loss: 5135916032.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3673451776.0000 - val_loss: 5357497856.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3739047424.0000 - val_loss: 6705056768.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4560602112.0000 - val_loss: 6469538304.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4393561600.0000 - val_loss: 6196724224.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 4207831808.0000 - val_loss: 5900515840.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 4015796736.0000 - val_loss: 5599717888.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3834524160.0000 - val_loss: 5316933632.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3683897856.0000 - val_loss: 5077142528.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3584197632.0000 - val_loss: 4903229952.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3550487552.0000 - val_loss: 4807869952.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3583697152.0000 - val_loss: 4775596544.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3655550464.0000 - val_loss: 7360265728.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 5065669632.0000 - val_loss: 7104457216.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 4886357504.0000 - val_loss: 6798588928.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4664678400.0000 - val_loss: 6453319680.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 4416502272.0000 - val_loss: 6086903296.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4160404992.0000 - val_loss: 5721159168.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3917873152.0000 - val_loss: 5382448640.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3713842688.0000 - val_loss: 5100397056.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3574509568.0000 - val_loss: 4902637568.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3521055488.0000 - val_loss: 4804166144.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3558554368.0000 - val_loss: 4794190848.0000\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_24 (LSTM)              (None, 10)                480       \n",
      "                                                                 \n",
      " dense_240 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_241 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_242 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_243 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_244 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_245 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_246 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_247 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_248 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_249 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2122646.5000 - val_loss: 2764124.7500\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2121022.0000 - val_loss: 2761825.5000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2119103.5000 - val_loss: 2758851.5000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2116622.2500 - val_loss: 2754931.7500\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2113352.5000 - val_loss: 2749771.5000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2109048.5000 - val_loss: 2743010.7500\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2103411.2500 - val_loss: 2734208.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2096073.5000 - val_loss: 2722832.5000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2086595.1250 - val_loss: 2708257.2500\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2074456.2500 - val_loss: 2689738.7500\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2059044.2500 - val_loss: 2666392.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2039629.7500 - val_loss: 2637151.5000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2015340.3750 - val_loss: 2600742.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1985138.3750 - val_loss: 2555659.7500\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1947810.2500 - val_loss: 2500130.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1901942.6250 - val_loss: 2432027.7500\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1845876.3750 - val_loss: 2348693.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1777578.6250 - val_loss: 2246399.7500\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1694206.7500 - val_loss: 2118363.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1590507.0000 - val_loss: 1947809.1250\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1454426.1250 - val_loss: 1745394.2500\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1295840.7500 - val_loss: 1551252.6250\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1147706.1250 - val_loss: 1355515.8750\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1005454.7500 - val_loss: 1169519.1250\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 882232.0000 - val_loss: 1021365.3125\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 806024.5625 - val_loss: 952287.5625\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 814115.6250 - val_loss: 982892.7500\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 912635.6875 - val_loss: 1040592.6875\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1007037.1250 - val_loss: 1046169.7500\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1016007.0625 - val_loss: 1008311.9375\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 958758.3125 - val_loss: 966016.1250\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 883716.8125 - val_loss: 946564.7500\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 825420.0625 - val_loss: 957149.2500\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 796705.7500 - val_loss: 990444.4375\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 794285.0000 - val_loss: 1033572.9375\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 807588.2500 - val_loss: 1074717.8750\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 825285.7500 - val_loss: 1106388.7500\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 837539.5625 - val_loss: 1303200.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 974655.3125 - val_loss: 1131760.7500\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 848839.6250 - val_loss: 1118193.1250\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 844941.6875 - val_loss: 1096153.1250\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 834485.8125 - val_loss: 1066686.5000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 819775.2500 - val_loss: 1033688.5625\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 804499.7500 - val_loss: 1001423.9375\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 792141.1875 - val_loss: 973826.0625\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 785442.0000 - val_loss: 953663.0625\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 785623.0625 - val_loss: 941717.5000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 791680.5000 - val_loss: 936535.0625\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 800392.0000 - val_loss: 935259.8750\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 807526.5625 - val_loss: 935313.1875\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 809870.5625 - val_loss: 935776.4375\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 806756.6250 - val_loss: 937466.9375\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 800024.5000 - val_loss: 941851.3125\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 792646.5625 - val_loss: 949784.7500\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 787144.3125 - val_loss: 960886.9375\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 784694.8125 - val_loss: 973682.3750\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 785101.1250 - val_loss: 986173.9375\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 787294.0625 - val_loss: 996454.9375\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 789945.5000 - val_loss: 1003133.6250\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 791938.5625 - val_loss: 1005524.0625\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 792613.8125 - val_loss: 1003652.3125\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 791828.5625 - val_loss: 998149.5000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 789885.2500 - val_loss: 990079.5625\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 787382.1250 - val_loss: 980721.5000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 785014.0625 - val_loss: 971339.1250\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 783365.6250 - val_loss: 962965.0625\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 782730.5625 - val_loss: 956252.7500\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 783024.1250 - val_loss: 951445.8750\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 783829.8125 - val_loss: 948474.9375\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 784581.9375 - val_loss: 947133.1875\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 784811.1250 - val_loss: 947229.7500\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 784332.0625 - val_loss: 948641.7500\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 783281.7500 - val_loss: 951256.8125\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 782005.3750 - val_loss: 954868.7500\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 780869.6875 - val_loss: 959108.8750\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 780106.8125 - val_loss: 963457.9375\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 779751.1250 - val_loss: 967330.8750\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 779670.3750 - val_loss: 970192.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 779651.6250 - val_loss: 971658.9375\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 779494.2500 - val_loss: 971564.5625\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 779076.3750 - val_loss: 969970.3125\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 778380.5625 - val_loss: 967131.2500\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 777477.1250 - val_loss: 963432.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 776481.6875 - val_loss: 959304.8750\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 775501.7500 - val_loss: 955154.3125\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 774590.6250 - val_loss: 951298.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 773724.9375 - val_loss: 947938.6875\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 772812.3750 - val_loss: 945162.8125\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 771722.8125 - val_loss: 942955.2500\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 770324.5625 - val_loss: 941215.1250\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 768506.9375 - val_loss: 939756.5000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 766170.8125 - val_loss: 938298.3125\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 763190.4375 - val_loss: 936480.8125\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 759386.8750 - val_loss: 934219.8750\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 754796.2500 - val_loss: 935074.3750\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 752353.4375 - val_loss: 948936.4375\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 760674.8750 - val_loss: 926702.2500\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 748211.0625 - val_loss: 918845.5625\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 746652.8750 - val_loss: 913926.7500\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 746361.0625 - val_loss: 909082.4375\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_25 (LSTM)              (None, 10)                480       \n",
      "                                                                 \n",
      " dense_250 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_251 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_252 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_253 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_254 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_255 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_256 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_257 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_258 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_259 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2242098.2500 - val_loss: 1801597.7500\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2241860.0000 - val_loss: 1801269.1250\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2241507.5000 - val_loss: 1800781.8750\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2240984.5000 - val_loss: 1800072.5000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2240223.5000 - val_loss: 1799057.1250\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2239135.2500 - val_loss: 1797631.3750\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2237609.2500 - val_loss: 1795660.2500\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2235501.7500 - val_loss: 1792970.6250\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2232630.0000 - val_loss: 1789339.7500\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2228760.7500 - val_loss: 1784478.1250\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2223593.2500 - val_loss: 1778005.3750\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2216738.5000 - val_loss: 1769403.2500\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2207672.0000 - val_loss: 1757919.7500\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2195647.5000 - val_loss: 1742367.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2179489.2500 - val_loss: 1720621.5000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2157028.7500 - val_loss: 1688250.2500\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2123257.7500 - val_loss: 1635072.5000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2064371.8750 - val_loss: 1559022.5000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1975777.2500 - val_loss: 1479615.5000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1888024.5000 - val_loss: 1395619.3750\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1796413.2500 - val_loss: 1301091.7500\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1692549.7500 - val_loss: 1194172.6250\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1573784.6250 - val_loss: 1075625.5000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1440249.7500 - val_loss: 949099.5625\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1294839.7500 - val_loss: 822383.1250\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1144386.0000 - val_loss: 709511.3125\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1001739.5000 - val_loss: 633523.9375\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 888635.1875 - val_loss: 627536.1250\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 837430.1250 - val_loss: 720608.6875\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 879785.7500 - val_loss: 875675.1875\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 990879.1250 - val_loss: 970795.0625\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1064607.0000 - val_loss: 956166.3750\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1051869.5000 - val_loss: 871347.1250\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 984447.8125 - val_loss: 769787.5625\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 908170.1250 - val_loss: 686681.1250\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 852523.6875 - val_loss: 635096.2500\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 826800.0625 - val_loss: 612503.9375\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 826350.8125 - val_loss: 609448.1875\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 840850.0625 - val_loss: 615947.4375\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 860233.2500 - val_loss: 624470.5625\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 877274.1250 - val_loss: 630482.3125\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 887865.5000 - val_loss: 631942.5000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 890381.1875 - val_loss: 628620.3125\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 884941.1875 - val_loss: 621544.3125\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 872864.8750 - val_loss: 612610.4375\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 856313.1250 - val_loss: 604250.6250\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 838010.1875 - val_loss: 599038.6875\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 820919.3125 - val_loss: 599124.9375\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 807756.5000 - val_loss: 605479.1875\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 800307.7500 - val_loss: 617108.3125\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 798676.6250 - val_loss: 630677.3125\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 800839.6875 - val_loss: 641071.2500\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 803002.3125 - val_loss: 643006.8750\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 800892.9375 - val_loss: 632909.5000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 791324.5625 - val_loss: 609778.6875\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 772835.5625 - val_loss: 574588.2500\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 744864.2500 - val_loss: 530815.7500\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 708040.8750 - val_loss: 511527.3438\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 700660.1250 - val_loss: 525608.8750\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 729911.3750 - val_loss: 478519.3438\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 667583.3750 - val_loss: 458142.4375\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 644695.0625 - val_loss: 434679.5625\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 614145.9375 - val_loss: 488085.8438\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 686034.0625 - val_loss: 428283.0312\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 592176.5000 - val_loss: 452523.2812\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 605998.3125 - val_loss: 492954.7500\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 639152.2500 - val_loss: 517081.7188\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 658094.6875 - val_loss: 512076.7188\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 651412.4375 - val_loss: 479211.8438\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 619729.6875 - val_loss: 432641.0312\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 575314.3125 - val_loss: 400343.0625\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 547543.0625 - val_loss: 417368.7812\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 580219.5625 - val_loss: 407300.2812\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 565671.6875 - val_loss: 380445.0312\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 524679.1250 - val_loss: 377460.4062\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 515845.4062 - val_loss: 385083.4062\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 518411.6562 - val_loss: 396577.4062\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 523016.2500 - val_loss: 405735.2812\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 526222.7500 - val_loss: 403348.4375\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 521096.9375 - val_loss: 391592.5312\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 509689.4375 - val_loss: 375258.7188\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 489643.7500 - val_loss: 355278.8750\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 462984.3438 - val_loss: 337499.6562\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 443430.6562 - val_loss: 341336.5625\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 456911.8750 - val_loss: 337529.7500\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 450350.0938 - val_loss: 332884.3750\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 429332.0625 - val_loss: 342104.8750\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 431565.0938 - val_loss: 323383.5938\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 411062.8438 - val_loss: 303353.0312\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 399563.3750 - val_loss: 301836.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 402410.9688 - val_loss: 289941.6562\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 374223.8125 - val_loss: 296312.4688\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 376476.1562 - val_loss: 283258.7188\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 365187.7188 - val_loss: 271248.8125\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 359475.1875 - val_loss: 260103.8281\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 342567.9375 - val_loss: 253299.8594\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 326874.5000 - val_loss: 247828.2188\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 320518.1562 - val_loss: 243814.5938\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 319139.3438 - val_loss: 241691.5312\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 306838.2500 - val_loss: 234880.3750\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_26 (LSTM)              (None, 10)                480       \n",
      "                                                                 \n",
      " dense_260 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_261 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_262 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_263 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_264 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_265 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_266 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_267 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_268 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_269 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2139489.0000 - val_loss: 2810002.2500\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2136884.5000 - val_loss: 2806565.7500\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2133921.2500 - val_loss: 2802115.7500\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2130085.0000 - val_loss: 2796176.5000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2124965.7500 - val_loss: 2788221.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2118110.2500 - val_loss: 2777633.7500\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2108989.5000 - val_loss: 2763683.7500\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2096976.5000 - val_loss: 2745487.7500\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2081315.7500 - val_loss: 2721978.7500\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2061097.7500 - val_loss: 2691886.5000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2035244.5000 - val_loss: 2653725.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2002502.7500 - val_loss: 2605775.7500\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1961436.8750 - val_loss: 2546086.5000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1910437.6250 - val_loss: 2472491.7500\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1847757.8750 - val_loss: 2382699.5000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1771610.8750 - val_loss: 2274472.5000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1680369.6250 - val_loss: 2145943.7500\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1572898.5000 - val_loss: 1996144.2500\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1449111.2500 - val_loss: 1825919.1250\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1310906.6250 - val_loss: 1639491.1250\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1163741.2500 - val_loss: 1447099.2500\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1019215.6875 - val_loss: 1267159.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 897195.4375 - val_loss: 1130274.5000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 828413.3125 - val_loss: 1078557.3750\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 850674.3750 - val_loss: 1120860.2500\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 956224.2500 - val_loss: 1170327.7500\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1036029.3750 - val_loss: 1162342.3750\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1025878.1875 - val_loss: 1117169.6250\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 957617.8125 - val_loss: 1074397.7500\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 880803.6250 - val_loss: 1057864.6250\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 826530.4375 - val_loss: 1071254.5000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 803584.1250 - val_loss: 1105385.8750\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 806009.6250 - val_loss: 1147305.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 822473.1250 - val_loss: 1186141.5000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 842563.6875 - val_loss: 1215027.8750\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 859202.7500 - val_loss: 1230712.1250\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 868642.6250 - val_loss: 1232531.3750\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 869659.2500 - val_loss: 1221535.7500\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 862766.9375 - val_loss: 1199950.6250\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 849684.3125 - val_loss: 1170884.3750\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 832991.6875 - val_loss: 1138111.8750\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 815837.9375 - val_loss: 1105761.8750\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 801525.4375 - val_loss: 1077749.3750\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 792847.5000 - val_loss: 1056925.8750\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 791187.8125 - val_loss: 1044190.6875\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 795685.2500 - val_loss: 1038138.1250\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 803112.7500 - val_loss: 1035832.2500\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 809040.5000 - val_loss: 1034527.7500\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 809964.2500 - val_loss: 1033154.3750\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 804992.3125 - val_loss: 1032419.5000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 795853.6250 - val_loss: 1033619.8750\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 785400.3750 - val_loss: 1037194.5000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 775808.8125 - val_loss: 1041879.6875\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 767461.3750 - val_loss: 1044845.4375\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 758878.1875 - val_loss: 1047522.2500\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 751353.1875 - val_loss: 1179986.8750\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 839953.5625 - val_loss: 1067809.1250\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 761173.8750 - val_loss: 1040771.1875\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 745212.0000 - val_loss: 1031999.5625\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 743717.0625 - val_loss: 1023524.6875\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 742576.2500 - val_loss: 1014076.7500\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 740030.2500 - val_loss: 1002972.3125\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 734558.9375 - val_loss: 987895.0625\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 723093.6250 - val_loss: 965358.8125\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 701523.9375 - val_loss: 963047.6875\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 690015.8125 - val_loss: 945250.5625\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 675225.9375 - val_loss: 921176.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 656563.5625 - val_loss: 887566.9375\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 626450.7500 - val_loss: 823584.2500\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 568382.0625 - val_loss: 831264.7500\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 557682.3125 - val_loss: 746809.5625\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 496776.8125 - val_loss: 680160.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 456182.1250 - val_loss: 650714.2500\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 444595.7500 - val_loss: 607143.8750\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 417661.2188 - val_loss: 540657.2500\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 364645.4375 - val_loss: 537326.0625\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 350577.5938 - val_loss: 470877.7188\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 311288.4062 - val_loss: 404094.9375\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 285114.2812 - val_loss: 375808.5938\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 283974.0938 - val_loss: 356469.6562\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 262351.7500 - val_loss: 378503.7500\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 263481.2812 - val_loss: 296657.7188\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 237426.6719 - val_loss: 272134.6562\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 225953.1875 - val_loss: 300064.3750\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 217656.0000 - val_loss: 290804.5312\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 218336.2812 - val_loss: 244757.9688\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 228064.6406 - val_loss: 246822.7656\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 226849.1406 - val_loss: 313967.7812\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 242345.1562 - val_loss: 236032.1875\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 225081.2188 - val_loss: 226537.6406\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 214707.2188 - val_loss: 302173.2500\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 224468.0625 - val_loss: 220074.2031\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 213831.9219 - val_loss: 232023.1719\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 207476.0625 - val_loss: 334026.5938\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 240088.7656 - val_loss: 230748.9844\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 228452.8750 - val_loss: 225281.1719\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 212467.6406 - val_loss: 306686.3438\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 224052.8750 - val_loss: 240315.5781\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 198963.0312 - val_loss: 214313.1719\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 214001.2500 - val_loss: 222844.5625\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_27 (LSTM)              (None, 10)                480       \n",
      "                                                                 \n",
      " dense_270 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_271 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_272 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_273 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_274 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_275 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_276 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_277 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_278 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_279 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2246966.0000 - val_loss: 1970005.8750\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2241096.0000 - val_loss: 1963998.2500\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2234946.0000 - val_loss: 1956638.3750\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2227411.0000 - val_loss: 1947098.2500\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2217641.5000 - val_loss: 1934531.5000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2204769.7500 - val_loss: 1917996.3750\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2187827.5000 - val_loss: 1896431.5000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2165722.2500 - val_loss: 1868620.5000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2137196.5000 - val_loss: 1833163.5000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2100798.7500 - val_loss: 1788468.1250\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2054866.3750 - val_loss: 1732771.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1997546.2500 - val_loss: 1664217.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1926859.7500 - val_loss: 1581016.5000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1840849.1250 - val_loss: 1481716.1250\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1737831.2500 - val_loss: 1365667.8750\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1616836.6250 - val_loss: 1233773.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1478324.5000 - val_loss: 1089703.3750\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1325344.8750 - val_loss: 941930.2500\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1165472.0000 - val_loss: 806830.6250\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1013833.5625 - val_loss: 712992.8750\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 897398.1250 - val_loss: 703411.3750\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 857718.1250 - val_loss: 811955.6875\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 930718.3125 - val_loss: 966182.8750\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1056781.7500 - val_loss: 1023443.7500\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1105414.3750 - val_loss: 970670.8125\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1060535.3750 - val_loss: 869086.1250\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 975923.7500 - val_loss: 773320.5000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 900164.8125 - val_loss: 710349.0625\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 856348.3750 - val_loss: 683151.3750\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 845453.4375 - val_loss: 681808.6250\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 856908.3750 - val_loss: 693495.8750\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 878005.8750 - val_loss: 707853.5625\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 898817.6250 - val_loss: 718366.0625\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 913306.8750 - val_loss: 721815.6875\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 918661.6875 - val_loss: 717311.3125\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 914279.3750 - val_loss: 705511.6875\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 900942.0625 - val_loss: 688232.3125\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 880269.6250 - val_loss: 669113.5000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 854811.9375 - val_loss: 663027.5000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 837822.6875 - val_loss: 640221.3125\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 809160.6875 - val_loss: 633116.3125\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 796382.3750 - val_loss: 639307.6875\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 795375.6875 - val_loss: 651035.6250\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 799541.4375 - val_loss: 659440.3750\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 800646.5000 - val_loss: 660623.3750\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 794494.9375 - val_loss: 672065.9375\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 810168.9375 - val_loss: 656782.1875\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 789515.8750 - val_loss: 643454.6875\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 779528.1250 - val_loss: 631318.4375\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 773821.8750 - val_loss: 615809.3125\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 764011.2500 - val_loss: 600588.5000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 753183.5000 - val_loss: 590623.6250\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 745772.8125 - val_loss: 588796.1250\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 745057.3750 - val_loss: 589857.8750\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 747360.0625 - val_loss: 583957.3125\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 743425.6250 - val_loss: 575084.3125\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 737333.4375 - val_loss: 568499.6250\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 732719.5000 - val_loss: 563746.3125\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 727945.6875 - val_loss: 559244.7500\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 721277.6875 - val_loss: 554468.4375\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 712041.8750 - val_loss: 551235.1875\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 701883.9375 - val_loss: 553317.9375\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 696520.1250 - val_loss: 552405.3750\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 691348.5625 - val_loss: 544209.7500\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 682036.6250 - val_loss: 541188.1250\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 678942.4375 - val_loss: 537518.4375\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 672510.0625 - val_loss: 533385.7500\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 663191.0625 - val_loss: 532044.6250\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 658954.8125 - val_loss: 519033.2188\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 647227.8750 - val_loss: 508876.1875\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 640541.3125 - val_loss: 499669.4375\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 631122.6250 - val_loss: 493635.7500\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 621353.1250 - val_loss: 488493.3438\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 614398.4375 - val_loss: 474566.3438\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 603620.5000 - val_loss: 464177.1562\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 596010.3125 - val_loss: 456302.1250\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 585353.4375 - val_loss: 452808.2500\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 575346.9375 - val_loss: 444079.3438\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 564243.4375 - val_loss: 429985.1250\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 552297.5000 - val_loss: 420592.2188\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 540103.5625 - val_loss: 416468.1250\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 526860.8125 - val_loss: 405871.6875\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 513026.3125 - val_loss: 390920.3125\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 499831.7500 - val_loss: 383355.7812\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 483968.6875 - val_loss: 375602.5938\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 469684.5625 - val_loss: 355567.3125\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 455540.0938 - val_loss: 351977.8438\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 437743.9375 - val_loss: 330734.7812\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 419784.1562 - val_loss: 319894.5312\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 402099.5625 - val_loss: 307670.8438\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 384803.7500 - val_loss: 288059.8750\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 368511.0000 - val_loss: 301480.7500\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 358370.0625 - val_loss: 275657.8438\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 380044.4062 - val_loss: 279669.7500\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 326752.0938 - val_loss: 267138.6562\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 310795.5000 - val_loss: 236825.7188\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 328962.2500 - val_loss: 248461.6562\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 284149.2812 - val_loss: 264397.8438\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 282927.5000 - val_loss: 226849.6250\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 322830.6250 - val_loss: 237902.8750\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_28 (LSTM)              (None, 10)                480       \n",
      "                                                                 \n",
      " dense_280 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_281 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_282 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_283 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_284 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_285 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_286 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_287 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_288 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_289 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2167192.0000 - val_loss: 2852116.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2162428.7500 - val_loss: 2845532.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2157085.7500 - val_loss: 2837224.7500\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2150346.5000 - val_loss: 2826326.2500\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2141510.0000 - val_loss: 2811923.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2129838.2500 - val_loss: 2792956.7500\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2114481.7500 - val_loss: 2768160.5000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2094426.3750 - val_loss: 2736045.2500\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2068489.2500 - val_loss: 2694900.5000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2035322.7500 - val_loss: 2642768.5000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1993405.1250 - val_loss: 2577434.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1941045.6250 - val_loss: 2496457.5000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1876436.6250 - val_loss: 2397282.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1797770.1250 - val_loss: 2277412.5000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1703441.8750 - val_loss: 2134716.5000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1592378.1250 - val_loss: 1967987.6250\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1464614.5000 - val_loss: 1777904.7500\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1322250.1250 - val_loss: 1568546.7500\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1170951.0000 - val_loss: 1349873.2500\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1022327.1875 - val_loss: 1141594.1250\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 897441.5000 - val_loss: 978015.1250\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 830331.4375 - val_loss: 905671.7500\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 860898.5625 - val_loss: 935625.6875\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 976350.8750 - val_loss: 978800.3750\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1058858.7500 - val_loss: 970989.3125\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1046877.6250 - val_loss: 932297.7500\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 975878.8750 - val_loss: 900880.8750\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 896931.2500 - val_loss: 898569.3750\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 841598.7500 - val_loss: 926918.9375\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 818568.6250 - val_loss: 974986.4375\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 821551.3750 - val_loss: 1028684.1250\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 838873.5000 - val_loss: 1076600.6250\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 859869.8125 - val_loss: 1111721.6250\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 877294.6250 - val_loss: 1130860.2500\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 887297.3125 - val_loss: 1133494.1250\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 888586.6875 - val_loss: 1120827.7500\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 881628.8750 - val_loss: 1095232.2500\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 868104.6875 - val_loss: 1059966.2500\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 850571.7500 - val_loss: 1018999.9375\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 832186.2500 - val_loss: 976762.1250\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 816317.6875 - val_loss: 937641.5625\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 805919.9375 - val_loss: 905192.4375\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 802639.7500 - val_loss: 881211.3125\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 805922.8125 - val_loss: 865199.2500\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 812719.6250 - val_loss: 854808.3125\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 818424.3750 - val_loss: 847263.1875\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 818885.3125 - val_loss: 840713.8750\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 812131.1250 - val_loss: 834195.7500\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 798229.3125 - val_loss: 825686.2500\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 776528.1250 - val_loss: 808667.7500\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 739352.8125 - val_loss: 827604.6250\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 700232.7500 - val_loss: 804849.3125\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 698806.3125 - val_loss: 801068.8125\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 697916.0625 - val_loss: 799332.5000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 687308.7500 - val_loss: 792596.2500\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 681651.0000 - val_loss: 743137.1875\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 679651.5625 - val_loss: 716353.5000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 670266.1875 - val_loss: 723500.5000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 650401.8125 - val_loss: 663676.6875\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 641411.9375 - val_loss: 649134.1875\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 627428.3125 - val_loss: 713827.2500\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 655006.5625 - val_loss: 647127.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 717757.9375 - val_loss: 679951.2500\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 774956.1250 - val_loss: 673338.2500\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 756626.8125 - val_loss: 645212.2500\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 689969.9375 - val_loss: 632645.3125\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 609288.3750 - val_loss: 1135659.3750\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 889960.4375 - val_loss: 667321.6875\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 624432.3750 - val_loss: 699208.6875\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 677450.3750 - val_loss: 724503.1875\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 704409.0000 - val_loss: 740411.7500\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 713790.1875 - val_loss: 750033.5000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 711946.3125 - val_loss: 753972.1250\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 700433.1250 - val_loss: 752725.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 679216.4375 - val_loss: 797758.0625\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 685625.2500 - val_loss: 767936.2500\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 670942.1250 - val_loss: 736567.9375\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 663041.2500 - val_loss: 716923.5000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 663367.7500 - val_loss: 697791.3125\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 660234.8750 - val_loss: 677540.2500\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 649867.2500 - val_loss: 663816.7500\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 635908.5000 - val_loss: 686095.8750\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 643666.7500 - val_loss: 641849.0625\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 628081.1250 - val_loss: 608784.1250\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 626633.2500 - val_loss: 594612.0625\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 627199.1875 - val_loss: 585295.5625\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 614934.8750 - val_loss: 597170.8125\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 607639.4375 - val_loss: 598256.8125\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 603217.0000 - val_loss: 561636.7500\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 587050.3125 - val_loss: 544044.0625\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 579989.7500 - val_loss: 533505.5000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 565653.7500 - val_loss: 526724.6875\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 543696.6875 - val_loss: 522424.3750\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 520152.3438 - val_loss: 493356.5938\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 481230.5938 - val_loss: 449231.0312\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 427634.1875 - val_loss: 457407.8438\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 405596.0312 - val_loss: 392950.5000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 363926.6250 - val_loss: 310330.8750\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 336210.9062 - val_loss: 281260.8125\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 342263.4375 - val_loss: 262574.0312\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_29 (LSTM)              (None, 10)                480       \n",
      "                                                                 \n",
      " dense_290 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_291 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_292 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_293 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_294 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_295 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_296 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_297 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_298 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_299 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2231567.2500 - val_loss: 2965826.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2225589.2500 - val_loss: 2957960.7500\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2218798.2500 - val_loss: 2947803.5000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2210030.0000 - val_loss: 2934264.5000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2198344.5000 - val_loss: 2916209.5000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2182769.2500 - val_loss: 2892351.7500\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2162201.7500 - val_loss: 2861151.2500\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2135330.7500 - val_loss: 2820761.5000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2100591.7500 - val_loss: 2769005.7500\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2056159.8750 - val_loss: 2703385.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1999965.7500 - val_loss: 2621159.7500\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1929796.2500 - val_loss: 2519497.7500\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1843451.5000 - val_loss: 2395691.2500\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1738991.6250 - val_loss: 2247578.2500\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1615187.8750 - val_loss: 2074276.3750\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1472292.6250 - val_loss: 1877445.3750\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1313341.3750 - val_loss: 1663457.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1146343.8750 - val_loss: 1446935.2500\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 987772.1250 - val_loss: 1256158.6250\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 867559.1250 - val_loss: 1138201.3750\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 832213.5000 - val_loss: 1140962.8750\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 919207.8750 - val_loss: 1215589.6250\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1049836.2500 - val_loss: 1239974.3750\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1086519.6250 - val_loss: 1201391.2500\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1028492.8750 - val_loss: 1147774.5000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 937729.3750 - val_loss: 1117701.6250\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 863272.6875 - val_loss: 1123674.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 824991.0000 - val_loss: 1158232.1250\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 820099.0625 - val_loss: 1205932.7500\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 835505.6250 - val_loss: 1252316.7500\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 857408.4375 - val_loss: 1287257.1250\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 875418.1250 - val_loss: 1304313.8750\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 882809.3125 - val_loss: 1302472.3750\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 879359.0000 - val_loss: 1341321.8750\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 915691.5000 - val_loss: 1255959.6250\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 848816.1250 - val_loss: 1214767.6250\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 823473.6875 - val_loss: 1174642.6250\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 803647.3750 - val_loss: 1137346.8750\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 789765.8125 - val_loss: 1108265.6250\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 785445.1875 - val_loss: 1090732.5000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 791807.2500 - val_loss: 1084155.8750\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 805694.0000 - val_loss: 1084071.3750\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 820166.8125 - val_loss: 1084747.6250\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 827799.1250 - val_loss: 1082860.1250\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 824894.3125 - val_loss: 1079134.3750\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 813179.7500 - val_loss: 1076787.5000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 797836.5625 - val_loss: 1078613.7500\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 784014.5000 - val_loss: 1085055.5000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 774418.4375 - val_loss: 1094005.6250\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 768830.2500 - val_loss: 1101717.1250\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 764933.2500 - val_loss: 1103992.5000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 759544.5625 - val_loss: 1098116.5000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 750550.7500 - val_loss: 1092514.8750\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 745883.0000 - val_loss: 1111540.1250\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 764654.2500 - val_loss: 1049273.1250\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 720953.6875 - val_loss: 1022465.6250\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 710302.2500 - val_loss: 1006537.8750\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 711195.6875 - val_loss: 990654.4375\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 711344.8750 - val_loss: 968304.5000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 702434.4375 - val_loss: 933653.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 676915.1250 - val_loss: 938307.4375\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 675880.0625 - val_loss: 919146.8125\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 664283.8125 - val_loss: 894599.1250\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 653472.1875 - val_loss: 895888.3750\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 664011.3750 - val_loss: 875514.8750\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 635532.5625 - val_loss: 871786.1875\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 623420.5625 - val_loss: 866183.8125\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 614734.0625 - val_loss: 829875.1875\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 587993.5625 - val_loss: 944955.6250\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 650857.0000 - val_loss: 1070105.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 725810.0625 - val_loss: 852002.3750\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 599775.6875 - val_loss: 799843.3750\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 564159.0625 - val_loss: 807802.5625\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 571275.0625 - val_loss: 777474.1250\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 558863.5625 - val_loss: 775208.1875\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 562697.9375 - val_loss: 752431.6250\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 538711.5000 - val_loss: 779905.7500\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 552837.1250 - val_loss: 731280.9375\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 524638.4375 - val_loss: 731142.1875\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 528597.0000 - val_loss: 704978.6875\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 499680.0312 - val_loss: 730313.8750\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 512240.6875 - val_loss: 679071.3750\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 479038.9062 - val_loss: 676195.5000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 482086.5000 - val_loss: 646701.7500\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 452306.5625 - val_loss: 664588.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 461751.1250 - val_loss: 610978.2500\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 428548.8438 - val_loss: 600619.3125\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 427089.0625 - val_loss: 567312.2500\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 394447.7188 - val_loss: 572903.6875\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 399124.8125 - val_loss: 537211.6250\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 388844.0625 - val_loss: 502448.9375\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 361178.1250 - val_loss: 542277.5000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 384130.6250 - val_loss: 480729.2188\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 358653.4688 - val_loss: 453652.7188\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 337373.5938 - val_loss: 483329.0938\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 346950.2500 - val_loss: 397403.5938\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 291401.3125 - val_loss: 391831.3438\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 293298.8750 - val_loss: 385274.9062\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 279578.3750 - val_loss: 332192.4062\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 244177.9531 - val_loss: 327110.7188\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_30 (LSTM)              (None, 10)                480       \n",
      "                                                                 \n",
      " dense_300 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_301 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_302 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_303 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_304 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_305 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_306 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_307 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_308 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_309 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 67192.4844 - val_loss: 1642471.8750\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 67021.9062 - val_loss: 1642220.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 66840.9062 - val_loss: 1641904.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 66614.2656 - val_loss: 1641487.2500\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 66316.5391 - val_loss: 1640934.8750\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 65921.8047 - val_loss: 1640211.7500\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 65400.8398 - val_loss: 1639316.7500\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 64721.5742 - val_loss: 1638245.6250\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 63850.0547 - val_loss: 1636972.6250\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 62754.4219 - val_loss: 1635492.8750\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 61410.2852 - val_loss: 1633838.3750\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 59810.5469 - val_loss: 1632104.5000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 57983.6797 - val_loss: 1630491.3750\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 56026.5508 - val_loss: 1629368.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 54157.5508 - val_loss: 1629307.2500\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 52777.8164 - val_loss: 1630893.7500\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 52401.3008 - val_loss: 1633526.8750\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 52955.9922 - val_loss: 1634933.1250\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 53216.9727 - val_loss: 1634315.3750\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 52577.4023 - val_loss: 1632606.2500\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 51508.6641 - val_loss: 1630824.6250\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 50591.8906 - val_loss: 1629492.5000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 50097.7148 - val_loss: 1628677.3750\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 49995.6875 - val_loss: 1628222.2500\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 50113.3906 - val_loss: 1627932.2500\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 50275.3281 - val_loss: 1627659.3750\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 50363.2070 - val_loss: 1627320.8750\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 50320.6797 - val_loss: 1626886.6250\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 50140.0078 - val_loss: 1626364.5000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 49851.7500 - val_loss: 1625783.1250\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 49515.7188 - val_loss: 1625180.8750\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 49140.2656 - val_loss: 1624641.8750\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 48850.9727 - val_loss: 1624194.1250\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 48695.1367 - val_loss: 1623786.1250\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 48652.3750 - val_loss: 1623304.6250\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 48644.5547 - val_loss: 1622647.2500\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 48611.1250 - val_loss: 1621874.3750\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 48532.9570 - val_loss: 1621060.2500\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 48375.3242 - val_loss: 1620201.7500\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 48216.5742 - val_loss: 1619384.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 48063.1289 - val_loss: 1618647.2500\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 47948.2930 - val_loss: 1617972.8750\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 47880.8164 - val_loss: 1617313.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 47847.9609 - val_loss: 1616655.8750\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 47789.2266 - val_loss: 1616075.7500\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 47720.5703 - val_loss: 1615558.5000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 47573.1328 - val_loss: 1615105.7500\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 47410.0820 - val_loss: 1614767.1250\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 47310.6016 - val_loss: 1614500.5000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 47278.7852 - val_loss: 1614196.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 47255.7617 - val_loss: 1613775.8750\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 47195.0703 - val_loss: 1613267.5000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 47151.0234 - val_loss: 1612803.3750\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 46974.1289 - val_loss: 1612477.7500\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 46918.0000 - val_loss: 1612353.8750\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 46888.8086 - val_loss: 1612344.8750\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 46832.4922 - val_loss: 1612347.2500\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 46792.8555 - val_loss: 1612195.2500\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 46746.4648 - val_loss: 1611862.5000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 46689.3320 - val_loss: 1611451.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 46583.9688 - val_loss: 1611045.3750\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 46490.1484 - val_loss: 1610725.7500\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 46500.7227 - val_loss: 1610487.2500\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 46405.1406 - val_loss: 1610295.1250\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 46352.1875 - val_loss: 1610132.2500\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 46325.9688 - val_loss: 1609944.2500\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 46290.5234 - val_loss: 1609730.2500\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 46240.3828 - val_loss: 1609555.5000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 46198.0000 - val_loss: 1609455.5000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 46156.2695 - val_loss: 1609407.8750\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 46113.9062 - val_loss: 1609404.8750\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 46087.6484 - val_loss: 1609399.1250\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 46026.0117 - val_loss: 1609349.1250\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 45973.7578 - val_loss: 1609338.3750\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 45945.0234 - val_loss: 1609402.1250\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 45911.1953 - val_loss: 1609534.8750\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 45865.8047 - val_loss: 1609688.5000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 45828.5508 - val_loss: 1609802.8750\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 45779.8203 - val_loss: 1609963.3750\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 45731.6875 - val_loss: 1610161.8750\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 45680.3984 - val_loss: 1610376.2500\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 45647.7734 - val_loss: 1610508.2500\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 45595.2852 - val_loss: 1610595.1250\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 45546.0742 - val_loss: 1610672.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 45493.3359 - val_loss: 1610742.6250\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 45434.9219 - val_loss: 1610748.1250\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 45372.2500 - val_loss: 1610702.8750\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 45316.6953 - val_loss: 1610655.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 45251.0234 - val_loss: 1610600.1250\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 45181.2383 - val_loss: 1610487.1250\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 45105.6562 - val_loss: 1610353.7500\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 45024.8711 - val_loss: 1610236.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 44934.5117 - val_loss: 1610107.2500\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 44848.5312 - val_loss: 1609911.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 44746.6875 - val_loss: 1609717.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 44639.8594 - val_loss: 1609555.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 44523.7891 - val_loss: 1609371.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 44392.6250 - val_loss: 1609189.7500\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 44255.3281 - val_loss: 1609058.1250\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 44098.8242 - val_loss: 1608898.6250\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_31 (LSTM)              (None, 10)                480       \n",
      "                                                                 \n",
      " dense_310 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_311 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_312 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_313 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_314 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_315 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_316 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_317 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_318 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_319 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 243769.9531 - val_loss: 43591.7812\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 243622.8281 - val_loss: 43483.4180\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 243453.7812 - val_loss: 43345.4102\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 243237.6875 - val_loss: 43163.1836\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 242951.0312 - val_loss: 42921.8281\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 242569.1562 - val_loss: 42605.1094\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 242064.0781 - val_loss: 42195.4922\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 241404.0781 - val_loss: 41675.1289\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 240554.2344 - val_loss: 41028.1797\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 239478.2188 - val_loss: 40245.4180\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 238143.0469 - val_loss: 39332.2227\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 236527.8438 - val_loss: 38321.0664\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 234638.4844 - val_loss: 37299.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 232535.8750 - val_loss: 36449.4453\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 230384.9531 - val_loss: 36116.0547\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 228535.1250 - val_loss: 36841.9414\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 227603.9844 - val_loss: 39012.4297\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 228209.4219 - val_loss: 41395.6172\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 229535.9844 - val_loss: 42025.4297\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 229781.8281 - val_loss: 40982.4492\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 228883.5625 - val_loss: 39237.1758\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 227655.2656 - val_loss: 37573.8008\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 226723.2500 - val_loss: 36350.7578\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 226287.5625 - val_loss: 35594.9688\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 226245.9844 - val_loss: 35179.8789\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 226391.2969 - val_loss: 34959.3438\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 226541.7656 - val_loss: 34825.6797\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 226583.9844 - val_loss: 34715.9922\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 226466.0938 - val_loss: 34594.6875\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 226179.2969 - val_loss: 34418.6289\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 225741.5469 - val_loss: 34051.6250\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 225182.6406 - val_loss: 33060.1992\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 224531.4844 - val_loss: 32528.6016\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 224315.6719 - val_loss: 32235.2949\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 223570.2656 - val_loss: 33109.1836\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 223442.7656 - val_loss: 34140.1445\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 223685.0625 - val_loss: 34544.7617\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 223871.4844 - val_loss: 33896.0820\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 223647.8281 - val_loss: 32571.0508\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 223005.1250 - val_loss: 32976.6602\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 222893.5156 - val_loss: 31837.1836\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 222291.2344 - val_loss: 31056.4746\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 222218.4219 - val_loss: 30791.4512\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 222264.9375 - val_loss: 30502.6523\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 222141.7344 - val_loss: 30490.8242\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 221849.9844 - val_loss: 31270.5371\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 222205.0938 - val_loss: 30340.1992\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 221642.8125 - val_loss: 30341.7520\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 221800.2969 - val_loss: 30407.1582\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 221454.6719 - val_loss: 31226.5293\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 221404.3750 - val_loss: 31094.6484\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 221137.3906 - val_loss: 30819.5703\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 221196.0469 - val_loss: 30817.9863\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 221049.0625 - val_loss: 31191.9492\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 220994.9688 - val_loss: 30652.8398\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 220793.3594 - val_loss: 29770.9258\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 220711.9688 - val_loss: 29234.4160\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 220498.4531 - val_loss: 29066.0605\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 220205.9219 - val_loss: 28706.2109\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 219979.3438 - val_loss: 28288.7754\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 219870.2344 - val_loss: 28432.5254\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 219496.3750 - val_loss: 28836.0273\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 219314.0781 - val_loss: 29446.6895\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 219326.6406 - val_loss: 29590.2988\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 219219.1719 - val_loss: 29942.8613\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 219283.3906 - val_loss: 30374.2852\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 219204.9219 - val_loss: 30724.1367\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 219218.3750 - val_loss: 30304.4922\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 219054.3281 - val_loss: 29894.4668\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 218996.7969 - val_loss: 29641.8887\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 218876.3125 - val_loss: 29459.0762\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 218905.0625 - val_loss: 28976.7227\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 218880.2031 - val_loss: 28743.9238\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 218874.7656 - val_loss: 28808.6953\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 218778.0781 - val_loss: 28964.2461\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 218726.4844 - val_loss: 28971.4707\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 218653.3281 - val_loss: 29193.0508\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 218633.3906 - val_loss: 29561.8711\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 218612.2656 - val_loss: 29661.7715\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 218583.5781 - val_loss: 29481.1211\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 218538.2812 - val_loss: 29362.0508\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 218456.5938 - val_loss: 29277.6074\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 218420.1250 - val_loss: 28953.4824\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 218373.6250 - val_loss: 28841.4082\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 218350.8125 - val_loss: 28972.6230\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 218309.4062 - val_loss: 28920.7168\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 218238.6094 - val_loss: 28908.4629\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 218193.7656 - val_loss: 29035.3965\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 218149.1562 - val_loss: 28915.6133\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 218105.4531 - val_loss: 28699.9375\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 218070.1562 - val_loss: 28593.4023\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 218018.2031 - val_loss: 28331.6914\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 217962.0312 - val_loss: 28076.2461\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 217912.4531 - val_loss: 28020.1211\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 217853.0625 - val_loss: 27858.8145\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 217782.2031 - val_loss: 27772.4219\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 217708.0781 - val_loss: 27790.2402\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 217629.6875 - val_loss: 27585.7461\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 217539.1719 - val_loss: 27527.0508\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 217443.0312 - val_loss: 27322.2344\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_32 (LSTM)              (None, 10)                480       \n",
      "                                                                 \n",
      " dense_320 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_321 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_322 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_323 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_324 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_325 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_326 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_327 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_328 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_329 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 240340.0156 - val_loss: 83267.6406\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 239737.4844 - val_loss: 82532.7969\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 239094.1562 - val_loss: 81647.3984\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 238320.7031 - val_loss: 80538.6406\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 237355.9375 - val_loss: 79163.3984\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 236166.8906 - val_loss: 77526.8594\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 234766.3750 - val_loss: 75716.0547\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 233245.2812 - val_loss: 73904.7891\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 231803.9688 - val_loss: 72442.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 230804.7969 - val_loss: 71800.8359\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 230683.4062 - val_loss: 71695.1797\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 231024.4688 - val_loss: 71033.3047\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 230637.9219 - val_loss: 69868.2422\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 229595.8906 - val_loss: 68785.5312\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 228518.6719 - val_loss: 68106.8125\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 227775.8125 - val_loss: 67789.5625\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 227380.9375 - val_loss: 67616.4688\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 227162.5469 - val_loss: 67385.6406\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 226944.9219 - val_loss: 66983.2969\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 226623.9219 - val_loss: 66382.1094\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 226172.1562 - val_loss: 65625.1797\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 225626.4531 - val_loss: 64811.9766\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 225073.3750 - val_loss: 64071.7812\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 224616.3125 - val_loss: 63500.1406\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 224300.2344 - val_loss: 63075.4258\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 224022.1094 - val_loss: 62683.8477\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 223584.0625 - val_loss: 62293.4023\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 222922.2188 - val_loss: 62005.0234\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 222172.2188 - val_loss: 61906.9727\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 221498.6406 - val_loss: 61957.9688\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 220947.1250 - val_loss: 62012.4102\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 220447.8750 - val_loss: 61933.8516\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 219917.6094 - val_loss: 61695.2734\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 219353.8906 - val_loss: 61402.6016\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 218852.4375 - val_loss: 61231.8242\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 218517.3906 - val_loss: 61309.2148\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 218315.3438 - val_loss: 61703.1758\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 218143.5938 - val_loss: 62520.4258\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 218076.2969 - val_loss: 63620.2852\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 218245.3906 - val_loss: 64203.7344\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 218423.8438 - val_loss: 63814.0195\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 218368.9531 - val_loss: 62885.5352\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 218203.6250 - val_loss: 62005.9648\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 218014.1094 - val_loss: 61418.3164\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 217741.3125 - val_loss: 61111.8438\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 217478.6406 - val_loss: 60957.8477\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 217355.5312 - val_loss: 60755.4062\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 217332.4531 - val_loss: 60397.7070\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 217317.7188 - val_loss: 59937.4727\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 217294.6094 - val_loss: 59498.4922\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 217291.0938 - val_loss: 59173.3945\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 217305.5938 - val_loss: 58989.6484\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 217300.7812 - val_loss: 58940.9297\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 217259.2969 - val_loss: 59008.7227\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 217203.9375 - val_loss: 59143.8008\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 217156.0781 - val_loss: 59265.8047\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 217105.3906 - val_loss: 59314.1562\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 217034.8750 - val_loss: 59298.0586\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 216955.5781 - val_loss: 59284.3438\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 216895.5938 - val_loss: 59339.0703\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 216862.1875 - val_loss: 59486.0898\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 216838.2656 - val_loss: 59706.5508\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 216815.9844 - val_loss: 59943.0977\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 216803.2031 - val_loss: 60103.9688\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 216796.0938 - val_loss: 60107.8594\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 216777.4688 - val_loss: 59945.9453\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 216743.9531 - val_loss: 59686.2305\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 216709.6719 - val_loss: 59417.5664\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 216680.2812 - val_loss: 59197.2500\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 216648.8750 - val_loss: 59036.5703\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 216615.7969 - val_loss: 58907.7539\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 216590.1250 - val_loss: 58764.3750\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 216572.5312 - val_loss: 58576.4023\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 216556.1406 - val_loss: 58353.4531\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 216540.2188 - val_loss: 58133.8438\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 216528.4219 - val_loss: 57954.4492\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 216518.5938 - val_loss: 57831.5742\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 216504.8750 - val_loss: 57759.8594\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 216487.7812 - val_loss: 57716.2773\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 216471.9219 - val_loss: 57670.3203\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 216456.5312 - val_loss: 57607.1758\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 216439.5938 - val_loss: 57544.6523\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 216423.4844 - val_loss: 57517.2461\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 216409.9531 - val_loss: 57546.4258\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 216396.0781 - val_loss: 57627.1445\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 216381.3438 - val_loss: 57730.3711\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 216368.1250 - val_loss: 57818.2422\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 216355.7344 - val_loss: 57868.8320\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 216342.2656 - val_loss: 57889.1289\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 216328.3750 - val_loss: 57902.4922\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 216315.3281 - val_loss: 57927.4648\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 216302.3906 - val_loss: 57966.4844\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 216289.5938 - val_loss: 58006.3242\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 216278.0938 - val_loss: 58028.0273\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 216267.7031 - val_loss: 58021.8984\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 216257.2969 - val_loss: 57995.5781\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 216246.9219 - val_loss: 57966.9648\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 216236.9375 - val_loss: 57949.8516\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 216226.8906 - val_loss: 57944.7930\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 216216.4844 - val_loss: 57939.3945\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_33 (LSTM)              (None, 10)                480       \n",
      "                                                                 \n",
      " dense_330 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_331 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_332 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_333 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_334 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_335 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_336 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_337 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_338 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_339 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 245849.6094 - val_loss: 43542.7070\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 245657.8125 - val_loss: 43330.1211\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 245442.1875 - val_loss: 43065.2812\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 245172.3125 - val_loss: 42720.3867\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 244819.0781 - val_loss: 42266.7070\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 244351.7344 - val_loss: 41674.8242\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 243737.8281 - val_loss: 40914.8867\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 242943.0156 - val_loss: 39958.8867\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 241932.2344 - val_loss: 38785.4805\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 240673.4219 - val_loss: 37389.5859\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 239145.1562 - val_loss: 35798.8945\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 237349.7500 - val_loss: 34102.6758\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 235337.2812 - val_loss: 32499.4727\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 233246.0156 - val_loss: 31372.2871\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 231367.4531 - val_loss: 31365.2188\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 230228.6406 - val_loss: 33155.2305\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 230475.7656 - val_loss: 35884.3359\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 231738.5312 - val_loss: 37052.1094\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 232266.2500 - val_loss: 36270.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 231645.5781 - val_loss: 34597.3047\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 230576.9062 - val_loss: 32991.2109\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 229711.8125 - val_loss: 31900.8535\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 229313.4531 - val_loss: 31355.4805\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 229327.3750 - val_loss: 31185.4238\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 229565.7812 - val_loss: 31197.8828\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 229848.3125 - val_loss: 31254.2715\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 230056.9375 - val_loss: 31279.9160\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 230135.5000 - val_loss: 31249.1836\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 230072.0000 - val_loss: 31169.4668\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 229882.5781 - val_loss: 31069.8086\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 229602.5625 - val_loss: 30992.3770\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 229280.0469 - val_loss: 30982.6973\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 228970.0469 - val_loss: 31075.3262\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 228723.6875 - val_loss: 31274.6562\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 228571.6719 - val_loss: 31536.2129\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 228506.3438 - val_loss: 31763.9004\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 228474.9688 - val_loss: 31839.3203\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 228397.2188 - val_loss: 31659.2031\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 228196.0469 - val_loss: 31106.9531\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 227807.9688 - val_loss: 30083.5840\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 227225.7188 - val_loss: 32368.1797\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 231242.6719 - val_loss: 29148.1738\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 226952.9062 - val_loss: 29528.2812\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 226866.3281 - val_loss: 30208.1406\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 227116.7188 - val_loss: 30703.2598\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 227245.1875 - val_loss: 31009.4395\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 227253.4062 - val_loss: 31154.9863\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 227163.1250 - val_loss: 31180.3027\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 227003.9844 - val_loss: 31137.8262\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 226811.7812 - val_loss: 31084.8438\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 226621.0156 - val_loss: 31068.4434\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 226455.5938 - val_loss: 31071.7656\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 226322.4375 - val_loss: 30924.2227\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 226139.4531 - val_loss: 30881.7227\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 226026.0781 - val_loss: 31020.8379\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 225994.3906 - val_loss: 31155.5234\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 225888.8281 - val_loss: 31253.8066\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 225713.4844 - val_loss: 31322.8105\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 225497.5625 - val_loss: 31385.9863\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 225283.7188 - val_loss: 31473.5605\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 225129.3438 - val_loss: 31502.0703\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 225028.8594 - val_loss: 31325.1934\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 224883.8125 - val_loss: 31037.7988\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 224733.1250 - val_loss: 30781.1172\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 224629.1875 - val_loss: 30600.7070\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 224548.2031 - val_loss: 30495.8164\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 224451.0781 - val_loss: 30465.6562\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 224315.7500 - val_loss: 30520.3730\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 224144.0469 - val_loss: 30686.0430\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 223967.0625 - val_loss: 30971.1016\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 223829.9688 - val_loss: 31244.2969\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 223696.5469 - val_loss: 31398.5762\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 223515.7344 - val_loss: 31528.5664\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 223343.4062 - val_loss: 31711.5801\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 223179.0938 - val_loss: 31919.4395\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 222961.5000 - val_loss: 32097.7266\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 222679.0156 - val_loss: 32233.0098\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 222234.5000 - val_loss: 32316.2266\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 222216.2656 - val_loss: 32228.9297\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 222178.0156 - val_loss: 32014.1836\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 222052.8281 - val_loss: 31812.8418\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 221904.0625 - val_loss: 31697.5078\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 221708.7500 - val_loss: 31674.7266\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 221435.6250 - val_loss: 31709.6465\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 221176.7812 - val_loss: 31675.7227\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 221162.7656 - val_loss: 31642.8965\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 220929.5781 - val_loss: 31635.3418\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 220679.6406 - val_loss: 31669.1602\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 220569.2344 - val_loss: 31741.3965\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 220453.8594 - val_loss: 31824.0762\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 220345.5781 - val_loss: 31759.9473\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 220235.5781 - val_loss: 31467.7617\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 220100.9062 - val_loss: 31120.0234\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 219984.2188 - val_loss: 30839.9082\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 219914.0781 - val_loss: 30722.9336\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 219774.1562 - val_loss: 30704.5820\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 219616.5938 - val_loss: 30583.1270\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 219541.8750 - val_loss: 30344.6602\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 219485.7031 - val_loss: 30127.1836\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 219407.6094 - val_loss: 29990.9512\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_34 (LSTM)              (None, 10)                480       \n",
      "                                                                 \n",
      " dense_340 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_341 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_342 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_343 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_344 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_345 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_346 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_347 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_348 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_349 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 246993.1719 - val_loss: 51771.5195\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 246893.7031 - val_loss: 51627.7461\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 246776.2031 - val_loss: 51441.0391\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 246623.4531 - val_loss: 51192.4727\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 246419.6094 - val_loss: 50862.4297\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 246148.0000 - val_loss: 50429.0664\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 245789.9062 - val_loss: 49868.2734\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 245324.7500 - val_loss: 49155.0938\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 244730.9219 - val_loss: 48266.5547\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 243988.6406 - val_loss: 47185.2305\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 243083.9219 - val_loss: 45905.9180\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 242015.1406 - val_loss: 44457.2539\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 240807.9062 - val_loss: 42924.9961\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 239540.0938 - val_loss: 41483.2734\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 238379.3906 - val_loss: 40450.1406\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 237587.1406 - val_loss: 40166.6172\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 237431.5781 - val_loss: 40256.7578\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 237472.0938 - val_loss: 39920.0977\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 236984.3750 - val_loss: 39105.7773\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 235985.1406 - val_loss: 38208.2891\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 234882.6562 - val_loss: 37525.8438\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 233977.0781 - val_loss: 37119.1250\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 233337.0312 - val_loss: 36889.5703\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 232874.2188 - val_loss: 36710.7422\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 232469.1875 - val_loss: 36506.2461\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 232047.3594 - val_loss: 36264.5391\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 231597.0312 - val_loss: 36028.0547\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 231161.4219 - val_loss: 35876.6797\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 230824.2188 - val_loss: 35897.6758\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 230681.1094 - val_loss: 36122.7656\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 230776.2969 - val_loss: 36446.4102\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 231016.2500 - val_loss: 36643.5820\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 231180.6562 - val_loss: 36557.1680\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 231109.0781 - val_loss: 36220.5469\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 230829.6719 - val_loss: 35773.4336\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 230478.3281 - val_loss: 35333.7344\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 230168.3750 - val_loss: 34944.4180\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 229936.7031 - val_loss: 34588.9727\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 229759.7812 - val_loss: 34231.5703\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 229596.0312 - val_loss: 33849.0312\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 229418.1406 - val_loss: 33442.6562\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 229225.9688 - val_loss: 33031.5703\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 229040.7656 - val_loss: 32633.2930\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 228886.3281 - val_loss: 32245.4102\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 228767.3906 - val_loss: 31840.4395\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 228660.2656 - val_loss: 31378.0898\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 228530.7500 - val_loss: 30861.7832\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 228384.7656 - val_loss: 30469.4688\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 228279.9688 - val_loss: 30145.9805\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 228069.5781 - val_loss: 29864.3672\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 227826.0625 - val_loss: 29611.8906\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 227577.8125 - val_loss: 29336.3770\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 227331.2188 - val_loss: 29041.7988\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 227146.2031 - val_loss: 28844.4922\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 227104.8281 - val_loss: 28615.4961\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 226845.7188 - val_loss: 28469.3379\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 226658.7031 - val_loss: 28422.2832\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 226623.2969 - val_loss: 28351.4004\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 226624.0938 - val_loss: 28153.2148\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 226549.8438 - val_loss: 27808.3750\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 226390.7188 - val_loss: 27411.8438\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 226264.5625 - val_loss: 27118.9395\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 226278.0469 - val_loss: 26774.8984\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 226132.4375 - val_loss: 26474.8203\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 225987.8281 - val_loss: 26270.3047\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 225945.5469 - val_loss: 26037.5156\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 225881.5938 - val_loss: 25753.3066\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 225762.0625 - val_loss: 25490.7402\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 225654.1250 - val_loss: 25310.0332\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 225582.8125 - val_loss: 25236.3770\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 225454.9688 - val_loss: 25340.1289\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 225140.5938 - val_loss: 25804.8145\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 224468.9688 - val_loss: 26410.5840\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 223910.4375 - val_loss: 26943.7832\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 223872.5312 - val_loss: 27341.0801\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 223953.1562 - val_loss: 27586.5352\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 223982.6875 - val_loss: 27671.0039\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 223908.7656 - val_loss: 27667.0332\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 223789.7031 - val_loss: 27641.0039\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 223688.2969 - val_loss: 27556.3340\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 223575.5000 - val_loss: 27319.0703\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 223383.3594 - val_loss: 26874.6035\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 223107.2500 - val_loss: 26236.9297\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 222825.8438 - val_loss: 25546.1914\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 222707.7812 - val_loss: 25120.8594\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 222844.0938 - val_loss: 25138.4648\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 222652.3125 - val_loss: 25540.3594\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 222337.9688 - val_loss: 26124.8945\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 222228.1562 - val_loss: 26630.9727\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 222167.9062 - val_loss: 26949.8086\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 222058.1875 - val_loss: 27112.9688\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 221929.9375 - val_loss: 27140.1367\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 221792.8594 - val_loss: 27020.9727\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 221624.4375 - val_loss: 26817.7930\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 221461.0625 - val_loss: 26639.4531\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 221354.6562 - val_loss: 26526.4336\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 221258.5781 - val_loss: 26474.5410\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 221098.8750 - val_loss: 26522.0469\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 220919.8750 - val_loss: 26675.4062\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 220795.0938 - val_loss: 26811.4961\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_35 (LSTM)              (None, 10)                480       \n",
      "                                                                 \n",
      " dense_350 (Dense)           (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_351 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_352 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_353 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_354 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_355 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_356 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_357 (Dense)           (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_358 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_359 (Dense)           (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 5s 5s/step - loss: 251188.9062 - val_loss: 82795.6484\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 250865.7656 - val_loss: 82021.8438\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 250506.2969 - val_loss: 81050.6016\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 250059.5000 - val_loss: 79791.3203\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 249488.5156 - val_loss: 78154.2734\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 248761.6094 - val_loss: 76049.2031\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 247855.4219 - val_loss: 73393.5703\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 246764.9844 - val_loss: 70128.5391\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 245520.6250 - val_loss: 66244.3438\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 244216.7812 - val_loss: 61821.8203\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 243038.3125 - val_loss: 57165.1406\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 242263.2656 - val_loss: 52915.7383\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 242120.7812 - val_loss: 49885.9961\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 242293.1562 - val_loss: 48230.5391\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 242012.6562 - val_loss: 47621.9062\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 241250.0312 - val_loss: 47896.0898\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 240425.3438 - val_loss: 48845.2734\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 239847.7500 - val_loss: 50120.2891\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 239580.4844 - val_loss: 51338.9062\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 239521.0938 - val_loss: 52217.1172\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 239526.9375 - val_loss: 52614.4844\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 239492.7500 - val_loss: 52508.2266\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 239367.0938 - val_loss: 51953.3438\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 239140.5156 - val_loss: 51060.7734\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 238828.3125 - val_loss: 50002.8867\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 238456.2969 - val_loss: 49060.5625\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 238082.2812 - val_loss: 47452.3125\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 237714.0156 - val_loss: 45933.6367\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 237388.3125 - val_loss: 44824.8516\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 237137.9219 - val_loss: 44441.3711\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 236817.5156 - val_loss: 44230.7070\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 236444.2031 - val_loss: 45253.3789\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 236313.8906 - val_loss: 44767.0703\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 235915.7812 - val_loss: 45204.1562\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 235790.5781 - val_loss: 45944.8086\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 235641.8125 - val_loss: 46431.7539\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 235443.2031 - val_loss: 47409.7539\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 235296.7500 - val_loss: 48167.7734\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 235207.1406 - val_loss: 47787.6367\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 234999.6719 - val_loss: 47852.3828\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 234871.3906 - val_loss: 48019.6289\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 234766.1250 - val_loss: 48118.4375\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 234679.0156 - val_loss: 48103.7734\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 234615.5312 - val_loss: 48000.3828\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 234559.1719 - val_loss: 48015.3047\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 234469.9375 - val_loss: 47587.5195\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 234331.3438 - val_loss: 46558.0508\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 234164.4219 - val_loss: 46124.8242\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 233981.1719 - val_loss: 45806.4492\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 233841.6250 - val_loss: 45034.5156\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 233725.6719 - val_loss: 44565.6094\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 233657.4844 - val_loss: 44575.6797\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 233607.4062 - val_loss: 44415.4258\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 233576.0781 - val_loss: 43926.3398\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 233540.8906 - val_loss: 43677.3477\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 233507.2812 - val_loss: 43788.3828\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 233467.1250 - val_loss: 43805.8398\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 233433.5625 - val_loss: 43666.3047\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 233405.1094 - val_loss: 43859.1406\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 233384.2500 - val_loss: 44318.4805\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 233363.7500 - val_loss: 44516.4609\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 233328.9062 - val_loss: 44672.6484\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 233281.6406 - val_loss: 44947.8594\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 233218.7969 - val_loss: 45228.5312\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 233150.1406 - val_loss: 45375.9883\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 233080.8281 - val_loss: 45394.9609\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 233013.9219 - val_loss: 45437.5898\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 232951.3438 - val_loss: 45565.7188\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 232891.7031 - val_loss: 45660.3125\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 232835.2031 - val_loss: 45722.2734\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 232779.9688 - val_loss: 45919.8828\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 232725.8438 - val_loss: 46268.1797\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 232671.9531 - val_loss: 46620.2812\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 232618.5781 - val_loss: 46937.7734\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 232564.8750 - val_loss: 47297.0156\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 232510.6250 - val_loss: 47666.6680\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 232454.6094 - val_loss: 47957.0117\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 232396.4844 - val_loss: 48142.9844\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 232335.7656 - val_loss: 48272.4023\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 232273.1719 - val_loss: 48393.6289\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 232207.6562 - val_loss: 48519.7266\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 232136.7031 - val_loss: 48638.4648\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 232059.7031 - val_loss: 48723.6484\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 231976.3125 - val_loss: 48763.9688\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 231886.3281 - val_loss: 48785.5586\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 231790.6094 - val_loss: 48829.5781\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 231689.9219 - val_loss: 48919.2617\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 231585.5000 - val_loss: 49052.3242\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 231480.8438 - val_loss: 49219.4492\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 231376.3125 - val_loss: 49428.5195\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 231265.5781 - val_loss: 49664.9766\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 231148.8906 - val_loss: 49887.2539\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 231032.8594 - val_loss: 50069.5898\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 230921.5156 - val_loss: 50217.8008\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 230813.1406 - val_loss: 50349.4414\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 230704.5469 - val_loss: 50489.6992\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 230595.1406 - val_loss: 50687.4766\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 230487.8750 - val_loss: 50995.8984\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 230389.4062 - val_loss: 51489.0312\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 230300.5938 - val_loss: 51805.7539\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for t in target:\n",
    "    for l in location[:1]:\n",
    "        df = pd.read_csv(r\"{}/Model/{}/{}.csv\".format(path, t, l))\n",
    "        try:\n",
    "            for x in df.columns:\n",
    "                if len(df[x].unique()) == 1:\n",
    "                    df.drop(x, 1, inplace=True)\n",
    "                elif x != t:\n",
    "                    df[x] = MinMaxScaler().fit_transform(df[[x]])\n",
    "\n",
    "            train_size = round(len(df) * 0.8)\n",
    "            train = df.iloc[:train_size, :]\n",
    "            test = df.iloc[train_size:, :]\n",
    "\n",
    "            for e in [100, 300, 500, 1000, 2000]:\n",
    "                for d in [2, 5, 7, 10, 14, 30]:\n",
    "\n",
    "                    X_train, Y_train = buildTrain(train, t, d, 1)\n",
    "\n",
    "                    ### shuffle the data, and random seed is 10\n",
    "                    X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "                    ### split training data and validation data\n",
    "                    X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "                    Y_train = Y_train[:, np.newaxis]\n",
    "                    Y_val = Y_val[:, np.newaxis]\n",
    "\n",
    "                    model = buildOneToOneModel(X_train.shape)\n",
    "                    model.fit(X_train, Y_train, epochs=e, batch_size=512, validation_data=(X_val, Y_val))\n",
    "\n",
    "                    X_test, Y_test = buildTrain(test, t, d, 1)\n",
    "\n",
    "                    predict_test = model.predict(X_test)\n",
    "\n",
    "                    Y_test = Y_test.reshape(-1).tolist()\n",
    "                    predict_test = predict_test.reshape(-1).tolist()\n",
    "\n",
    "                    mae = mean_absolute_error(Y_test, predict_test)\n",
    "                    mse = mean_squared_error(Y_test, predict_test)\n",
    "                    data.append((t, l, e, d, mae, mse))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "resulta = pd.DataFrame(data, columns=['target', 'location', 'epochs', 'days', 'mae', 'mse'])\n",
    "resulta.to_excel(f'{path}/Model/singleLSTM_test.xlsx', index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7z41guyLk0I"
   },
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1643302406349,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "CAbf9qqoLloY"
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 365071,
     "status": "ok",
     "timestamp": 1643303296581,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "b7TpejMyLm8X",
    "outputId": "d221a835-16c0-4bae-c2b1-f8dce7d02c66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_cases United States\n",
      "(1, 1, 16162.91)\n",
      "new_cases United States\n",
      "(1, 2, 16135.85)\n",
      "new_cases United States\n",
      "(2, 1, 16131.28)\n",
      "new_cases United States\n",
      "(2, 2, 16078.49)\n",
      "new_cases United States\n",
      "(3, 1, 16133.28)\n",
      "new_cases United States\n",
      "(3, 2, 16122.68)\n",
      "new_cases United States\n",
      "(4, 1, 16130.07)\n",
      "new_cases United States\n",
      "(4, 2, 16120.63)\n",
      "new_cases United States\n",
      "(5, 1, 16038.92)\n",
      "new_cases United States\n",
      "(5, 2, 15782.03)\n",
      "new_deaths United States\n",
      "(1, 1, 10515.19)\n",
      "new_deaths United States\n",
      "(1, 2, 10448.98)\n",
      "new_deaths United States\n",
      "(2, 1, 10392.49)\n",
      "new_deaths United States\n",
      "(2, 2, 10220.47)\n",
      "new_deaths United States\n",
      "(3, 1, 10349.18)\n",
      "new_deaths United States\n",
      "(3, 2, 10189.61)\n",
      "new_deaths United States\n",
      "(4, 1, 10249.05)\n",
      "new_deaths United States\n",
      "(4, 2, 10055.24)\n",
      "new_deaths United States\n",
      "(5, 1, 10114.28)\n",
      "new_deaths United States\n",
      "(5, 2, 9978.84)\n",
      "icu_patients United States\n",
      "(1, 1, 10978.22)\n",
      "icu_patients United States\n",
      "(1, 2, 10980.19)\n",
      "icu_patients United States\n",
      "(2, 1, 10979.55)\n",
      "icu_patients United States\n",
      "(2, 2, 10981.48)\n",
      "icu_patients United States\n",
      "(3, 1, 10981.0)\n",
      "icu_patients United States\n",
      "(3, 2, 10979.58)\n",
      "icu_patients United States\n",
      "(4, 1, 10973.26)\n",
      "icu_patients United States\n",
      "(4, 2, 10974.98)\n",
      "icu_patients United States\n",
      "(5, 1, 10975.2)\n",
      "icu_patients United States\n",
      "(5, 2, 10971.72)\n"
     ]
    }
   ],
   "source": [
    "data1 = []\n",
    "for t in target:\n",
    "    for l in location:\n",
    "        df = pd.read_csv(r\"{}/Model/{}/{}.csv\".format(path, t, l))\n",
    "        # try:\n",
    "        for x in df.columns:\n",
    "            if len(df[x].unique()) == 1:\n",
    "                df.drop(x, 1, inplace=True)\n",
    "            elif x != t:\n",
    "                df[x] = MinMaxScaler().fit_transform(df[[x]])\n",
    "        Order = []\n",
    "        for p in range(1, 6):\n",
    "            for q in range(1, 3):\n",
    "                try:\n",
    "                    model = ARIMA(df[t], order=(p, 1, q))\n",
    "                    results = model.fit(disp=0)\n",
    "                    print(t, l)\n",
    "                    temp = (p, q, round(pd.read_html(results.summary().as_html(), header=0, index_col=0)[0].iloc[2, 2], 2))\n",
    "                    print(temp)\n",
    "                    Order.append(temp)\n",
    "                except:\n",
    "                    pass\n",
    "        if Order:\n",
    "\n",
    "            result1 = min(Order, key = lambda x:x[-1])[:2]\n",
    "            p, q = result1[0], result1[1]\n",
    "\n",
    "            value = df[t].values\n",
    "            length = int(len(value) * 0.8)\n",
    "            train1 = list(value[0:length])\n",
    "            test1 = list(value[length:])\n",
    "            predictions = []\n",
    "\n",
    "            for i in range(len(test1)):\n",
    "                try:\n",
    "                    model = ARIMA(train1, order=(p, 1, q))\n",
    "                    model_fit = model.fit(disp=0)\n",
    "                except:\n",
    "                    model = ARIMA(train1, order=(p, 1, 1))\n",
    "                    model_fit = model.fit(disp=0)\n",
    "                pred = model_fit.forecast()[0]\n",
    "                predictions.append(pred)\n",
    "                train1.append(test1[i])\n",
    "            mae = mean_absolute_error(test1, predictions)\n",
    "            mse = mean_squared_error(test1, predictions)\n",
    "\n",
    "            data1.append((t, l, p, q, mae, mse))\n",
    "\n",
    "resulta = pd.DataFrame(data1, columns=['target', 'location', 'p', 'q', 'mae', 'mse'])\n",
    "resulta.to_excel(f'{path}/Model/ARIMA_test.xlsx', index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kp2DcQHEMJjJ"
   },
   "source": [
    "# **Model Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5173,
     "status": "ok",
     "timestamp": 1643304697805,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "FTqzNkTYML_z",
    "outputId": "be89a674-36bd-42e7-ff61-4254e15db211"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1643304697806,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "QI99AIXRMRhW"
   },
   "outputs": [],
   "source": [
    "target = ['new_cases', 'new_deaths', 'icu_patients']\n",
    "location = ['United States', 'United Kingdom', 'Turkey', 'Germany', 'Canada', 'South Africa', 'South Korea', 'Japan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DzFVxFfMa5w"
   },
   "source": [
    "### 將ARIMA的最佳結果繪製成圖表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1643304713344,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "rQTdtmLKMU9W"
   },
   "outputs": [],
   "source": [
    "arima_model = [('new_cases', 'United States', 5, 2),\n",
    " ('new_cases', 'United Kingdom', 4, 2),\n",
    " ('new_cases', 'Turkey', 3, 2),\n",
    " ('new_cases', 'Germany', 5, 2),\n",
    " ('new_cases', 'Canada', 5, 2),\n",
    " ('new_cases', 'South Africa', 4, 2),\n",
    " ('new_cases', 'South Korea', 1, 2),\n",
    " ('new_cases', 'Japan', 5, 2),\n",
    " ('new_deaths', 'United States', 5, 2),\n",
    " ('new_deaths', 'Turkey', 1, 2),\n",
    " ('new_deaths', 'Germany', 5, 2),\n",
    " ('new_deaths', 'Canada', 4, 2),\n",
    " ('new_deaths', 'South Africa', 5, 2),\n",
    " ('new_deaths', 'South Korea', 1, 2),\n",
    " ('new_deaths', 'Japan', 5, 2),\n",
    " ('icu_patients', 'United States', 4, 1),\n",
    " ('icu_patients', 'Germany', 1, 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "executionInfo": {
     "elapsed": 156964,
     "status": "ok",
     "timestamp": 1643305257807,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "bXNC2btCMXlq",
    "outputId": "1101c674-0278-4073-bdb2-b094e3cb99eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United Kingdom - new_cases\n",
      "Test Data : [23401.0, 25713.0, 30871.0, 29323.0, 25782.0, 24173.0, 22287.0, 21855.0, 29151.0, 30009.0, 31633.0, 28344.0, 27244.0, 25056.0, 23469.0, 29381.0, 32885.0, 32539.0, 29226.0, 26484.0, 28358.0, 26709.0, 33758.0, 36533.0, 37128.0, 31886.0, 32034.0, 31856.0, 30762.0, 35618.0, 38117.0, 37854.0, 32142.0, 32992.0, 26285.0, 32167.0, 35577.0, 37982.0, 42355.0, 36725.0, 36545.0, 40976.0, 37535.0, 38579.0, 37570.0, 37012.0, 28899.0, 28641.0, 30416.0, 26763.0, 29901.0, 26619.0, 32566.0, 29476.0, 29298.0, 35943.0, 31188.0, 33829.0, 35847.0, 35716.0, 29976.0, 33068.0, 37583.0, 34847.0, 35266.0, 35833.0, 34684.0, 29520.0, 29719.0, 34502.0, 33190.0, 38922.0, 39898.0, 35120.0, 39493.0, 33654.0, 39738.0, 37855.0, 41669.0, 44745.0, 44387.0, 42899.0, 44803.0, 48965.0, 43628.0, 48798.0, 51719.0, 48992.0, 45102.0, 38846.0, 36369.0, 43739.0, 44122.0, 39478.0, 43258.0, 41009.0, 37732.0, 39999.0, 33712.0, 41530.0, 36963.0, 33972.0, 30454.0, 29889.0, 32325.0, 32899.0, 39699.0, 43511.0, 39149.0, 37807.0, 36217.0, 40665.0, 37130.0, 38254.0, 48053.0, 44835.0, 40153.0, 39548.0, 45653.0, 43020.0, 42950.0, 47287.0, 49829.0, 39834.0, 36591.0, 42874.0, 40501.0, 47927.0, 53528.0, 51181.0, 41574.0, 43361.0, 51746.0, 45473.0, 51003.0, 50545.0]\n",
      "Prediction : [25495.970691530267, 24378.12882258483, 26635.456984792145, 30774.428483580486, 29008.874231514306, 24715.28972639049, 23547.4696910486, 23094.126077874378, 23209.600225070903, 29829.5441453477, 30252.918283571154, 29896.78445686206, 27341.8915661462, 26604.035139449217, 26283.130258433182, 25439.55267283397, 30642.533881392286, 32934.7961655384, 31095.566623060753, 27068.46890840823, 25211.714987295258, 28681.800938125063, 29478.51993536443, 33959.74392884952, 36526.051534708895, 35377.201698118275, 29907.227040140217, 30309.409628674577, 32881.58992760059, 33609.75588774586, 37136.54752074846, 38211.07310260896, 36085.957824788566, 30415.189784789767, 31367.214939772195, 28690.111883424226, 33805.79313995635, 37685.82264340794, 37871.944268209205, 39225.7948135575, 34559.95105425166, 34531.483888560644, 41043.07423673437, 41043.99839297637, 40787.69633964656, 38410.116531947606, 35882.29432773936, 28479.347521474047, 27849.34916054131, 31472.407094539303, 29904.81965408978, 30991.7227213661, 27362.35192787132, 30002.214386857282, 28554.60744137255, 28315.86946878983, 35040.183304205115, 33402.24507986854, 34059.77283204097, 35585.73506918568, 35063.97761531371, 30079.46583376873, 31988.524391344326, 37268.30269475399, 35143.894244640476, 34392.586105901064, 35569.398503692675, 34311.2150481182, 30169.96470103738, 30067.477192291204, 34641.45514206457, 34475.148599100125, 37774.009985317985, 38544.8993697267, 34578.20497219771, 37682.279347596304, 35373.25448609542, 39858.61246952168, 39625.342479583625, 41039.83587325008, 42955.45447041502, 42842.95133869008, 42090.54422403851, 44453.8432830227, 49206.434204227204, 46106.509511082455, 47952.508962295295, 50296.63226539687, 48445.43714921198, 45166.069366701115, 40552.35208653989, 38535.74131637079, 44235.31418160324, 45255.56860365292, 40022.859485969, 40615.271266217016, 39840.48264352335, 38501.65682849089, 41004.1649078567, 37162.90843601196, 41059.669699283324, 37728.03033678183, 33498.10776595162, 30085.13692137298, 30217.11170490767, 33338.614968856506, 34609.90914191897, 38749.85031577343, 41376.91666799596, 37874.244646850006, 36332.89118242711, 36559.15409596491, 41547.65564359942, 40289.84762007342, 39164.29373124549, 44730.77955213185, 43195.73551067011, 39499.623714392364, 39381.925043336945, 45697.54106877259, 45785.49910283046, 44471.468657031925, 45561.3490991317, 47054.36812953536, 40443.90196240459, 37694.00989140534, 43073.825440600216, 43278.20767428619, 47862.76280827054, 50894.472102991866, 48846.67566388137, 41429.51643944901, 42836.854355354284, 51475.10983003793, 49621.774817651, 52145.22629590304]\n",
      "MAE : 2911.2368671961303\n",
      "MSE : 12702971.608056663\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9eXxcVf3//zyTbTIzSWayb22T7istbSmtoNKCUBYr4oYfEHBXXPD3+bjh1498fiKK/D4ooIArFn+IKCiCQJEdZKcFWkqbtmmaNvskk5lk9kwy5/vHuXcySSZrkyZtz7OPeXTuudu5N8l93fdy3kdIKdFoNBqNZiJYprsDGo1Gozl+0SKi0Wg0mgmjRUSj0Wg0E0aLiEaj0WgmjBYRjUaj0UwYLSIajUajmTBaRDTHHCHEr4QQ/z2Jx6sXQpwzwX3fFUKcNVl9GXRsKYSYPxXH1mhmClpENOMm1cNRCPE/Qoh7xrK/lPJLUsrrjf3OEkI0TkU/jeNvFUL8KGl5mRCiRQjxTaMvy6SUz03V+TWaEx0tIpqTBiHEqcCzwI+klP873f3RaE4EtIhoJh3TuhBC/JcQwm28+X86af1WIcSPhBB2YBtQLoQIGJ9yIYRFCPFdIcRBIYRHCPFXIUR+0v6fEkIcNtb9nzH2aR3wJPA9KeXtSe0JV5hhTf1VCPFHIYTfcHWtTdp2tRDiLWPd/UKIvwyycr5lXGuzEOIzg86fZxy33ej794UQFmPdVUKIl4QQPxdC+IQQdUKI9xjtDcY9vHK8P4ekc492XeVCiL8ZfTskhPi60W4VQoSFEIXG8v8RQvQKIXKN5euFELeMcu5sIcTNxjV3CSFeFEJkG+vuF0K0Gu0vCCGWJe13gRBij9HfJtNyNNZdJIR427hXLwshTkla9x1je78QYp8Q4uyJ3jfN2NAiopkqSoE8oAL4LHC7EMKVvIGUMgicDzRLKR3Gpxn4GnAx8H6gHPACtwMIIZYCdwKfMtYVAJWj9GUd8Djw/0gpfzfKtluA+wAn8DDwS+O8mcCDwFYgH/gz8GFzJyHEZuCbwAeABcDgGM0vjPsx17iuK4BPJ60/HdhlXM+9Rh9OA+YDlwO/FEI4Run7RK7LAvwT2In6WZ0NfEMIcZ6UMgK8YfQX4//DwBlJy8+Pct7/BdYA70Hdt28DcWPdNtS9KgbeBP6UtN/vgS9KKXOA5cAzRn9PBe4Cvoi6V78GHhZCZAkhFgFfBU4z9jsPqB/T3dFMHCml/ujPuD6ABOYPavsf4B7j+1lAGEhPWu8G1hvft6JcSua2jYOOtRc4O2m5DIgB6cAPgPuS1tmBHuCcYfq6FegGDgGFKdbXm/sa1/BU0rqlQNj4/j6gCRBJ619Muo67gBuT1i007xOQZvRxadL6LwLPGd+vAg4krVth7FuS1OYBVk3w5zXSdZ0OHBm0/bXAH4zv1wO3Gfe+FbgGuBGwGj/jghHOazG2WTmGPjqNa84zlo8Y9yh30HZ3AtcPatuHErT5xu/ZOUDGdP+dnCwfbYloJkIfkDGoLQP1oDfxSCl7k5ZDwFjfpOcADxruCh9KVPqAEpT10WBuKJU14xnleLcD24EnB1tDKWgd1GerECLdOG+TNJ5aBg1J38sHLR9O+l6Iuj+HB62vSFpuS/oeBpBSDm4bcv+EEO9NcgW+O+xVDX9dc1DuRF/S/f4e6l6DsjTOAlYD76Bcgu8H1gO1UsqR7n0hSmwOpuh3mhDiRsNl2U2/xVBo/P8R4ALgsBDieSHEBqN9DvBfg/o7CyiXUtYC30CJplsIcZ8QonyE/mkmAS0imolwBKga1FbNwIfkWElVRroBOF9K6Uz6WKWUTUAL6qEBgBDChnJrjEQf8B9Gv/9l+vTHSQtQIYQQSW2zBq1PXp6d9L0DJbBzBq1vmkA/BiCl/LfsdwUuG32PITQAhwbd6xwp5QXG+peBRSjX3fNSyj1G3y9gdFdWBxAB5qVY9x/Ah1BWQx79v0/CuK43pJQfQrm6/gH8Nam/Nwzqr01K+Wdjv3ullGei7rUEfjqem6EZP1pENBPhL8D3hRCVQgXBzwE+CDwwgWO1AQVCiLyktl8BNwgh5gAIIYqEEB8y1j0AXCSEONOIU/yQMfweSyljwMdQD7bHhArqj4dXUGL0VSFEutGfdUnr/wpcJYRYagjbdUnn7jPW3yCEyDGu6z+BMaVETzGvA34jIJ1tWAjLhRCnAUgpQ8AO4Cv0i8bLwJcYRUSklHGUm+9nRvA+TQixQQiRBeQAUZQVaQN+bO4nhMgUQlwmhMgzfm7d9MdRfgt8SQhxulDYhRAXGvd1kRBik3H8CMp6i6OZUrSIaCbCD1EPkhdRQe+bgMuklLvHeyApZQ0qSF1nuCfKgVtRwd8nhBB+4FWU7x4p5buoB9q9qLd/LzCmcSZSyh7gEtQD5p9mltA49/0s4EMFux9BPQiRUm4DbkEFgGuN/5P5GhAE6lD37V7UA3ZaMQTuImAVKm7UAfwOZR2YPI9yx72etJwDvDCGU3wT5QZ7A+hEWQYW4I8oy7UJ2IP6GSfzKaDecHV9CbjM6O924POoxAAv6l5fZeyThYrXdKDcd8Wo+I5mChEDXbwajWasCCFeA34lpfzDdPdFo5kutCWi0YwRIcT7hRClhjvrSuAUVOqwRnPSokVEoxk7i1DjKXzAfwEflVK2TG+Xph+hBi8GUnwum+6+aaYe7c7SaDQazYTRlohGo9FoJkz6dHfgWFNYWCirqqqmuxsajUZzXLFjx44OKWXR4PaTTkSqqqrYvn37dHdDo9FojiuEECkHE2t3lkaj0WgmjBYRjUaj0UwYLSIajUajmTAnXUwkFbFYjMbGRiKRyHR3ZUZhtVqprKwkI2NwwV6NRqNRaBEBGhsbycnJoaqqioFFWk9epJR4PB4aGxuprq6e7u5oNJoZinZnAZFIhIKCAi0gSQghKCgo0NaZRqMZES0iBlpAhqLviUajGQ0tIhqNZsbxdN3T1HTUTHc3NGNAi8gJQlVVFR0dHdPdDY1mUrjs75fx38/+93R3QzMGtIjMQKSUxON6QjbNyUmgJ0BbsI0DngPT3RXNGNAiMkOor69n0aJFXHHFFSxfvpzrr7+e0047jVNOOYXrrkvMtMrFF1/MmjVrWLZsGb/5zW+msccazdRwyHsIgNrOWnSV8ZmPTvEdxDce/wZvt749qcdcVbqKWzbfMup2Bw4c4O6776a7u5sHHniA119/HSklW7Zs4YUXXuB973sfd911F/n5+YTDYU477TQ+8pGPUFBQMKn91WimkzpvHQDBWJDWQCtlOWXT3CPNSGhLZAYxZ84c1q9fzxNPPMETTzzBqaeeyurVq6mpqeHAAWXa33bbbaxcuZL169fT0NCQaNdoThRMEQFljWhmNtoSGcRYLIapwm63Ayomcu211/LFL35xwPrnnnuOp556ildeeQWbzcZZZ52lx3FoTjgGi8h757x3GnujGQ1ticxAzjvvPO666y4CgQAATU1NuN1uurq6cLlc2Gw2ampqePXVV6e5pxrN5FPnq2NF8QrSLenaEjkO0JbIDOTcc89l7969bNiwAQCHw8E999zD5s2b+dWvfsWSJUtYtGgR69evn+aeajSTT523jmVFywj3hjnQqd21M50pFREhRD3gB/qAXinlWiFEPvAXoAqoBz4upfQKNTz6VuACIARcJaV80zjOlcD3jcP+SEp5t9G+BtgKZAOPAdfI4zSdo6qqit27dyeWr7nmGq655poh223bti3l/vX19VPVNY3mmBGXcQ55D/HBhR8kGAsqS6SvD9LSprtrmmE4Fu6sjVLKVVLKtcbyd4GnpZQLgKeNZYDzgQXG5wvAnQCG6FwHnA6sA64TQriMfe4EPp+03+apvxyNRjNVtPhbiPZFmeuay4L8BeS/tQ/pcEBd3eg7a6aF6YiJfAi42/h+N3BxUvsfpeJVwCmEKAPOA56UUnZKKb3Ak8BmY12ulPJVw/r4Y9KxNBrNcYgZVJ/rmsv8/PmsORhCRCLw/PPT3DPNcEy1iEjgCSHEDiHEF4y2Eilli/G9FSgxvlcADUn7NhptI7U3pmjXaDTHKYd8aqBhtbOa+fnzWeY2VrzxxvR1SjMiUx1YP1NK2SSEKAaeFEIMqKgmpZRCiCmPYRgC9gWA2bNnT/XpNBrNBKnz1iEQzHHOAaC43Vjx+uvT1ynNiEypJSKlbDL+dwMPomIabYYrCuN/812jCZiVtHul0TZSe2WK9lT9+I2Ucq2Ucm1RUdHRXpZGo5ki6rx1zMqbRWZaJnNyZ7GkA+IWAbt2QTQ63d3TpGDKREQIYRdC5JjfgXOB3cDDwJXGZlcCDxnfHwauEIr1QJfh9voXcK4QwmUE1M8F/mWs6xZCrDcyu65IOpZGozkOqfPWMdc1F4DMplbsMXjnlBKIxWDnzmnunSYVU2mJlAAvCiF2Aq8Dj0opHwduBD4ghDgAnGMsg0rRrQNqgd8CVwNIKTuB64E3jM8PjTaMbX5n7HMQSJ3/epLx3HPPcdFFFwHw8MMPc+ONNw67rc/n44477jhWXdNoRqTOW8dcpxIR9uwB4C9rs9WydmnNSKYsJiKlrANWpmj3AGenaJfAV4Y51l3AXSnatwPLj7qzxwl9fX2kjTNffsuWLWzZsmXY9aaIXH311UfbPY1mYnz843DVVYQ+cBYtgZaEJWKKyL1lHdxQWorQwfUZiS57MkOor69n8eLFXHbZZSxZsoSPfvSjhEIhqqqq+M53vsPq1au5//77eeKJJ9iwYQOrV6/mYx/7WKI0yuOPP87ixYtZvXo1f//73xPH3bp1K1/96lcBaGtr48Mf/jArV65k5cqVvPzyy3z3u9/l4MGDrFq1im9961vTcu2ak49gT5DDvsMQDML998PDD1Pvqweg2lWtNnr3XYIFORxO89OzeqW2RGYouuzJYL7xDXh7ckvBs2oV3DJ6Ycd9+/bx+9//njPOOIPPfOYzCTdTQUEBb775Jh0dHVxyySU89dRT2O12fvrTn/Kzn/2Mb3/723z+85/nmWeeYf78+XziE59Iefyvf/3rvP/97+fBBx+kr6+PQCDAjTfeyO7du3l7sq9ZoxmBn770U36949e0ffQ11VBfP2CMCAB79hBaUA3son3pHCq3PQHd3ZCbOz2d1qREWyIziFmzZnHGGWcAcPnll/Piiy8CJETh1VdfZc+ePZxxxhmsWrWKu+++m8OHD1NTU0N1dTULFixACMHll1+e8vjPPPMMX/7ylwFIS0sjLy/vGFyVRjOU2s5a3EE30RZjqFd9fcISqXJWgZSwZw9yyWIAjiwqU207dkxPhzXDoi2RwYzBYpgqVJLZ0OXkEvEf+MAH+POf/zxgO21FaI432oJtAASaDpEFcPgw7QE3AkGRrQgaGyEQIHPFKuj+K/uqHLwHlEtr48Zp7LlmMNoSmUEcOXKEV155BYB7772XM888c8D69evX89JLL1Fbq8pjB4NB9u/fz+LFi6mvr+fgwYMAQ0TG5Oyzz+bOO+8EVJC+q6uLnJwc/H7/VF2SRpOS1kArAKGWI6ohEqGnuQGn1UmaJS0RVHecuh6B4FBaN8ybp0euz0C0iMwgFi1axO23386SJUvwer0J15NJUVERW7du5ZOf/CSnnHIKGzZsoKamBqvVym9+8xsuvPBCVq9eTXFxccrj33rrrTz77LOsWLGCNWvWsGfPHgoKCjjjjDNYvny5DqxrjhmmiPS09o8PTm9opMBmTPX87ruqbfkKShwltPhbYNky0DN5zji0O2sGkZ6ezj333DOgbXCJ902bNvFGirexzZs3U1NTM6T9qquu4qqrrgKgpKSEhx4aOh7z3nvvnXinNZpx0tPXQ2dYDfXqdbck2q2NrRSuLlQLe/ZAUREUFlLmKKM50Ayz58ILL0xHl2cc9b56FTuaAWhLRKPRHFPcQXf/Qns7GAkeOc0eCrINS2TPHli6FIDynHJlicyeDT6fytA6idnRvIPqW6vZ0Twzkgy0iMwQBk9KpdGcqJiuLABLRyfMmQOFheS3+ZU7q68P3nkHlqtxxGWOMloChogANDSkOuxJQ7O/GYB329+d5p4otIgYHKcTIk4p+p5opoJkEcn0dim3VVUVxR1hZYnU1EAgAOvWAcoSaQu00VtZrnY6fHg6uj1jCMVCAGqw5gxAiwhgtVrxeDz6oZmElBKPx4PVap3urmhOMEwRsQgLVm8AioromzObWd4+JSLmyHRDRMpyypBIOgptqv3Ikeno9owhISJdM0NEdGAdqKyspLGxkfb29tE3PomwWq1UVlaOvqFGMw7aAmqMyDzXPBxd9VBURFjEqPJBQXa+SuPNzYWFCwHlzgJotPVSmp6uRcQQkSNdM+M+aBEBMjIyqK6unu5uaDQnBa2BVpxWJ7OzS7GHDkBREV3pESp6oTyUriyRtWvBohwl5TnKjdUScsOsWVpEZpglot1ZGo3mmNIabKXUUcqcmKrEQGEhnmIHAJUtATVvyGmnJbYvy1GWSLO/WQXXtYgAyhKZCS54LSIajeaY0hpQIjK7x5gnpKiI1sIsAGY99yb09ibiIQAl9hIEoj9DS4sIAJHeCO2h6XfBaxHRaDTHlLZAGyX2EsqimQDIwkIanWqeHNcjT6uNkkQkIy2DIntRvyXS2KiE5iTFFBGYGRlaWkQ0Gs0xxbRESsPq8RN2OWi1BOnIhvSmFigthYqKAfuU55T3WyJ9fdDSkurQJwXBWDDxfSbERbSIaDSaY0awJ4i/x0+po5TCoPLndzrS8IQ8HHEZj6N162BQResyR1m/JQIntUsrFAupSsdoS0Sj0ZxkmCXgSx2l5Af6iAMd1jiesCcRF0l2ZZmUOcr6S5/ASS8i5Tnl5GTmzIg0Xy0iGo3mmGGOESmxl5DbHcVjA0/UhyfsocPI0ErOzDIpzymnLdhGX6Xh5jrJRcSeaWeOc452Z2k0mpMLc7R6qaMUe3eYDht0hjvpCHXQOL9YDTJMISJlOWXEZRy3CEF+/kkvIrYMG7PzZmsR0Wg0JxfJImL1Bmi3gSfswRPysHPTUmhuBpdryH6JAYc6zTchInPy5uiYiEajObloC7apKXDtRWR0+mi3K0vEE/ZQaCsCYyrowZilTxLB9ZO4CGMoFsKeYWdO3hy8ES/+6PAzk+5o3kGwJwgPPwyXXw6h0LDbThQtIhqN5pjRGmil0FZIuiUdS3sHPkc67cF2vGFv/6yGKUhYIn5tiSQsEeccYPgaWpHeCO+56z3c9tpt8NZb8Kc/QWbmpPdHi4hGozlmmGNEiMfB4yHgtHHQexCJ7J+QKgUljhIgyRLp6lKfk5DkmAgMP1bEG/bS09ej5h3x+cDhgPTJL5c45SIihEgTQrwlhHjEWN4qhDgkhHjb+Kwy2oUQ4jYhRK0QYpcQYnXSMa4UQhwwPlcmta8RQrxj7HObEIOSyzUazYwiISJeL8TjRF05HOhU86aPZIlkpmVSaCtUMZE56g181MmppITvfQ9efnmyuj8jSI6JwPCWiDfiBWC/Z78SEadzSvpzLCyRa4C9g9q+JaVcZXzeNtrOBxYYny8AdwIIIfKB64DTgXXAdUIIM/J2J/D5pP02T+WFaDSao6Mt2KZExJh2IZafx8HOgwAjWiKgXFrN/mZVyRdGd2nt2AE/+Qncd99R93um0BfvI9oXxZZhoyynjAxLxrDBdV/EBygRkV1dx6eICCEqgQuB341h8w8Bf5SKVwGnEKIMOA94UkrZKaX0Ak8Cm411uVLKV6UqZflH4OKpuRKNRnO0SCn7LRFDROKFhcTiMWBkSwSg2F5MR6hDlUUBcLtH3J4//EH97/EcVb9nEuHeMAC2DBsWYaEyt3JYd5YpIl3RLmKe/rnsJ5uptkRuAb4NxAe132C4rH4uhDCGqVIBJNunjUbbSO2NKdqHIIT4ghBiuxBiu554SqOZOmo7aznkPZRyXXe0m0hvhBJ7SUJERFFRYv1olojT6lQPRnOfkUQkEoF771XfTyARCfaoulm2DDXL49LMCnyNtSm3NUUEoMfjPv4sESHERYBbSrlj0KprgcXAaUA+8J2p6oOJlPI3Usq1Usq1RUm/tBqNZnL51IOfYtWvV/HC4ReGrKvpqAFgrmtuQkQySsoT60ezRPKy8uiKdqk0YKs1cYyUPPSQigPk5Z1QImJW8DVF5Np7G7jx5l0pt00WkXiX9/gTEeAMYIsQoh64D9gkhLhHStliuKyiwB9QcQ6AJmBW0v6VRttI7ZUp2jUazTRxwHOA7mg3591zHo/sf2TAuu3N2wE4reI06OgAIKtEOQ/SRBp5WSO7WxKWiBBQXDyyJfKHP6jYyQUXnNAiUubp4ZSGaEpB9YZVYD1NpJHe5T/+3FlSymullJVSyirgUuAZKeXlRiwDI5PqYmC3scvDwBVGltZ6oEtK2QL8CzhXCOEyAurnAv8y1nULIdYbx7oCeGiqrkej0YyMP+rHE/bwzQ3fZHnxci6+72J2tu5MrN/esp1iezEVORXQ1AQuF06nSt0tsBUwWnKl0+okFAvR09ejXFrDWSKNjfDEE3DllWq7E1hEcoMqnhR75qkh2/oiPrLTs5nnmktWMHpcWiLD8SchxDvAO0Ah8COj/TGgDqgFfgtcDSCl7ASuB94wPj802jC2+Z2xz0Fg2zG6Bo1GMwgzwLu2fC3bLttGXMb5R80/Euu3N29nbflaJRZ798KSJYk4yGjxEFAiAtAV6VKWyDAi0vvnP6n03quugoIC6O6GWOwor25mYIqIPUON7LcFogBEn/7XkG19ER9Oq5MV9rmkxeWUicjkjzxJgZTyOeA54/umYbaRwFeGWXcXcFeK9u3A8snqp0ajmTj1vnoAqpxVFNoKWV22mmfqn+E6riPYE2RP+x4uWXyJ2njPHvjgB8nPzgdGj4dAv4j4Ij6Kiorg3XeHbOOP+nni8VvYaBO45s5F5Kvj4/Uq4TnOGWCJSIm1Wy2nvfDikG19UR+ubBcrMlU0IJ6bMyVWgx6xrtFoJoVkEQE4u/psXml4hWBPkLdb3yYu46wtX6viIW43LF2aEI+xWCJmzKQrmmSJSJlY3x3tZvOfNhN1t9KZJemOditLBE4Yl9YAEQkGsfT20WaH7H0Hh8SITEtkcYaqO+bNkkOONxloEdFoNJPCIe8hstOzKbarN/5N1ZuIxWO81PBSIqi+pnyNcmUBLF3ab4mMw52VSPMNhyGoUl7jMs6F917I602vsyS9hM5saA+1n9gi0qm8+v9YbKx8/vkB23rDXpxWJ/OEugcNluELNR4NWkQ0Gs2kUN9VT5WzKhEgP3P2mWRYMni67mm2t2ynPKdcFVLcs0ftYIiIQFBoKxz1+ANExHRNGXGRZn8zLx55kR+e9UOqpRNvNrQHZ56IdIY7kXLiFkEqEXlqLvTYsuDZZwdsa1oic1AW3KF4J1OBFhGNRjMp1PvqE64sAHumnfWV63mm/plEUB1QIuJwwKxZpFvSufcj9/Ll07486vGHWCKQcOF0R7sBqHZVY+0O47WCO+ieUSLS0NVAyf+W8PShpyd8jFQi4nFYOLS8Ep57bsC2vogPZ5aTgpgKfe/rbZvweUdCi4hGo5kUBosIKJfWjuYd7OvYx9qyJBFZskSN9wAuXX7pkP1SMZIlYs6pkZOZQ0Z3YIg7K9TawL3v3HuUV3h07Pfspzfemxh0ORFSiUjc5WT3kgLlJmxVk35JKfFFVGDd0qUE9t3eqRlGp0VEo9EcNd3RbjrDnUPE4Ozqs5HGvwGWyJIl4z6HI9OBRVhUiu8gS8Tfo0QkNzMHi6+r353lcEBGBntq/s1lf7+M2s7UJUKOBc3+ZqB/nvmJEIqFSLekk5GWoTLOgLTCIl6bb1UbvKiytIKxIH2yTwmvT41c3xmun3jnR0CLiEajOWrMSrIJEenthTvvZH1thFxLNmAE1bu61BS4S5eO+xxCCPKy8ga6swxLxHRn5cUsiL4+gvZM5c4SAgoK6GtXYrPbvTvlsY8FLYEWQFUynijBWDAx0NC0RDILStjlVONFzBkfzdHqTqsTurqIZaSxN3CI3njvhM89HFpENBrNUXPIp4ouJkTkySfh6qvJOOdcmn8S47aXnSprKykzayI4rU58UZ+qn2WzDXFnOcMqaN3nzFXuLICCAizGW/t0iohpibiDo1QfHgFzLhFAiUhWFrmuEuriHnU/mpTLyqybZVoi8bxcvr7u60R6I0d3ESnQIqLRaI6awWNE2L5dWQF//jOW972frz3hgzffHJCZNRES9bNAWSODAusOowyIdDkHiEiGT4nMTBCRo7FEhohIfj5F9mLawx1QUTGsiGTlF3HzeTfjyHQc3UWkQIuIRqM5aup99dgybBTZDDfTjh2waBFceinZf/kbuFzwgx8oEbFaoapqQucZICJJpU/MmIgj0ANAWn5h/xt/QQHZ3WoejhkhIkcZEzFLnpgiUmwvxhfxES8rGyIiLqtLuRCnqOQJaBHRHEfsdu/mqbqhheY004+ZmZUoorh9O6xZo77n5cG3vw2PPgr33w+LF0Na2oTO47Q6VWAdhlgiGZYMMrvV4MP0omIVWAeky0VuQFko+zz7VAHHaWDS3VleL7hcCeGOlhSoeBP9U+MmAutaRDQauP6F6/ncw5+b7m5oUjAgvbe1Vb0Rr13bv8FXv6oe+keOTNiVBSNYIlE/OVk5iYyl7MJy2kPtSCnpceWQH5KcWrKK3nivmnP8GCOlpNnfTLolnWAsmJhcarykdmcpEfEX5SkRMdJ7IUlEpqgMPGgR0RxHtPhb8ISnf9CYZij1vnqq8qrUwg5jHrpkEXE44Npr1fcJpPeaJLKzoN8SkRJ/j5/crNxExpK9dBY9fT34e/x02dPJ6oOLKjYC0+PS8ka8RPuiLCtaBkw8LpJSRAxLxJdvg2gUPJ7EPcqz5ml3lkZj0hZsI9ATINZ3YpT1PlHoinThjXiHBtVXrRq44Ze+BF/8InzsYxM+l9PqxN/jV6mqxcXqoRkI0B3tJifTsEQyMnDmqxkT3UE3HpVhzDl5p5Im0qZFRExX1qrSVYl+TYSRLBG3K1O1NzXhi/hwZDpIt6Rrd5ZGY2L+4Zn+Xs3MYEhm1mAfIoEAACAASURBVI4dytpwDMoEys6GX/1KBdwniDlqvTvaPWCsSMISMeIExQ412VV7sJ22LDU2oqo3h4UFC6dVRE4tPRWYeHA9ISI9Par4ZJIl0pJjnqxZjVa3upTIRiLanaXRRHujCRO9Mzw1heQ0E8MUkTnOOaph+/aBrqxJZMjEVABut7JEsnKGvJ23h9ppylRjI4oiFlaUrJheESkzRGSs7iwpB9T9SoiIEfvB5UoUsTzsMAYSNjXhjXgTAw0BbYloNMnmvzkaVzMzaPKrtNJZubNUYLelpT8za5JJWYSxvR1/dKAlYr6du4NuDqcHAMjuDrG8aDl13roxBbavf/56Nt2dcg69cWOKyMqSlYl+jYl//hPKy6GuDkgSESP2Q34+aZY0CmwFHLIaAwkNd1ZyyRMtIpqTnuQ/Om2JzCya/c1YhEWNSE8VVJ9E8qzKLTOgCKPbjb/H3x8Tcbn6LZFgO3UW423c42F58XIkkr0de0c91y73Lt5ofmNS+t3sb8ZldZFnzcNpdbLy94/Ce987+o4HDijX1T/+gZQypYgAFNmKaI11qnuSLCLaEtFoFMnmv46JzCya/c2UOkpJs6QpV5bFMjSoPkkMZ4kkAuuGO8uWYcOeYac91M5+2aG2M0QExpahFegJEOgJEI6Fj7rfzf5mNZcKUGIvYc5bdfDKK9DXN/KOHUbfH3qISG8EiUwpIsV2Y1xMRUUiJjLAEtExEc3JzlgskdebXscT8sDrr8Mpp6gA7qJFcPXV0N19rLp60pH8gGT7djUOxGabknMNEJHsbLDbkW0qay/ZnQVQZC/CHXTTEG4lZMuAzk7muuZiTbcOLyK9vXDTTeD3J+pxtfuaYP16eGriA12T71GxvZji5i4lIG2jxEZMEXnxRcItDQBqxLopIknX2h5qV64vwxJxWV3anaXRmCRns6QSkbiMs/Hujdz00k3w73/DO+8oIVm8GH79a1i+HP71r2PZ5ZOGlkCLekDGYuren3HGlJ0rEViPGm6a4mJibao6bm66XT00jQdrsb0Yd9BNs7+ZcK4NPB7SLGksLFjIPs++1Cd4+WX4znfg4YcTpVS6DuyG114bMnPgeEgWkVmZhRR3GlV3GxtH3rGjQ5WJiceRjz0KMDCwnuTOMi0R2dREV6RLu7M0mmTagm3YM+zkZuWmDKx7w15CsZCqJtvVpcYp/OUv8NBD6sHgcMD550PNxCcE0qSm2d9MuaNcPWj9fjj33Ck7V25WLgIxYMBh3K0mYiqMZai2pAdrTUcNPX099DpzE1lOs3Nn0dDVkPoE5u9HczOBHhWQD9YfUG1Hjkyoz3EZ7xdaYGlXFhZzhtxhRKSmo0b9nns8cPrpUFFB5iPbgKQJqYRIuKmKbEV0hjuJl5ch2tvJ6JXanaXRJOMOuim2F5OfnU9nZKglYrq7GroblIjk5irfPKg/wr/9TaVLbt9+LLt9whPtjdIR6lAPyCeeUPd80+RkNKXCIizkZuUOLH1i1M/Kjxh1u5JcPA3dSixkfr56IPf0cPONb/Pp+4Z5mUgSEdOdFWmsV20NwwjPKHSEOuiN9yZEZIEnaY71YUTkrK1ncf0L1ytLpKgItmzB9uyLZMWSRMTlSvyOF9mLkEgCRUosSgNJJU8slqFjdiYRLSKa44K2YBsljhLys/NTWiIJEekyRGTwm9f8+ZCe3l+KXDMptAaUFZAQkdNPn1LXCagMrYSIVFaSfrgBJLhCxsPZDDbbihP7pBUVKxH5yU9Y+E4TX3khSmDvrqEH32e4uVpaEu6seJMhHhO0RFr8yt1W5igDYLZbFYCU6ekphSnSG6Et2MZB70ElIoWFsGULaaEwmw4NEhEDM6W5M18Nz6/o7p+QCqczMRXxVDAmERFC2IUQFuP7QiHEFiFExhj3TRNCvCWEeMRYrhZCvCaEqBVC/EUIkWm0ZxnLtcb6qqRjXGu07xNCnJfUvtloqxVCfHfsl6053nAH3ZTYS3BZXSljImb2VkughXiqgnMZGbBwYf+kSJpJwRz/MKcvB954Y0pdWSYDijCuWkV6d4AqH+SF46otyRIxySouVw/sG26g+cxVxNIgdv3/DD24YYnI5qbEBE6i1YjHNTaOnk2VAvMemZZIWXM3rXaIzipLaYmYL0RNvgYlfIWFsHEjvXYbF9ckiYghlsnX6s5NB6DCD65s15QXX4SxWyIvAFYhRAXwBPApYOsY970GSP7L/SnwcynlfMALfNZo/yzgNdp/bmyHEGIpcCmwDNgM3GEIUxpwO3A+sBT4pLGt5gSkLdCWcGelSvE1//DiMk6Px536bXjJEm2JTDLmA3L+ziMQjx8zEUkE1k9VI8BPbemfkCohIrZ+EbGVVqrxFk4njb+8gd+sgbz7H04M4gNUeZD6egDixrwcAFltRoZULDZ6NlUKBouIq6Gd/QUQLHaNKCJ+d4O6pwUFkJVF6/vX8OEasGHMr54kIsV2ZXXts6o4TsISmeK6WTB2ERFSyhBwCXCHlPJjqIf6yDsJUQlcCPzOWBbAJuABY5O7gYuN7x8yljHWn21s/yHgPillVEp5CKgF1hmfWillnZSyB7jP2FZzgtEX76M91D6iJZKcAtzr86R++1q6FGprVT0hzQB8Ed+E6jmZD8iSl3apONS6dZPdtSEMsERWrCCeZuHU1v4JqRKBdePtvNBWSHp5pVr3i19QXn0KPz0D4hYBP/4xAN9/5vvc8v9/VT20y8oQLS1geMeyO7r6Tz6BuIh5j0odpQDYDjezvwB8hfYRRUS2G+JVWAhA3XmnURSCwld3DbFEFhcu5pSSU7j2rf+P3sx0yv2D3FlTyJhFRAixAbgMeNRoG8usMrcA3wYMO5MCwCelNGeLbwQqjO8VQAOAsb7L2D7RPmif4dpTdf4LQojtQojt7cb8A5rjh85wJ3EZT8REOsOdSCkHbDPgATicCb9kiXpIHDgwxT0+/vj4/R/nQ/eN/g7WF+9LTPbEli1svvpnnNKRjvXZF1RAPT19ins6SESys/HOKeHUFsgOGGU/klJ8wbAALrsMtm2Dj3+cMkcZ7c50Xr9wFdx9NzQ0cO8799K4/Rm1/8aNWMIRco13jbzOICxYoBYmEBdp9jdTaCskKz0LurpId3dwoADaXVlq3pV4fMD2pogUmGMcDRE5uG4BvizIf+iJITGRdEs6v/3gb2kOtNDgiFPhPzZzicDYReQbwLXAg1LKd4UQc4ERk6aFEBcBbinljqPs41EjpfyNlHKtlHJtUVHR6DtoZhRmvKPYXowr20VvvJdgbGDtI3fIrWo3ARZ/YHhLBLRLaxAHPAd4su7JRCHFkbh7591U31qNL+yFp55iwZv17LijF3H48DFxZQE4s5JEBGiZX8LqFrD6w2oAYlYW0O/OKs8ph5wc2LwZhCDNkkZlbiUPnl0Ovb1E/34/h3yHKG00jnnWWWo/PxRkF1Dgi6qEAZiYiASaE0F18wXmcHEmzXkW5SJzD6yjZb4QFYaMBkNEAiLGg0sg+5HHlTgkWSIA6yrWcc3p19DoiFPRrdKhZ4w7S0r5vJRyC/ALY7lOSvn1UXY7A9gihKhHuZo2AbcCTiGE+bpSCZjOxyZgFoCxPg/wJLcP2me4ds0JhvlHVWJXlggMHXDoDrpZULAAR4adrMAwpa8XLlRZKjq4PoDfvfk7QKWixmX/W/Ge9j3ct/u+Adse8h4iGAvydu1LEA7z1/PnsO09RepBd9FFx6S/edY8uiJdib4ers6nPABp+w6kDDaXO8qHHGN23mxet/tg/nzC//w7AJUtQaisVL8nKBFZYq8iLyyRixYpIZqAO6sz3EmBrUAtGCLSOauQ+hwjSD/IpWVaIoNFJBQLcd9ysHT7lfUySEQArt90Pb4CG0s8Astzz88cERFCbBBC7AFqjOWVQog7RtpHSnmtlLJSSlmFCow/I6W8DGXBfNTY7ErgIeP7w8YyxvpnpPJZPAxcamRvVQMLgNeBN4AFRrZXpnGOh8dyPZrjC/OPqtherEo5MLSSb1ugjRJ7CQutlaT1xVP/4WRnw9y52hJJoqevh607t2IRFvpkX//85cCtr946ZDpi0wLYu/cFAHYWxNj6tfeqaWpnzeJY4LQ61ZgIYzBgbVUuAOKFFwa4eGwZNtZVrOOM2UNH0M/Om81h32G44AIcL72BNQZz3T1KLMqNLCo/rBbKgogUF8Ds2ROyRLqj3eRlGS81+/eDEETmVFBrM9xvg0Uk5KYgu2CIiARjQZ6tBmkspxIRR6aDlVd+h8KedOVe9PtnjDvrFuA8lGWAlHIn8L4JnvM7wH8KIWpRMY/fG+2/BwqM9v8Evmuc613gr8Ae4HHgK1LKPiNu8lXgX6jsr78a22pOMEx3lhkTyeyF0s99A956K7GNORhxQboxNmC4P5ylS7WIJPHwvodxB918YtknADX/hklbsI1gLEhPX0+izRdVInJkv6pueyC9K+Wb/lQyoH4WUDPLqlb4/QNEBOC1z73GZ079zJBjzM6dTWN3I32bzyM90sNZ9bCoA2IL5kKZEo5yPyyNqeP5XFYlkhMQka5IV6L6MPv3w+zZOJ2l7M1S41AGWzfuoJv5+fOpiGbSm24Bux1Qlkim1Y74qPEOPuhaE9d2zQ9I83TCgw/CNdfApZeOu8/jYcyDDaWUg+24MSdMSymfk1JeZHyvk1Kuk1LOl1J+TEoZNdojxvJ8Y31d0v43SCnnSSkXSSm3JbU/JqVcaKy7Yaz90RxfuINu0i3puKxqAp6z6qHk0efU4DYgHAvj7/FTYi9hvsWIeQ0nIkuWqD/k3t7U608yfvvmb5mVO4vLVlwG0B80p1+8k60+87u7XhUwrMsI9hdfPEYMFpHW9AiNBQNLnozG7LzZ9Mk+WtYsJJJp4TNvQV4UgtWV4HDQY7dS7of5EfUAb3dmKEtkAu6s7mg3uZnKWuLAAVi4kMrcSt7qbUBmZg6xRMx09lkxG105mYmBgoky8J/+tIr7jDRDpMMBF18Mt9yi6sdNIWMVkQYhxHsAKYTIEEJ8k4FjPzSaKcP8oxJC4Mp2cb6ZXGVk2iW7u6os6u0s5himiuzSpWq8QPL4gJOUI11HePLgk3z21M8m0k87Qh2J9WYsKnlcjvngFm61ndvOtIuIP+qndo4xN+wwb+eDmZ03G4AjUTcvzsvgEuNp5qtW98FfkEO5H2aHVPi22SGViLjdEB57aXgppXJnWfNU2Z39+2HBAjZWbaQr5idaWpgyJlJsL6YsmkGnvX+keUJE1q2DQOCophmeTMYqIl8CvoJKoW0CVhnLGs2U0xZU8Q6A/Ox8zq81VhhZLckiUhlXD5OOjFjqgy1Zov7XLi32tu9FIjln7jkDppM1ScxpHx4oIkW2IoqN5Lj2aRARM7nCtJq6o90crjYskDFaIuZUvrvadvH36ihpRsZ4+ywVAO/Kt1MWgOKuXnos0JQR6Y/5jFZ5N4lQLESf7FOZUu3tatzGwoV8YN4HSBNptDjTBxwvLuO0h9opthdTFBa0WfsGHMuWYbwcHYNU6rEy1uysDinlZVLKEillsZTycimlZ/Q9NZqjx3wzA7A3tLHI/M0bZImUOEoo61O1g5rTVFTy0f2PsqM5KcvcFBGdoZXIcMvPzqfQpoK17cF22LWLYNCXSKMebIlsrN5ISRACVguRDCjLKTum/Z7nmgfAfs9+APw9fpoXKAtirJaImQ6+rXYb24whIMEMcDuVW8zrslIRENg93bQ6wB3uUJYIjMulZY6sz8vK64+nVFXhtDp5z6z3qLhIkoj4Ij56472U2EtwBvpoyuoh1qdeiAaIyAxirNlZNwkhcg1X1tNCiHYhxOVT3TmNBvqLLwKIxx9XbeV5CRFJHkdS1JsJwBG68Ef9fOz+j/G5fyZlGOXkqDdKbYkkxMGV7cKWYcOWYSP3tbdh5UqCf/pD/3bJMZGIl1m5s5jXm0uLTaXYHmtLJCcrh4qcCmo8qs6VP+rHvXgWZGb2P+jHcAyX1cVTdU9R74LAvNnUFIKvR01e1pGXQVm3JL3VTVuuRb2omJbIOILrZrZbblYutKpilWbg/oIFF/BOphfZ2JgYcGi6EIvtxeR0R+iw9Re5PK5FBDhXStkNXATUA/OBb01VpzQnGPH4hAPZUspE+i4A27ZxuDCDmvnOlDGR/B5l5h+Kd/LAngcI94Z5u/Vt3mrpz+RiyRJtidAvDmbadJGtiI13Pw9A9EB/qXRTbHr6egjFQjitTqpjNtx2yErLSux/LFlcuJiaDtXH7mg3sqRYVeD9j/8Y8zFm580mFAvhyHQQ3fo7vnhR/0O/LS+N7F5g7146XVbl5qs0SqeMQ0S6o0qU8qx5/SJSqqymCxZcQEMuiJ6exAyGid9lawFZ3SE82dDYrSyV411ETAfchcD9UsqukTbWaAAe3PugKs1+ww1qANegUiVjwd/jJ9oXVe6sSASeeYbXTymgzSYHiIgj04Etw4Y1GKHXAodibu7eeTdz8uaQlZbFXW/d1X/QRYvUA2cC/TmR8Ea82DJsqhwHcF6jlaW71Ztwb1O/y8YUG/MB67Q6KQlZEkF1MYVlxofDFJG4jPdPjVtVNa5YgRlcX1a0jJwN72dHRX+wviXHuKamJgIFDvVwz8pSAjBRd5YpIiXqhWhF8QrCJUYMx3BpmSJSFrMipKTDpkSkL97H4a7D/YMWZxBjFZFHhBA1wBrgaSFEERCZum5pjndqO2u55K+XcMrtywj+8udw6NCEcuyTR6vz/PMQDrN7TSWt2b0qSyYYpC3YloiZ4PMRtKbxYuNLPH/4eT63+nN8eMmH+dM7f0qU9mbRIpXd0tIyWZd7XNIZ7hxgRXx5WzsduRlqQKb5wKPfEjEfsC6ri9yuyLRkZpksLlxMd7Sb2s5aJJKczJxxH2NOngquLytaRmZaJtnp2YlrbMzpH7kfLnL1F/gc51gR0xJJuLNcrkRZFiEE1cvfC0DvkXogyTUbUaUJTRF58ciLtAZa+dCimVdjdqyB9e8C7wHWSiljQBBdMVczAnvaVcxhS0sedrfhU9+5c9zHOdKl/mAXtvTAT34CWVk0rp5PY5ZRHc/tHhB4p6uLkD2Tt1vfBuBTp3yKz6z6DN6Il4dqjOIIZmrkvmHm2T5J8Ea8as4JgOefZ9WeTm7f5IDqajKM8udFtqKEiJj/OzNzyfD46HCIaRURgDea1KDH3KzccR/DtESWFy8HBpaYP5Ldn90XKy7qz1ob56h103pLuLMMV5bJ6tO2AHBop3IjuoNuBAJXUGVlBXKzaOxu5M+7/4w9w86FCy4c72VOOeOZ2bAc+IgQ4gpUWZJjU21Nc1yyr0M9oH/T9T7CVuVikBMQkX2tu/nz/XD6+Z9TU9v+9Kc48oqpzzRqQrS3DxERc4zIxqqNzHHOYVP1Jmbnzeautw2XlikiJ/l8696wN5Euyx134HfauG1VBMrKyO7wkZeVR6mjVLmzfv1rojWqIERhWCDicVav3MwVK6+Ylr4vKlA/wzealYjkZI3fEkm4s4rVrBbJ1YHrs/unCxBlZbiDblU52hSRMbpCh1gig0Tkves+itsOgRdVBWF30E2hrZC0TiXYacUl1HfV88CeB9iyaAv2TPu4r3OqGWt21nWo4ou/ADYCNwFbprBfmuOcmo4a5mQWk/WPf3Jw06nUuqD37fEXdI78+1kufRfkl78Mhw/DNdfgynZxONMY8NXePjDw3tVFPFc9UK5cqUqxpVnSuGrlVTx58EklbhUVqpTEcWCJHPYdZsWdKxIW2WTijXj73Vm1tbgXV9JJmFhxIbmdQUqMqsk9ne3wpS9RsvV+AAoDytVz/nuu5KKFx6bo4mAqciuwZ9gTIjIRS+SihRdx0zk3sbFqIzBw2l23CBKyq0y/9MrZ9MZ71brycgiFVImVMWBaNjmZOSlFJMeayzuLXZS/eQCk7H8hMgLt1tJKHjvwGJ6wh0uXT235kokyVkvko8DZQKuU8tPASlSVXY0mJTWeGq5qKAC/n8YtZ7GzFNiVYk7rUQjUq7EA4itfUTO8ocY1tBtJKnF3W2JwFgBdXeQWz+Ls6rP5yNKPJI7zpbVfIs+ax6cf+jS9xFWg/zgQkR0tO9jt3s1LR16a9GN3hjv73VlHjtBToR5wgYIcMnrjzCMfl9VFdpOKB9j21wOQ1228pRcXDz7kMcMiLCwqXMSbLW8CTCgmYs+0860zvkVGmhobkmyJ+KN+ugscAGTPngsYAzGNoPhYZzjsjnbjyHSQZklLKSIA3RvWUNIZJbJ/b398zxARR3kVkd4IeVl5nDfvvCH7zgTGKiJhKWUc6BVC5AJuBpZh12gGsK9jHx/eEYSKCiJnns6uEkg/WA/B4Kj7JpPIEirrH9DmsrpoN6z6cPMR4jI+ILCeX1rFU1c8hSPTkdinLKeM2y+4nVcaX+Gml27qz9Ca4ZgB3drOWvjlL+EHP5i0Y3vDhiUSCkFHB9IYB+F1qYKGC3tycWW7yGtVrpW8gyqDKMdnWIHTKCKg4iJmssRE3FmDMWMicRknGAsSKsiFtDRyKpWIuIPufhFJSjwYia5Il8rMCgTU7765fxKu8z8MwOGH7sYddKsxUR0dYLVSXFgFwCVLLklk0c00xioi24UQTuC3wA7gTeCVKeuVZkbwl91/YcPvNyRGzI6VjlAHMa+H5Tsa4ZOfJN9RxM4SEFLCu2MvtOwNe8nxBOjNTB8wEjk/O59AJsSzMgk3HwYY4M4arvjiJ5d/kk8s+wTXPXcdLeW5aj7tyMxOMjRF5EDnAfjb3+COOyYlNbmnr4dgLKhiIkZ6aXqVGgnekavezOdGrLisLgralOsmx+OnKJpOpseYvGm6RaSgv7DgRNxZg8nLUu6sYI960QlUFkNVFcU5ynpwB939lsRYLZGebtU3c/sUlsjKsy7FbYPo0/9S7ixbMXg8UFhIZa4amzJTXVkw9uysq6WUPinlr4APAFcabi3NCUqzv5kvPvJFXm18dcD85WNhX8c+qr2oeT02bMBldbHLfAEbR3B9b8deyvzQU1SQqGQKaoQ1AqL5ecTaVJpusb1YPVy7u4edhEcIwR0X3kGhrZA/Rl5T29fWptx2pjDAEvF41Ke5+aiPO2CgoZFtlD1XTcbUYFcDQ2cFVeXkMk9/Kfh1XQ5EeztYLGOuUzVVmBlaMDF31mBMd5a/R4nmm1/7KGzblrByB1giYxSRRBn4QQMNk3HZ8nl7cR4l22vojnT3u7MKC/n4so9z2+bbOLv67KO+vqlirIH1Dwsh8gCklPXAESHExVPZMc30IaXk6kevTgQFPeHxlUmr6aihsttYqKzEle2i3gk9duu44iJ72/dS7gdLRcWAdjOjKOS0I939o9UJBNTo+BEm4cnPzmdt+Vp2Og0LZIa7tIaICEwotjSY5JInpojkLlCprm+hHnhlAbW+ykciWeHUzkxV+LKwENLSjrofR0OyiEyGJeK0Ounp60kUdswoKYcFCyi2F5Mm0tTA2cJCJaBjdWdFuwYONEwhIgC+9aso6YxS7WWAiBTYCvja6V9TMZUZyljdWdclj1KXUvqA66amS5rp5oE9D/DQvoe4YMEFwNCpaEdjn2cf1QFj5HBlJS6rC2kB99yScVkie9r3UB4UZFVWDWgvdZSSnZ7Nzr4muhqVJVHiKFGuLBh1JreczBz2uIwyLMeJiLSH2pGdxs9hAqnSgxliiQhB7twlpFvSeTN4gEAGFHb34rIqEQmtXUUkM43l7UKJyDS7sgAWFCxAIEgTaVjTrUd9PHP2QbPMiBlnSbekMztvNod8h5RwFhWNK7A+oG7WMCLi3KziIhvrB4rI8cBYRSTVdjOnFrFm0uiL9/H1x7/OmrI13Mr5vPpb8HnG5z6p6ahhRY9L/cGVlGDLsJGZlklDVb56ix6jT39vx14q/AIxyBJxWp08fcXTUFSE1esnKy1LWSc+w1c/iog4Mh24RUil+s7wsSLtoXay0rLI7gFhxm8mQ0QMSyQ/O1+JSFkZIjOTQlsh77a/S0sO5HlDCUskUFlEXZmVha2xGSMi1nQr1a5qcrNyJ6X0ijlPSUO3SuZITsyodlUrEQHl0hpvYL21VVkwwwjDqo2X0maHCw7Amjv/oea7MWt1zXDGE1j/mRBinvH5GSrArjnBaOxupDXQyudXf56SF9/m9CawP/3CuI5R01HDwnC2yqlPS1OTSVldHJxlV9bCGEf81je9S04kPiAzy2TDrA1sOv1S5kStPPGpJ7AIy5gtEUemQ/m9j4MMLXfQzdrytRQkz4M0Ce4s07pMuLOM6reFtkKOdB2hxQEOj5/CWCb5EfCVOtlXksac5tCMERFQLq3JcGVBv4gkLJGkOEu1s5pDXkNESksnZokUFw/rAix2lPDmohw+shcqb9uqprT9zncmfjHHkLGKyNeAHuAvwH2oull6UqoTkIPeg4ByFdgOqTeyisdfHvP+PX091HnrmOW3DHiTcmW72FuuBm+N5SEYioWINRpiUz5MaY2iIizhCO8rWquWTREZJrBu4sh0EOwJImd4IcbeeC+ekIcNlRsoMAbos3Sp6vNRZpUNcWcZIlJkU5NTteRApruTog514vZiB7uK+sj3RtSgzxkiIj943w+4+dybJ+VYQ0Qka6CItAXbCMVCyhIZg4j0xnsJxoLDljwZTM2H38dj8yH0zL/gnntOLHeWlDIopfyulHKtlPI0KeX3pJSJhH8hxC+mrouaY0ltp4oxzHPNI22/+j7/5b1jHt9xsPMgfbKPws7IABHJz85nZ5ExS9s774x6nH0d+ygNGAspLBFA+aYhUc13PJaIRNIzv1rt4x5f9tmxwhPyIJFUu6pZgJEJtXEj9PUd9XwoicC61TlQRIwZDjty07C0tuFsVfe0tSCLN/ONQYax2IwRkdMrTx8wqPRoyLOq35vh3FkA9b56JQatraO+fPijKssrkeI7ioh88OpbqL/3dmwblLd+MQAAIABJREFUj6+KUuOpnTUSZ0zScTTTTG1nLZlpmVRmFUF9Pa/MyyQz2guPPTam/fd59oEEh9vXP4kP6o23Bb96wI/hLW5vh8rMAka0RIBxi4jppgjNNfo3BS4td9CN48eOYUeau4Nu7tl1T6JK8XDbgAq0LrEYD+1Nm9T/RlzEHXQnik2OB2/YS05mDukeL0Sj/e6sbPX2G8jPQfj95NSqcTgHHD28XZA0J8wMEZHJZDR3FqBcWiUl6p51dw89SBJDysCPIiLz8+dz9WlXT7j/08VkiYjmBOGg9yBzXXNJO1QP8TiPvLcEb14W/PWvY9q/pqMGZwTSwpEh7ixv2KvGFnSOnu21p30PFQEjWDpWS2QcgXUAf4kxgHESxl0M5pD3EMFYkF1tqV13N798M5968FOU/6ycc/54zsApfA2SRWS+NPq6bh3YbLBzJ7G+GOf/6Xze+4f3KjfLOOiMdA5I7x1siUSK1PnSX3uDUAbspo0jedBjM0ZNnwQiklzs0LREDvkOjXnUeqL44jB1s04UtIhoBlDbWavmsDbezjurSnhmjQsefVSNwxiFfZ59nNpr+HKT3VnWfBXMdbnA6x1m7372duxlacyppjwdblCb+SAz3VFdXWpSouzsEY+dEBGzisQYi+mNB/MtNFFCfBA7WnawqGAR3zvze5x913O8+qvvD9nGFJEiWxFzelWfA7lWWL4cdu3ix//+MW+2vEmgJ8Bj+x+Fm2+G/fvH1L9EyZPBImLERPpKjXv7yis05qdzqKseBPjnGT/TE1BEstOzSbekE4qFEt9NSuwlZKdnK0tkjKPWzTLwBVGLcgFqERmRYz+1mWbSkVJysPMg8/PnJx5GoapKHlyRoSaAeuSRUY/R4m9R6b0wxBLpinYhXa4xWSIHOw8yN2JTVshw6Zup3FlO5/DbG5gi0m2KyChuiYlgPkDMgWvJSCl5q/Utzpx9Jtev+DrXPt/HZ296cohbLdkSKY1lEcyAg6EmWLmS3rfe5EcvXM8nl3+SEnsJrz32W/jmN9WcK2PAG/H2p/fCEEtElBkuxPZ2Wgus1HnrAAgvVKVRTkQREUIkrJHBtbiEEFQ5qwZaIqOJiPEiUdBllA06mUVECDFvlE1uTbGPVQjxuhBipxDiXSHE/2u0bxVCHBJCvG18VhntQghxmxCiVgixSwixOulYVwohDhifK5Pa1wgh3jH2uU1MxzydJxBtwTaCsaASkX37oLQUW0EpT5WFlTXw1FOjHqM91M68kPF0ThYRo+R4T55jTJZIQ3eDiokMFw8ByMlRlkqyiIziyoJ+EenKMAL9x9gSaehuoDPcyamlp6rZGgFLXMInPjEg68oddJMm0nBluygMCzzZylKMLl1Euq+LU/oKuf2C2/nIko9Q8NizaqdHHlGB91FIVPA9ckS5xwxrr9CmrMiMytmJbT0ljoSgxdatUfd9OBfjcY4pIslBdZPEWJFxurOcPuNnmqL44onAWC2Ru4QQB4UQ9wkhviKEWJG8Ukq5NcU+UWCTlHIlsArYLMT/be/N4yOr6rz/96lKqlKVpVJZOkln631vbJpuaMEGBZRFEB11HlBAHWwcFdHR8QFHZ3448+KZZ9QBdQZ5RGRzVFQGgREBWRpolgaavZve16zdSaVSlaSWpFLn98c5t6qSVKoq6a6kO33er1denbpL1b110/dzv7tYp9d9W0q5Sv9YUcGLgIX651rgdgAhRAWqOv4M4HTg/xNCWN34bgc2pOx3YY7nY0jD3h6V3jvfO19ZIosXU+GqoDvqR9bXJ9pTZ6I71E1Tn00VVqU8eVmtSiKlRVlFJDQUoifcQ1VgMPPNSghljaTGRHIQEespsz8WUnNF8mmJhLpUFk8sGZR+s+NNAFbXrYaNGwk77Xzni3NVsPxb30ps1xXqorq4GpuwUdY/hM+tYkX/4FPxqTtnfxmvy8tfL/s0H383xpDLqa7Ry9l7o45wZzU1Jaw3y51VWjcHClUjxkBtsvnl0BeuVqOO3e6j+HaOXxKWSJpeXPPK57HPvw9ZUaHqPXJ0Z5X6dWbjyWyJSCnPAZaihlKVA48KITL6JKTCcqIX6p9MOXGXAffp/TYD5UKIOuAC4EkpZY+U0g88iRKkOqBMSrlZSimB+wDTz+sosNJ7E5bIokVUuisZlsPEvJ6c3FDdoW7qAnH1H0bfhIDE3IpQSZF6nwzpkS0BlWJZ5hvIbInASBGZoCXSP9gPZWV5sUSsp9DuUDd8//uwenXinN/sfBOB4JSaU2DjRnYtncVDSyRcf73q0tvWBjBiYmOhP0BfSSE3b7qZ2+OvEvaWcuofNgHwgf4Klvjg/k8sUN/5ww9nPb7EQKqU9F5Q1/6C+Rdw7rzzEje9gfrqxPpyd0VirstMxGp9kq61/FzvXILRIP6hYE6tT6y/AbdPP6SczCIihPgA8C3gu8BHgT+RQ7GhEMIuhHgLNX/kSSnlK3rVzdpldasQwvJM1wMtKbu36mWZlremWZ7uOK4VQmwRQmzp6kof6DQoEbELO83xUvVEu3gxlS51w4iWFWcVkdBQiNBQiOqe6JiWDZY7q6+4QAUZQ+NnE7UEW3ANgqM/lN1tMmvWyMD6BESkL9qnXDP5dGcNdKnWKu++m4g/vNn5JourFlPc0wfbt7N/1RyVdPDhD6uddbbYiLG/PT0MlZcRHY7y/Yt/gOt7N8HTT8Ozz2J/8I/EBXyvaQ+xsz8AjzyS8djCQ2EisUgyJpIiIq5CF49f+Tgra1YmvvvB+uQ1sJ7UZyoZ3Vmpab5WrUgGAtEABbYCCo/4wOnM6W/zRCRXd9azqKf8O4AP6tbwv822k5RyWEq5CmgAThdCrAC+AywB1gIVQN5r+6WUd+hCyTXV1dXZdzhJ2evfS5OnCcfeA2rB4sVUupWIhEqLsopId0i5u7y+/rEioi2RXrf+k8vwXi2BFuosGzbflkhpaX7cWVpEukPdSKt+5YUXAOXOSo2HHF67lEAkQLxSZ6Fpt+EIEfH5mDd/DT+98Kd8+6xvw5e/rG7y//iP8MAD9J2xikNFUd46o1m5IjP0BOt/cSMX7oZqUayeplNEZARaRIabVD2N1QNtJpPJnTUmzTcHS6TMWYY4oueQzNCQba4iUgX8M/B+4HEhxFNCiH/J9UN019+NwIVSyg7tsooCd6PiHABtjJyW2KCXZVrekGa5YZLs6dmTdGWBcmdpS6S/pDCrG8oSkZIjgTEiYsVE/Faz1QxxkZZgC3WWcZCLJXLgACxdqp7gs7Q8AXDandiFPa/uLMsfPhQfYtivW7hv2oQv5KMl2KJEZONGKC0lvHIpEklfqTbKU0XEPUu1t+/pYe6CNXztjK+pbVwu+N73lDBt3UrpZ/4Gm7CxcaW++Y2yRgKRALe9ehvD8WFKrv97Hvs1fOaTN6mV44lIYyMUF+OYra7lTLdCIMWdlU5ERhcc5mCJeJweNbOmPq2TZEaQa0ykF9gH7Ac6gPnA2Zn2EUJU62mICCFcqGFWO3QsA51J9XFgq97lEeBqnaW1DghIKTuAJ4CPCCG8OqD+EeAJvS4ohFin3+tqILsz2DAue/0p6b0FBTB3bsIS6XXbVZVuBjdU10AXpREo7A+N687qLoqrBVkskSVDuqleNktkwwb40pdU7cS558Jll2U5S5WuWeIoSVoieXRnAQwHdBHkCy/wZqcKqp9ap0Xk7LMpL1UZUf4S3Zyvu5vwUJi+wT5liQQCSkhGxyKuuSYhALZPfpIKVwX7SoZU/GVUXOTB7Q9y3WPXsfHARuydh9lcD9Em/d2uWpX+JG68ER57DK9+ADgZRCSTO8tT5MFb5FWWiNWEMcNDVTAapHHIDZs3w3nH71CpoyWndu5CiH3ADuAFVEbUF6SUg5n3og64VwhhR4nV76WUfxJCPCOEqEbVlrwF/K3e/s/AxcAeIAR8AUBK2aOtntf0dv8spbTuQF8B7gFcwGP6xzAJesI99IR7dKHhyzBvHhQWJiyRHqt+r6dHZTSloTvUTb11Px4lIs4CJ64CF4cd+s8miyVy/mAZEMxuiSxdCrffnuXsxlLqLM2vOyui/OGxeCzZjmXbNrbvUC6t1bJWifWGDQmB9RXGmGO3g8+XSA2uLq5OCu7ookunE+66C15/HWbPptJVqQaIXXwx3HyzEnydRWWl6P55x584t6eXp8+EgvvvYY1n6bjXk/p6qK/Hu0sd/8kkIuPNbE+m+X4YBgeTtUlpCEQCfGRXTD0AfOxjeTvm6SbXmSALpJTxibyxlPId4NQ0y88dZ3vJOMF6KeVdwF1plm8BVkzkuAzpsdJ7lSVyr2qTjvpPJRAjLYjGxrTv0R3qptF6AE8zC6HCVUFHYTT5PuPQEmxhXsSlMo3ylAmUaAdfVpU3S2Ru+Vx29+zGFuyDU0+FN98kumkjjbMaqXhZZ7Z/6ENUuJR11xPxq/Pt7h5RaJiYaJjuuzjvvMRTbqVbi8j8+eoJubNTPQyQrFfZ9M4j2IbjdLm1dTiegKRgiZz170wmU0wElEtr65GtyUyrzs5xRSQYDfLBt4PKml69Ou02M4FcYyILhBBPCyG2AgghThFCjO3TYDhhsVrALyifB7t3wyI1b9tus1NeVJ60IDLc/LtCXTT16eBhGhHxury0FWh3WCZLJNBCQ78tc7X6UZJ3d1YkwPyK+Yg4FA6E4fzzobCQ8i1bOW3W++CHP1SuqFWrEkkH/ohftf+eiIikUOmqxBfyjbzBaax4VV+rmolxpDiZ7JANa7uTwRKxOvmmc2eBEpEDvQeQVsV+huB6eKCX1e90wSWXqLqpGUquZ/YLVFbVECSsjMvzdVCGqceqEZknKlTVdIq1UemupN2hq24ziEh3qJuFIe33ShPL8BZ5aZdBVag1jogEIgEVCwimH0Z1rEiISFmZivUMZvPOToxgNMh873xKrbetqWF4zWks29HDF1+Lq5kq//7vYLcnnvD94aMTkSp3lbJE0lRUd4W6mFU8i1m67q27OBlEzoZ1fCeDiGRzZzV5mogOR/GX6Sy1DMH1Fdu6cUViM9qVBbmLiFtK+eqoZbG0WxpOSNr72qlyV+Hy66fylN5Ila5KWu3agshiicwfcKh9nc4x6ytcFfijvRk7+VqzHDx9g3ntzzTCEoFjao1EY1Giw1HqSuqoienvwePBt3oJa9rh/LufU0kAn1RzMNJZIlbPrVnFs8aPiYxijCWS8pTcNdDFqtpVrC5QDwdhbyl2W/ope6Oxju9kcGc1e5qxC3siE2s0DWXKwm5169vfOJaIlJIPbR1gsKgw2b5/hpKriHTr/lkSQAjxKVSWlmGGEIwG1ZOpVbiXKiLuSg7YdfA5g4iEfJ2c/84ArFmTdn2iHXyGTr5WG253IJTXyW6ljtK8iUhijkSRh2abvvF6POxZWosjDo6BCPz0pwlXnavAhdPuHGOJuApcFBcWK0tECPW9ZaDSXUk4FiZUXqy2H+XOqnZXc45rCQCDlblbFUUFRdz78Xv54uovTuRrOCFpLm/m8N8fZn3z+rTrLRE5aO9TGYzjWCKRoTCX7IhzYO3CrF2lT3RyFZGvAj8Hlggh2oBvkMyqMswAErOg04mIq5K2mF81O8wgIpf8aRfl/UNw001p13uLvKoyO5MlolueFPYG89peY4Q7C45phpZVI+JxemiS2mXk8fDKnAIidhj6ypdg+fLE9kIIvC793VRWgs/HkYHDzCqehRBCiUh5+bjzuS2sTDrfYEAVYY5yZ1W7q1lt17Gq6okJ9NXvu5rm8uYJ7XOiYqW1p8MSkZb+NuVubUtfmhZ6fTNNQWj94MwNqFvkKiJtqMLAm1Ez1p8EPpdxD8MJRTYR8UX0DW48Eenq4nNP+Xj9rHmwdm3aTSpcFQwMDRAvLx/XEmkJtlA8JLBFonkXkb7BvrxbIvVSB2jLyng31sb6G6px3PrTMft4i7xJd1YsRn93+4iWJ7l8F9bNzxf2jWjLEYlF6B/sp8pdReNgEX6XoLQ4s2vMkJ5ZxbMosBUoi7mxEVpa0m43+PYbAAysXpl2/UwiVxF5GLgUFVhvB/qB3IZuG6YeKXNqlpjKCBERYsRNq9JdSf9gP/GK8WeByJtvpigm2bRh/PnQyXbwxcS6u/j5lp+P2aYl2MIyoW+eeXRnlThKGBgcIF6iU1zzZInUDmtXhsfDLt8u3AuXprUovK4UEQGGDnckZnvg82WNh0CKJRIaKSJWZlZ1cTW2rm7sNXV8+8xvH9U5nqzYbXbqS+uzikh8v5q/Yps7byoPb1rIVUQapJSXSyl/IKX8d+snr0dmmDBffOSL3Pz8zchHH1U3kQMHct53hIhUVip/r8a6OcU8pelFpKMDbr+du1eBWLxk3M+wArThkiIi3R387aN/m8hCsmgJtCTniefZEpFIIm6dZZMnS6Q6JbC+u2c3CysWpt3HW+RNxkSASGcrTWW6HYnPl9N3Yc0CGW2JWEH6arfqM1bWMI8LFlww6fM72Wkoa0iKSGtr2qr1oX276SyG0vKZN7xrNLmKyEujZ4gYji/iMs59b9/H9zZ+j0fv/a7qlPviiznvP0JERg3PScwCGa+T7yuvIAYH+cXq5I0sHZYl0l4YwT0wiIhDb6R3xDYtwRYWxHUAOY8iYhWTDTj1f4FjKSIplkjlkGqHf9gW5sjAERZVLkq7T4VLjw/WIlLoDzKnfI5amaOIJNxZqZaIlIlCwyp3lbq+M3Aq4VQyQkQikTFzdgYGB2h790U6Kx2cVnfaNB3l1JGriHwAeF0IsVO3cH9XCPFOPg/MMDGODBxhKD7EqtpVxN5Vl0a+8kqWvZIkROTw4TE3mWQnX2d6EdGjdHdUZRYRS4z+p+tFbBI80eTMBVBpkS2BFuboeeL5dmcB9Dt1MeMk3Fm3vXobD+14aMxy65w8RR4qBm3EBLzYoyrUM1oiKe6sqhBJEckxJmJ9v92hbiUig4PQ2zvCnWVE5OixRERaBbWjXFr/8PQ/UN0Vpm7lmRQ7sncEONHJVUSsqYMfQcVGLtH/Go4TrNTYm865ibP71c0k9NKzOe1r1TUkLJHRIqLdWX3FhcnCt1R27iRS5aWvKDmjOx2WO2v7sHKzeMPJp3ZQ/bvCsTD1QzqOkGd3FkDAodu5TMIS+dHLP+K2124bs9xyZ5U6SimLqlnuL7WqaYPjWSJel5dgNMiwV6XeJkRkaEgJXA4xEYfdQamjdGTB4eHDSXeWs0JdPzMO4ahoKGsgHAsTnKUz71JE5NkDz/Ifm3/KvKCNmuWnj/MOM4ucemdJKQ/m+0AMR4clIk2FVXjb/UTtUPTuDvU06sg8A6JvUN1AxxURq5NvsQ3CYfWTmvu+axf+pmrAn5M7y2rmWBEe2e3WKjSsieg/yxxunJMlYYnEI+pcJiEivZFeDvQeGLM8EAngLnRTaC+kJBwnUAQvtbyEQDC/Yn7a97K+m15HnHK7japQXImIZfnlKKiJ/lkprU+64l3YhA1vKK7898YSOSoSBYcegQdGiMi3n/w2ZxQ0URA7BM0nR0r0zG3ocpJh1Vc0d4YRUvLwYrAPDqmJelmw3C/lNreaUz6OJdKjZ4Ec2v82MjWYuHMnh+vVU1lGEXF51Y10nvITeyMjLRFLCCtDUg2XShmve6w52sFUcRknEAlwsPcg8VG9SRNzJAB3aIigE17veJ1GTyNFBUXp3i45byXay4DHRU3YPqGWJxbp+md1h7qpdFVi69K+eyMiR4UlIgcKB9QDmhaRWDzG251vc2WZLlScM2eajnBqMSIyQ2gNtuKwOyjfq4qf/us0/TT/2msZ9lJYIlI1oIVh1E3GXejGaXfyZO/rAFz8n+/njDvP4IH3HmDY1w1dXbTUuHAXunEXusf9nAJbAXdcegffuPifgbGWiC+kbpjFfZG8z/G2eiNNdjBVMBpEIhmKD9HRN7J5QyAaSDTyc4aiBJwwODw4risLkq6+nnAP/mI7jTGXKjS0grZHY4mEupSb0ZoAadxZR0XCEulvV41GtYjs9+9nKD7E8pCOgxgRMZxItPa1Ul9aj+297VBYyO7VzQTKnPDq6JZnY7FEpKJP9wMaJSJCCJrLm3l3SAnUjUs34I/4+fQfPs2/3HkVAHur7RmtEIsvrv4ic+aqCQHe8MjAuj+iChCLevvzLiJjLJEJiog/nCyWHO3SCkSSlkhB3wDBIhW8Hy+oDoxowtjlilMb0VbYe++pfxeOv28qVe4qJcZer7LkOjvpGlDV6ukKSQ0Tp7akFpuwjakV2d69HYB5AX1bNe4sw1QTiUW49eVbGRyeeEfZ1mCrekLatg0WL2Z2RTNb57gnJCLlQf25aW4yz1z9DL/Z8DgAVzZezI6v7mDD6g0cePUvALxXMaxuVLmge0DVDBaOcGf5w34EggJ/IK+ZWZAUkb5on7JEJujOsgQP0ohIiiUiAgEGi1WtSC6WiD/ip805SJU1QHLLFiWoOd6QEoOphEik+XaHupXAW5aIEZGjosBWQF1JHa19rSMskR3daq59jS+i/n5zmNUyEzAichzx4PYH+eZfvskz+5+Z8L6twVYaPY1KRJYvp7Gskc11w+pJNstTtiUipYGwWpDmJlNfVk9lg34a7unBbrPz5TVfZkFXnLjdxraSUE6WCABFReByUTvoGOHO8kf8eIo8iBzrIo6GnCyRtrYxs8otUutbMlkiBAIMlSgXXyZLxIqJtARa6HAMUtanBX3LFjjttJznqlS6KumN9KqJilpErL5ZiW4EeUxYOFkYUSvS1gbDw2zv3k5tSS3Olo6TxpUFRkSOKzYd3ATAwd6JJcPFZZzWYCvzCmbB/v2wfDlNniaeqe5T2Tivv55xf0tEiv368Xe8J1Xrxq4zhlbVruK0/lLaqpx0RH25iwhARQXVgwVKRG64AX73O3ojvWqewxSIiNPuxC7smUXkP/9TzWy3nuBTyOTOSnRElhKCQYbLVPwloyWi3Vlvdr5JtxtcwZAab7tt27hdkdNhZdL1hHugthbZ2Ykv5EvWiFRVZW3kaMhOo6cxKSKxGBw+zPau7SytWqo6RRgRMUwHmw4pETkUODSh/bpD3QwOD7LCp28OK1bQ5GnilTodKM/i0rJExNUTVHNArKaEoykpUe1QtIgIIVgddPN2WZjWYGvu7iwAr5fKiA3Pvnb4wQ/gnnvwR/xUF3jUDT3P7iwhxMhOvuncWYf0dXj++TGrLHdWXUkdBwIHRqxLuLMiERgaQnjKsAt7sngwDdYM+jc63qDbDba4hOeeUzeoiYhIav+smhpkZwcSmXRnGVfWMaGhtIGWQEui4FAeOsSO7h0sqVwMBw+eNPEQMCJy3OAL+djWtQ2AQ8GJiYiVGrugQ88v15aIrxjCjXXKJZKBYDSIXdgp8PnVTWY814nlCrFqF+JxqtsD7KyCofjQxCwRrxdvBM5/UjWqY/du/GE/zcO6Wj3PlgioDK2s7ixQN3OLWAzeeivhzjq17tQRlsjQ8BChoZCquQkoV9285lP5/KrPU2jPnLLsdXnZ5dtFt5Xg9riKQU3GErEytERXN7Y4SXeWycw6JjSUNTAwNEB/ja7v2f0ugWiA1bYG9fBgLBFDLmxp38ILh144Ju/1Yovqc1VcWDxhS8QSkdmHepUlMX8+TR7VvK+3vjL5RD0OVssTkUtLjFQRaW3FFo4wNH8OkLlGJN37zOod4qKXtJ/+wAH6BnpoGMx/tbrFiHbw4bASiFRa1fc6QkR+8hNYvZpYyyHsws6K6hUjakUSLU+cnoR1c8byj3Dnx+7MejzeIi8SSbBUi80TT6jrkWZe/XgkmjDqWhERj1MVIpniayyRY0JqwSFA9843ATgloufTGBEx5MJ3nv4OV/3xKnWz+NCHkumYk2DTwU047A4uWnjRhEXEKjSs2N8OS5aA3U5jmRqD6vMUqi67GRjRfHEiIqJ7Zi16/yVA5pYnY/B6md3Rjycch89/HoaHKW3rZnZUV9fn2Z0FaQZTpVojUipLxOFQBZvWOd93H0hJQUsr5UXlzPXOHVErktrB17JE8OQ2y9wKrjtq9Hz6nTuVFZJjUB1S3FkptSK1/SnNF40lckwYMeHQ5WJg304A5gd1fZZxZxly4cjAEQ70HqBj81Pw7LPwwAOTfq9NhzaxdvZaFlUsojXYynB8OOd9W4OtFNgKcOzcm5iYV+wopsJVQXuJTHRzHY9Ji8hO9R/nvAu/wrWrr+Wc5nNyPmYrQ2hntYBrrgGgur032fJkiiyRcUfk+v3KLXHJJeq727QJtm6Fd1Rzy8KOI3hd3kScw3JppXbwTYiIJVJZsNJ8i2ubkgsn4MqCpDsr0YQRJSLVhXoQmLFEjgkJS6SvDRobkYcOUuIooeKwjq0ZETHkglVhvWPrs2rB5s2Tep+BwQFe73id9U3rafI0EYvH6OxPP7s5Ha19rdSXzEZ0do5wfTR5mjjoiqr+WeNMEgQtIo7StG3gx1BRkWzFsWsXlJTgmbuEn1/684xjRcega0V+dpoktkAN7mk+MkRVRD91T6UlYomIdj/9buvv8O/ZqpZ94hMqJfm55+DXv05YBUVHfJQXlY8VkaOwRKwMrfKGBcmFp02slXhxYTEOu2NE65PafpJ1J0ZEjgl1pXUIBPv8+6CxkaLOLpZULUEcPKj+j+T44DATyJuICCGKhBCvCiHeFkJsE0J8Xy+fK4R4RQixRwjxOyGEQy936td79Po5Ke/1Hb18pxDigpTlF+ple4QQN+brXNIhpUy02G7ZpVuLbN6c8Yl/PF5pe4VYPMY5detYu7kF59DEMrRag60sKJoN0ZEjZZs8Tex29KsXneOLUjAapCbuVk/euVoi8bhqqbJo0YTcLQlOOw1/QxW/OgWCpQ7injIW9kCl1XplKgLrjtIx7qy9PXu5/L8v56nn71HL5s2Ddetg40b4zW/gwgvB6aT0SABvkZdmj3riTGuJWBlfExSRurqFyaaZE7REhBCH9sZBAAAgAElEQVTJgkP9QDAn4sTZowXNuLOOCQ67gw/N/RC3vXYb/TUVeLv6T8r0XsivJRIFzpVSvg9YBVwohFgH/Btwq5RyAeAHrtHbXwP49fJb9XYIIZYBlwPLgQuBnwkh7EIIO3Abqk39MuAKve2UEBoKER1W2VC9B1SlKn5/Ik4wETYd3IQ9Dud+7y5Wf+1mHv0NtLfvzHn/1mArS4S+OaSKSFkT79m16ylDXCQQDdAQ0TetXESkvx++9CV4+WW48sqcj3MEF13EQ4/8AL8bAtEg0blNLPSBZyAGbrd6+s8z6dxZm1uVNTnUckAtq6+Hc86Bt95SCQqf/Sw0NODp7sfr8uIqdFFTXJMQkdRZIhN1Z1kxkTneucoSq6uD2bMnfF6J/lklJYSLCrjyzbhKCABjiRxDfnHpLxiOD/OHvpeZFYzzz//yosqom5++U/NMJW8iIhX6MZhC/SOBcwEreHAv8HH9+2X6NXr9eUIIoZffL6WMSin3A3uA0/XPHinlPinlIHC/3nZK8IWVS2flrJU4u3uR1tP4JFxazx14lt9urMDx0CMM/fWnOOcAnPn5f0xb5DYaKSWtwVbmx9UcijGWiHNAvchiidSFdSwiFxEBuPNOVST4jW9kPcbxKHOWJT6/v7mOhT1Q2jc4Ja4sGJWdBRAMJkTE1q5Ft65OiQgg3W5VfFhfT4U/QrlTfeen2uq54l//B3p6ku6so4iJzCmfo25EZ589qfNKdPIF7v2redjsBXD33WplY+Ok3tMwlnneedxywS085mzFBlT2RuGf/gluvXW6D21KyWtMRFsMbwFHgCeBvUCvlNLKpWwF6vXv9UALgF4fACpTl4/aZ7zl6Y7jWiHEFiHElq4cbsy5YP0n/eTST1LbD73zZqubxQRFZJdvF+v+61k+vakHbryRwt/9gSuvLqZyXwdcfXX24wj7iMQiNMfG1lc0eZro0IszWSLBaJCaXH3mVnfY66+Hf/3XybmyNFZ/qUA0gL+hiqYAlHQHpsSVBUpEBgYHiJfqL6mvj81t6voVHdbuIIcD1q0jUijY8YElUFKCrK+ntncocdP/6IFCzt18GP74x4Q7q8ypCxiLi0fMq8/EWY1ncWbjmayYtQIefhh+8YtJnVeVuyrxkPPzs91840fnQXu7cj+eZK6WfLNh9Qb6P3YBs78J7a88BTfdpKzXk4i8ioiUclhKuQpoQFkOS/L5eRmO4w4p5Rop5ZrqY+QTtv6TnjPnHOpDdtpKJJx++oRF5NaXb+WqdwSDH3g//J//A8D2dfN56uyGnGIsVo1I/ZB2/6TcgBs9jfQ5YbjIOa6IxOIxQkMhqvvTt4Efw6WXwlNPwY9/fFQCAiT6SwUiAbpme7BLKHl315SKiEQSLlI3+cHeHt7qVGNsy7qCiZtBr4iy/vOSW/5aPcXH6mZRF4RyffxLe1SngPiTTxKIBnDanTgLnMoSyTEeAqpw8cW/eVH19fJ6x+8ckIVKVyXdoW4e3fUoe3v2qvTeuroJx1cM2RFC8KtP/pp/u/o+FlUtnu7DmRamJDtLStkLbATeD5QLIaxHswZAlwXTBjQC6PUewJe6fNQ+4y2fEixLpNpdTVPYya7CoArAvvMODAzk9B5dA13c8/Y91MVcOBYvS9yUmzxNbPUOqQFR1jyJcbBEZFZEtzwZZYkgoL+ydFx3Vl9UpbUm2sBnE1mHA84776gFBEZaIu21quOpvTf/HXwtSh16poies97ZvpNYPMaKWSuo9EeROh5xsPcgW+phG6qVemhWBUXDUKdrWpq6VLNE+fRTBEO9ifMiEJiWLJ1KdyVHBo5wyW8vobq4mq+u/eqUH8PJRKW7kqved5Wa/3ISks/srGohRLn+3QV8GNiOEpNP6c0+Bzysf39Ev0avf0aq8XmPAJfr7K25qFnvrwKvAQt1tpcDFXxP33I1D1iZWVWuSiqCg+xx9HNkxTyVtZSlzYjF7VtuJxKLUBaVI55Ym8qaeK1EZ/bszBxgtwoNK3UD3tQOrXUlddiFnd7yonEtESsQXL+7U1khWUbpHktSYyIHqlNagkyhJQLQzyA4nRzu2APAXy35KxqCEKpR3+XBgGqIaWXMBauU+NQGVC1PXXsfMQH2bh8dLz+R7OAbDE7IEjlWnNN8DqtqV3HHJXew46s7WDPbWCCG/JFPS6QO2CiEeAd1w39SSvkn4Abgm0KIPaiYxy/19r8EKvXybwI3AkgptwG/B94DHge+qt1kMeA64AmUOP1ebzslWO6sikE7BYMxOkvgxdn6aT4Hl1YkFuG2127j0rkXYguFR9xsmsubeb1U5yRkEZHDA4cBKAlG1FNvykhZu81OQ1kDh0tt41oiwWiQpUeg6dm34Nprsx73sSTVndXhiCZmr0+1iFjB9Z6uQzR7mllbsYLKMPgrVRMrK/Oqva+doeEheirUgVb7oyAlJQfb6Tr//QAsfONQsnJ/gu6sY8UFCy7gzS+9yYbTNmTt12UwHC25RfwmgZTyHeDUNMv3oeIjo5dHgE+P8143AzenWf5n4M9HfbCTwBfyUeYso7BLiUlnCey1B2DBgpxE5NFdj3Jk4AjfWr4BeHykJeJp4mA5xJ0ObFlEpDfSS6mjFFuPP+3Nd075HFrcOzl9R/qhS8FokH/YBMMuJwVf/3rW4z6WFBUUUWgrVIH1iJ8D1Q4qDk1tdhYkR+SGfB2sa7iU5gFljR0pL6SBZGt+iaStr42uCjVkqsIXhsOHoa+PukuvgLYA3xss4/DH7lIfEAiYbCjDjMdUrE8SX9in+hTpJ/yg16XiE6efnnV+B0BHv3IvrXDqm8woEYnbYKCpLmvdiT/iV1lC48zgWFCxgF2OPhVfiUTGrI/t2sEVW+HIVX81ZTdvCyEEniIPgUgAf9hPW61uXzvFlkggEmCouAhbf4gz6s9gdlC5qVpKVbJBaqv3Q4FDdBZDHCjz9cHu3WrFwoVw/vmUv/o2i0t0y4tpskQMhqnEiMgk8YV9qs2HFhFRW6dEpLlZxR/i8Yz7W7GIkrDukTVKRAC6G7xZ3Vn+sF9VOmcQkT2O8WtFGm//NYN26L/uSxk/J1+UOcsIDgbpjfTSNVt/B1MkIvO883AVuPjCw1/g4HAPZVFY17AOr0/lO+9zq2LSg70HmedVrVkOBQ7RM9zH4RJwH0kpLl20CD78YdUN+OWX1bJpiokYDFOJEZFJ4gvpSX76xuyob1IiUlOjWopn6FUFKiuq0FaIo19HxFNuNlZA/FCtG/buHduiPIVcLJFErchoEfH5mPPwc9y5GoqbpqfK1uPUlkjET3ezjiVYtSh5pqakhlc3vEqTp4mdQ52UDQpOrTs1UWi43amE/kDvAdY3rQeUoPRGemkrBUdHl7JECguhqUkVJdrtKgU6FlNZekZEDDMcIyKTpDvUnXRnFRbiqZuTFBFQvvIMBKNBSp2lCKu/UkoqqBUQ31kpYWhI9eMZh2wjZRdULKBzvILDPXuwDcd5Yn4yU2qq8RR5VEwk7Gfn2cvUTPNTTpmyz18xawUvX/My85rex1xRQVFBEbS1MVBkZ+9wF/2D/fjCPpZULaHaXc2hwCH8YT+d5XZs7e1KRObPVwWFpaVw1llqrO4tt6gPOIka8RlOToyITJIRMZHaWho8jXT0dzBcreMKWUSkb7BvxPS70U+sTZ4m3izTbqgMLi1/2E9loW6xkUZE5nvn02HVrGkR+fHmH/PNJ74JLSo9uNWjWsdPB6mWSJm7QhUzTnG+faG9kKUXfJaKNh/85S/Q2oq/0k1bsC0RVJ9TPocmTxOHgofojfbiq3CpeSO7dql4iMVdd6nOuzfcoE/QWCKGmY0RkUkwNDxEMBpUMZHDh6GmhoayBuIyTlep/kpzsEQyicia2Wt4SG5XLzKJSMRP/dD40wBLnaXYqmcRFyTcWXe+cSe3b7md2KEDAPRWlWAT0/OnUOYswxf20T/Yn2gjMi1cf70Sg698BfbuZaC6nLa+tkR6b7OnWYmItkQClSXJhpuLFiXfZ/58ePpp+NWvYO1aeP/7p+d8DIYpwojIJOgJq864IywRa0iNa0htlIMlUuooHbdd+BUrrqDTOUTEUzKuiAwODxIaClEb1bUA4wSk51YvpLdUTTjsH+xne/d2IrEIR3a8TtRhZ9g7fU/LHqcnMRXQaoU+LTid8LOfqRjUW28xVDuL/sF+3j3yLpBiiQQO4Y/46Z+lv7PBwZGWCChL6sor4dVX1aRJg2EGY0RkEliFhonsrNpa6ktVn6UDIqD84xOxRJxO9ZPCmtlrWFCxgL3V9nHTfHsjvUD6liepJILrnZ280fFGch743vfwVbooK5pGESnyIFGptNNqiQCcfz5cfrn6XQ/3eqnlJRx2BzUlNTR5mugf7Ge/fz+RmpTvOtUSMRhOMoyITAKrb1aV06umAaZaIv3tqn1IjoH18WoJhBB8ZsVneLUkwPCO7Wnfwx9WGWBVYR1DyCAih9xDDLe3saVdtWSpdldDSwuHvY5pC6pDsmodptkSsbjlFli4ELnuDECJSLOnGZuwJVKvW4ItDNWmNKocbYkYDCcRRkQmgdU3qyZsV/UgtbVUuFRmTyJDK5s7K9pHmaMsY0HaFSuvYGcl2DsPJ91eKfgjSkS8IV2TksUSiXe08Vr7azSWNXLJoksoORKg3SOmVURSP7u8qHzajiNBnSrwdH1CNU/whX00l6viQWuKIZBozojbPanBUQbDTMGIyCSw3FlVQV2/UVuLEIKGsoakiGQYAgXZLRGAJVVLiC2Yq15YldEpWO4sT78+jgwi0lkC9i4fW1peZW39WtbXn0ltMM5WR2B6LZEUV9q0u7NSsNyTAHM8c4BkESiAu7JGpe8uWAA289/IcPJi/vongeXO8vaqimarOG6EiGSwRIbjwwwMDSRjIhnSQJefrRoeH/nLQ2PWWe6s4r6oKngrKRmzDSTTfG2xYQJt+1g7ey3nFC2mQML+kpiyiKaJ486dpXEVuhLHY1ki1cXVOO0qdlVeVA7Llql0XoPhJMaIyCTwhX1q8FC3rkofLSK1tSpWMs5Aqf5B1aG31JHZEgH40EVf5oVGcN/6H6qlRgqWO8sdDCkrZJz6Cq/Ly0CFEpjafhW0n9uvMrpaPNNXaAjHryUCJOJcc8rnAGATNho9qteZt8gLjz2mCgsNhpMYIyKTwGp5IixrQ1epN5Q20NbXRnzWLJX62dubdv++QTUIKhdLpLl8Dv96UQklXQGVgpqC5c5yBgay9psSjcoVM6dXiYhoVcOsWsumV0Sszy4qKFLV4scR9WXKpZUaC7FcWl6XF8rLVUzEYDiJMSIyCbrD3cn03pKShBupoayBWDxGsFzfDMdxaVnNF3MRESEEfetOZfOKcjXTPCXA7g/7cRW4sI/TBn4EK1cyLOAj/grlitEi0jLNImK5s46LoPoorLiIZYlAUkSOx+M1GKYDIyKTwBfSLU8OHx7RLNByfxy2OoiMIyLWSNrSAjf092dtjXFKzSnc8MEh1R/L6smEcmdl6puVStPspWyrhg8c1vUoLS0MFznxu0a6lKYa67OPp3iIxYpZK6hyVzG7NJl91VSmLZHj8HgNhunAiMgkSLSBP3gQ6pNZPLlWrVuWiHdIzwTLIiIrZ63k+aoBBi67GG6+Ge68E8jewTeVBRULeK0eFu8PqlhNayu2pmbuuPQOPrn0k1nPOV+UOEoQiOMuHgJw3enXsedre7Db7Illp80+jeLC4oSry2A42TEiMgl8IR+VRRWwdSssX55YbonIAacOgGcREU9EB96ziUjNSgA2fe8qOPdc2LAB/u7vCA704HXmZomsa1jH9rmluAIDsH8/tLQgGhrYcNqGab2B24SNUmfpcflkX2ArGGOlXbroUrq+3WXcWQaDxojIBJFS0hPuYV7IqeITK1Yk1lUXV1NoK2Sv6FVzJcZzZ+nAuscaNJhFRFbMUp/xRngfPPqoahb44x/ziT/uZLYoU+3is4jI/Ir5/Oi7z6oXr72mYiLHyejWupK6ES6j4xkhBK5CV/YNDYaThLzNWJ+pBKIBhuUwC9q1AqRYIjZho76snpaBNqiuzmqJFId1kWAWESlzltHsaVbNAAsK4Cc/gf37ufyZRzl4vr6EuUwDXLlS9ejavBna2xP9oaab/7nif8yTvcFwgmIskQlitTxpatFZUikiAiqjJ1vBoRVYLw7lJiKgguvvHn43ueDb36ZiIM7HH92nXuciIoWFsGqVGvw0PHzcWCILKxdSXVw93YdhMBgmgRGRCWJVq9cc6FZ9lkbdvBvKGjgUOJRRRILRIA67g8J+Ncs7FxFZOWslO307icZUlXz8rDN5uQHOeEo3Z8x1Lvnpp8M+LTzHiSViMBhOXIyI5IqUEInQ0a9mX1TsbRsRD7FYXr2c/f79RKvKM8ZERgykymGE6sqalcTiMXZ07wAgEA3ywzPBFtfB+VxFZO3a5O/HiSViMBhOXIyI5IKUsG4dXH8973W9h4iDe/fBtCKyvnk9EklL0ZASkTStT7JNNUzHylkqQ8sakuSP+Hl4CQQbdUvyyYiIsUQMBsNRkjcREUI0CiE2CiHeE0JsE0J8XS+/SQjRJoR4S/9cnLLPd4QQe4QQO4UQF6Qsv1Av2yOEuDFl+VwhxCt6+e+EEI48nYyaGfHAA+xsf5ezYnWIcDitiJxRfwaFtkJ22HsgEoG+vjHbjJhqWFgIRdnbfSyqXEShrTARF+mN9BK3wY5vfV6NYM1VRBYtUpaP2w3e4y+t1mAwnFjk0xKJAd+SUi4D1gFfFUIs0+tulVKu0j9/BtDrLgeWAxcCPxNC2IUQduA24CJgGXBFyvv8m36vBYAfuCZvZ3P55eD3U7bpFc4P6yr1UUF1UN1f19av5fW4aiuSzqU1whLxeMZtnJhKob2QZdXLkpaI7uAb+fhH4aWXVEpxLthsyhppbs7pcw0GgyETeRMRKWWHlPIN/XsfsB3IVOZ7GXC/lDIqpdwP7AFO1z97pJT7pJSDwP3AZUIIAZwLPKD3vxf4eH7OBvjIR5BeL2e+cJDTe3TTvWXL0m56dtPZbB7ar16MIyLZZomkY2XNSt7qfAspZXIg1WSK9H72M/jVrya+n8FgMIxiSmIiQog5wKnAK3rRdUKId4QQdwkhrLtgPdCSslurXjbe8kqgV0oZG7U83edfK4TYIoTY0tXVNbmTcDgIfvR8LtkeZ9nBEMyZA6WlaTdd37yedreeNphGRPqifTk1XxzNuvp1dPR3cDBwMGGJTKq+YtEiMwfDYDAcE/IuIkKIEuC/gW9IKYPA7cB8YBXQAfx7vo9BSnmHlHKNlHJNdfXk6xHePXcFpYPQ9NxbaeMhFmc1nsURqwljmgmHwWgwp1kiozm7+WwAnjvwXKIN/PHYc8pgMJw85FVEhBCFKAH5tZTyQQAp5WEp5bCUMg78AuWuAmgDUnNOG/Sy8Zb7gHIhRMGo5Xlj01wbncUgpMwoIp4iD3Xz34fP44DnnhuzfkSK7wREZPms5VS4Knj+4PP4I34KbAUUFxZn39FgMBjyRD6zswTwS2C7lPKWlOV1KZt9Atiqf38EuFwI4RRCzAUWAq8CrwELdSaWAxV8f0RKKYGNwKf0/p8DHs7X+QBs7dnOY6v1CNo0QfVU1s85hweWDCMffRRCocTyWDxGaCg0KRGxCRvrm9bz/KHn8Yf9eIu8CBMcNxgM00g+LZGzgKuAc0el8/5ACPGuEOId4EPA3wFIKbcBvwfeAx4HvqotlhhwHfAEKjj/e70twA3AN4UQe1Axkl/m8XzYdmQbr128ChYsgLPPzrjt+ub13L9kGBEKwZ//nFg+kdG46Ti7+Wz29Ozhve73TL8pg8Ew7eStAaOU8gUg3WPyn9Mss/a5Gbg5zfI/p9tPSrmPpDssrwzHh9nRvYMPn/41uPGHWbdf37Se/9UMofJi3H/4A3xKGUyJqYaFJaqGZBIiAvDCoRdYM3vNBM/CYDAYji2mYj1H9vr3Eh2Osqw6fVrvaGpKamj0NvPi2hrVvl27tKzmixWxQlXNPkERWVW7ilJHKXEZPy5ncBgMhpMLIyI58l7Xe4AKbufK2vq13LcwDAMD8PjjQMpUw0H91U9QRApsBZzVdJZ6D5OZZTAYphkjIjmy7YgKw+RqiQCsqVvDbys7iFdVwh/+AKRONdQbTVBEQBUzApQ7TUzEYDBML0ZEcmRb1zaaPc2UOEpy3mdt/VqG7dB+3ulqhsfu3YmphmU5jsZNxzlzzgGMJWIwGKYfIyI5sq1r24SsEIDT6lRV+EOfWKoaHn70o0SOtANQEh5WG01CRNbMXsPqutWcXj8lOQUGg8EwLmY8bo7ccNYNqrZjAniKPCyqXMTT7OO6hx6Cc8/lg9/8KR9ZDt43HtQbTVxEHHYHr1/7+oT3MxgMhmONEZEc+czKz0xqv7Wz1/LsgWfhf50Fd99Nw2c/yxNvgnQ9A5/+NMybd2wP1GAwGKYQ487KM2tmr6Gtr42Ovg74zGe4+/uf4BNXFSJ8Pvj978GRnxEoBoPBMBUYEckza2erSYJb2rcA8MrqWbx0ihdcruk8LIPBYDgmGBHJM6tqV2ETtoSIJKYaGgwGwwzAiEieKXYUs7x6OVs6lIgkphoaDAbDDMCIyBSwZvYaXm17lWgsmpxqaDAYDDMAIyJTwKeXfZruUDdXP3Q1gUjAWCIGg2HGYERkCrho4UX84Pwf8Pttv+ftw28bETEYDDMGIyJTxN+f+fd8/YyvA5jAusFgmDGYYsMpQgjBLRfcQoWrgvPnnT/dh2MwGAzHBCMiU4hN2Pinc/5pug/DYDAYjhnGnWUwGAyGSWNExGAwGAyTxoiIwWAwGCaNERGDwWAwTBojIgaDwWCYNEZEDAaDwTBpjIgYDAaDYdIYETEYDAbDpBFSyuk+hilFCNEFHJzk7lVA9zE8nOnEnMvxyUw6F5hZ53Oyn0uzlLJ69MKTTkSOBiHEFinlmuk+jmOBOZfjk5l0LjCzzsecS3qMO8tgMBgMk8aIiMFgMBgmjRGRiXHHdB/AMcScy/HJTDoXmFnnY84lDSYmYjAYDIZJYywRg8FgMEwaIyIGg8FgmDRGRHJACHGhEGKnEGKPEOLG6T6eiSKEaBRCbBRCvCeE2CaE+LpeXiGEeFIIsVv/653uY80FIYRdCPGmEOJP+vVcIcQr+vr8TgjhmO5jzBUhRLkQ4gEhxA4hxHYhxPtP4Ovyd/rva6sQ4rdCiKIT5doIIe4SQhwRQmxNWZb2OgjFT/U5vSOEWD19Rz6Wcc7lh/pv7B0hxB+FEOUp676jz2WnEOKCiX6eEZEsCCHswG3ARcAy4AohxLLpPaoJEwO+JaVcBqwDvqrP4UbgaSnlQuBp/fpE4OvA9pTX/wbcKqVcAPiBa6blqCbHT4DHpZRLgPehzuuEuy5CiHrgemCNlHIFYAcu58S5NvcAF45aNt51uAhYqH+uBW6fomPMlXsYey5PAiuklKcAu4DvAOj7wOXAcr3Pz/Q9L2eMiGTndGCPlHKflHIQuB+4bJqPaUJIKTuklG/o3/tQN6p61Hncqze7F/j49Bxh7gghGoCPAnfq1wI4F3hAb3JCnAeAEMIDnA38EkBKOSil7OUEvC6aAsAlhCgA3EAHJ8i1kVI+D/SMWjzedbgMuE8qNgPlQoi6qTnS7KQ7FynlX6SUMf1yM9Cgf78MuF9KGZVS7gf2oO55OWNEJDv1QEvK61a97IRECDEHOBV4BaiRUnboVZ1AzTQd1kT4MfC/gbh+XQn0pvwHOZGuz1ygC7hbu+fuFEIUcwJeFyllG/Aj4BBKPALA65y41wbGvw4n+j3hb4DH9O9HfS5GRE4ihBAlwH8D35BSBlPXSZXrfVznewshLgGOSClfn+5jOUYUAKuB26WUpwIDjHJdnQjXBUDHCy5DCeNsoJixLpUTlhPlOmRDCPFdlHv718fqPY2IZKcNaEx53aCXnVAIIQpRAvJrKeWDevFhywzX/x6ZruPLkbOAjwkhDqDciueiYgrl2oUCJ9b1aQVapZSv6NcPoETlRLsuAOcD+6WUXVLKIeBB1PU6Ua8NjH8dTsh7ghDi88AlwGdlskDwqM/FiEh2XgMW6iwTByoI9cg0H9OE0HGDXwLbpZS3pKx6BPic/v1zwMNTfWwTQUr5HSllg5RyDuo6PCOl/CywEfiU3uy4Pw8LKWUn0CKEWKwXnQe8xwl2XTSHgHVCCLf+e7PO5YS8NprxrsMjwNU6S2sdEEhxex2XCCEuRLmBPyalDKWsegS4XAjhFELMRSULvDqhN5dSmp8sP8DFqIyGvcB3p/t4JnH8H0CZ4u8Ab+mfi1HxhKeB3cBTQMV0H+sEzumDwJ/07/P0H/4e4A+Ac7qPbwLnsQrYoq/NQ4D3RL0uwPeBHcBW4FeA80S5NsBvUbGcIZSFeM141wEQqIzNvcC7qIy0aT+HLOeyBxX7sP7//7+U7b+rz2UncNFEP8+0PTEYDAbDpDHuLIPBYDBMGiMiBoPBYJg0RkQMBoPBMGmMiBgMBoNh0hgRMRgMBsOkMSJiMEwhQoibhBB/P93HYTAcK4yIGAwGg2HSGBExGPKMEOK7QohdQogXgMV62QYhxGtCiLeFEP+tK71LhRD7dYsahBBl1mshxPVCzYN5Rwhx/7SekMGQghERgyGPCCFOQ7VoWYXqErBWr3pQSrlWSmnNELlGqjb9z6Ja3aP3e1CqXlQ3AqdKNQ/ib6fwFAyGjBgRMRjyy3rgj1LKkFSdk62+ayuEEJuEEO8Cn0UNBQI1J+UL+vcvAHfr398Bfi2EuBLVhdVgOC4wImIwTA/3ANdJKVeiek4VAUgpXwTmCCE+CNillNaI04+i+jWtBl5L6YxrMEwrRkQMhvzyPPBxIYRLCFEKXKqXlwIdOv7x2VH73Af8Bm2FCCFsQKOUciNwA+ABSqbi4A2GbJgGjAZDntGDgD6HmqRhEAoAAACMSURBVEdxCHgDNYDqf6MmG74ClEopP6+3rwX2A3VSyl4tNBtR4iGA/5JS/t+pPg+DIR1GRAyG4wwhxKeAy6SUV033sRgM2TB+VYPhOEII8R/ARahMLoPhuMdYIgaDwWCYNCawbjAYDIZJY0TEYDAYDJPGiIjBYDAYJo0REYPBYDBMGiMiBoPBYJg0/z8Wnf07I3NyAgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for v in arima_model:\n",
    "    df = pd.read_csv(r\"{}/Model/{}/{}.csv\".format(path, v[0], v[1]))\n",
    "    for x in df.columns:\n",
    "        if len(df[x].unique()) == 1:\n",
    "            df.drop(x, 1, inplace=True)\n",
    "        elif x != v[0]:\n",
    "            df[x] = MinMaxScaler().fit_transform(df[[x]])\n",
    "    p, q = v[2], v[3]\n",
    "\n",
    "    value = df[v[0]].values # 目標變數\n",
    "    length = int(len(value) * 0.8) # 訓練集筆數\n",
    "    train1 = list(value[0:length]) # 訓練集\n",
    "    test1 = list(value[length:]) # 測試集\n",
    "    predictions = []\n",
    "\n",
    "\n",
    "    for i in range(len(test1)):\n",
    "        try:\n",
    "            model = ARIMA(train1, order=(p, 1, q))\n",
    "            model_fit = model.fit(disp=0)\n",
    "        except:\n",
    "            model = ARIMA(train1, order=(p, 1, 1))\n",
    "            model_fit = model.fit(disp=0)\n",
    "        pred = model_fit.forecast()[0]\n",
    "        predictions.append(pred[0])\n",
    "        train1.append(test1[i])\n",
    "    predictions = [p if p>=0 else 0 for p in predictions]\n",
    "    print(f'{v[1]} - {v[0]}')\n",
    "    print(f'Test Data : {test1}')\n",
    "    print(f'Prediction : {predictions}')\n",
    "    print(f'MAE : {mean_absolute_error(test1, predictions)}')\n",
    "    print(f'MSE : {mean_squared_error(test1, predictions)}')\n",
    "    plt.plot(test1[:120], 'g-', label='real')\n",
    "    plt.plot(predictions[:120], 'r-', label='predict')\n",
    "    plt.xlabel('days')\n",
    "    plt.ylabel(v[0])\n",
    "    plt.title(f'{v[1]} - {v[0]}')\n",
    "    plt.legend()\n",
    "    plt.savefig(r\"{}/Model Result/{}_{}.png\".format(image_path, v[1], v[0]), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxKpUnaHM_iU"
   },
   "source": [
    "### 將單特徵之LSTM的最佳結果繪製成圖表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1643305315263,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "CUY0y1HuNAOS"
   },
   "outputs": [],
   "source": [
    "def buildTrain(train, target, pastDay=7, futureDay=1):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureDay-pastDay):\n",
    "        X_train.append(np.array(train.iloc[i:i+pastDay][[target]]))\n",
    "        Y_train.append(np.array(train.iloc[i+pastDay:i+pastDay+futureDay][target]))\n",
    "    return np.array(X_train), np.array(Y_train)\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    np.random.seed(10)\n",
    "    randomList = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randomList)\n",
    "    return X[randomList], Y[randomList]\n",
    "\n",
    "def splitData(X, Y, rate):\n",
    "    X_train = X[int(X.shape[0]*rate):]\n",
    "    Y_train = Y[int(Y.shape[0]*rate):]\n",
    "    X_val = X[:int(X.shape[0]*rate)]\n",
    "    Y_val = Y[:int(Y.shape[0]*rate)]\n",
    "    return X_train, Y_train, X_val, Y_val\n",
    "\n",
    "def buildOneToOneModel(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, input_length=shape[1], input_dim=shape[2], return_sequences=False))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(8))\n",
    "    model.add(Dense(4))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1643305315705,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "inK_6JctNCOX"
   },
   "outputs": [],
   "source": [
    "single_lstm_model = [('new_deaths', 'United Kingdom', 7, 200), ('icu_patients', 'United Kingdom', 7, 100), ('icu_patients', 'Canada', 5, 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 28402,
     "status": "ok",
     "timestamp": 1643305352452,
     "user": {
      "displayName": "派森嗨爾波",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7kfIFwq07Eh6g-eJC3S8T7Dghfw5_XM9Mb7D=s64",
      "userId": "13170033664621182072"
     },
     "user_tz": -480
    },
    "id": "hwZL5XVuNEG0",
    "outputId": "960efd86-aa64-4f27-9ee6-51c95fb4ced0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 10)                480       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,385\n",
      "Trainable params: 181,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 179139.4219 - val_loss: 188556.9062\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 178872.7656 - val_loss: 188192.2812\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 178555.5000 - val_loss: 187720.3750\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 178144.9844 - val_loss: 187092.2656\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 177598.7344 - val_loss: 186253.3281\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 176869.4375 - val_loss: 185139.4688\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 175901.8125 - val_loss: 183674.9844\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 174630.6562 - val_loss: 181768.6719\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 172978.0781 - val_loss: 179307.8438\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 170848.8125 - val_loss: 176148.7500\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 168123.6719 - val_loss: 172108.0312\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 164652.8438 - val_loss: 166985.4219\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 160270.2344 - val_loss: 160646.3438\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 154871.0938 - val_loss: 153089.5312\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 148478.5469 - val_loss: 144415.2969\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 141199.3125 - val_loss: 134830.6719\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 133256.7812 - val_loss: 124822.4453\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 125152.0312 - val_loss: 115419.1328\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 117888.6875 - val_loss: 108509.1719\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 113244.2812 - val_loss: 106816.4375\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 113651.0078 - val_loss: 110991.9219\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 119195.9062 - val_loss: 114756.1719\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 123296.4219 - val_loss: 113914.0000\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 122148.6406 - val_loss: 110291.4453\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 117886.8047 - val_loss: 106706.8281\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 113460.4766 - val_loss: 104733.9844\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 110540.2266 - val_loss: 104557.8125\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 109416.1250 - val_loss: 105562.0781\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 109568.8203 - val_loss: 106957.4453\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 110275.2344 - val_loss: 108131.5078\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 110953.1016 - val_loss: 108731.3047\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 111251.5156 - val_loss: 108617.5859\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 111016.1875 - val_loss: 107798.5078\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 110230.6484 - val_loss: 106383.3906\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 108972.7969 - val_loss: 104557.3750\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 107389.8984 - val_loss: 102565.0781\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 105681.4062 - val_loss: 100685.8281\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 104074.9609 - val_loss: 99184.0156\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 102781.3125 - val_loss: 98225.6797\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 101921.7891 - val_loss: 97783.2422\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 101449.1875 - val_loss: 97599.0469\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 101124.3672 - val_loss: 97289.8203\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 100617.5391 - val_loss: 96569.0312\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 99707.0391 - val_loss: 95645.8125\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 98633.4219 - val_loss: 94091.3281\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 97010.1875 - val_loss: 92755.4609\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 95647.1875 - val_loss: 91676.0156\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 94573.9375 - val_loss: 90849.7344\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 93772.6406 - val_loss: 90138.9219\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 93092.6953 - val_loss: 89341.4531\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92328.6641 - val_loss: 88349.1094\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 91375.7266 - val_loss: 87192.6797\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 90265.7656 - val_loss: 85967.9531\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 89093.2266 - val_loss: 84771.2656\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 87953.2266 - val_loss: 83663.3203\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 86909.1562 - val_loss: 82642.6250\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 85971.4453 - val_loss: 81635.1016\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 85089.8125 - val_loss: 80517.6797\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 84172.5547 - val_loss: 79175.7578\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 83134.5078 - val_loss: 77563.3203\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 81949.3906 - val_loss: 75730.1484\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 80680.6875 - val_loss: 73837.7578\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 79514.4531 - val_loss: 72366.1953\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 78928.0938 - val_loss: 71446.2812\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 78561.9609 - val_loss: 70001.3125\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 77267.0234 - val_loss: 68683.9766\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 75821.7031 - val_loss: 67791.5000\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 74624.5469 - val_loss: 67139.7266\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 73632.0312 - val_loss: 66390.4609\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 72632.0312 - val_loss: 65294.2656\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 71457.3750 - val_loss: 63825.8047\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 70098.7656 - val_loss: 62155.5078\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 68672.7266 - val_loss: 60488.9727\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 67276.1250 - val_loss: 58900.7188\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 65861.0312 - val_loss: 57351.8789\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 64324.2109 - val_loss: 55843.7070\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 62684.0195 - val_loss: 54405.1758\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 61023.1055 - val_loss: 53003.6602\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 59363.6953 - val_loss: 51522.6602\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 57651.0547 - val_loss: 49789.2266\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 55804.1875 - val_loss: 47672.8555\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 53790.2656 - val_loss: 45260.6797\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 51699.7031 - val_loss: 42927.5195\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 49677.3398 - val_loss: 41024.1211\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 47594.0820 - val_loss: 39625.2227\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 45410.8398 - val_loss: 38209.7578\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 43295.0430 - val_loss: 35892.8555\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 41091.0000 - val_loss: 33352.0195\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 39071.6211 - val_loss: 31826.2773\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 37050.8477 - val_loss: 31323.0371\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 35269.8672 - val_loss: 29134.2285\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 33632.4258 - val_loss: 27071.3535\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 32560.6426 - val_loss: 28705.2910\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 31990.1758 - val_loss: 25391.6465\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 31542.7441 - val_loss: 27949.3672\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 31366.1934 - val_loss: 25654.3828\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 31197.2012 - val_loss: 27287.8398\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 31055.4238 - val_loss: 25260.1094\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 30781.9590 - val_loss: 28349.2871\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 30836.9727 - val_loss: 24713.3613\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 33176.0742 - val_loss: 39485.4258\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 35892.4766 - val_loss: 25451.4062\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 28568.7012 - val_loss: 26861.1602\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 35298.0938 - val_loss: 25102.4160\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 28058.7773 - val_loss: 36822.5742\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 33251.6602 - val_loss: 24516.1328\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 27699.2617 - val_loss: 23163.9297\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 31627.3945 - val_loss: 21440.6406\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 28115.2578 - val_loss: 28614.8164\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 29314.0352 - val_loss: 28898.3398\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 29443.9375 - val_loss: 21699.8477\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 27410.0352 - val_loss: 21519.4238\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 29578.0391 - val_loss: 21390.6348\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 27661.7129 - val_loss: 25850.0723\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 28030.3750 - val_loss: 27735.2832\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 28767.8594 - val_loss: 23407.4570\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 27417.4668 - val_loss: 21416.8594\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 28207.4922 - val_loss: 21393.0566\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 28276.8984 - val_loss: 22800.5410\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 27429.0430 - val_loss: 25851.9316\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 28160.6309 - val_loss: 25229.6797\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 27971.2188 - val_loss: 22326.4395\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 27469.3086 - val_loss: 21373.3672\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 28035.9707 - val_loss: 21605.4570\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 27691.4062 - val_loss: 23403.7969\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 27445.8789 - val_loss: 25106.3535\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 27835.5879 - val_loss: 23740.4297\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 27450.4668 - val_loss: 21857.3320\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 27424.1074 - val_loss: 21462.0605\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 27637.0703 - val_loss: 22213.7734\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 27266.6855 - val_loss: 23938.1133\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 27412.9473 - val_loss: 23923.5137\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 27396.0684 - val_loss: 22195.4316\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 27173.7070 - val_loss: 21418.7422\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 27374.3809 - val_loss: 21843.8320\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 27166.7520 - val_loss: 23271.3945\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 27183.8457 - val_loss: 23546.2930\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 27212.5801 - val_loss: 22194.7070\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 27046.9727 - val_loss: 21530.2773\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 27174.8281 - val_loss: 22026.0137\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 27025.8965 - val_loss: 23217.8633\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 27076.4434 - val_loss: 23071.5938\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 27042.0918 - val_loss: 21901.3594\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 26990.7480 - val_loss: 21601.5098\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 27043.0039 - val_loss: 22365.7148\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 26948.2070 - val_loss: 23079.6719\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 27018.8281 - val_loss: 22339.7129\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 26929.6211 - val_loss: 21661.8906\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 26982.9141 - val_loss: 22015.6035\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 26919.6074 - val_loss: 22862.2363\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 26949.0859 - val_loss: 22579.2930\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 26911.5234 - val_loss: 21809.6406\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 26921.8984 - val_loss: 21902.8398\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 26899.7109 - val_loss: 22626.7695\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 26900.4648 - val_loss: 22527.0840\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 26886.3340 - val_loss: 21845.7188\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 26883.6934 - val_loss: 21907.5488\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 26869.8066 - val_loss: 22544.4531\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 26867.0859 - val_loss: 22444.2305\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 26851.6797 - val_loss: 21883.9004\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 26851.9883 - val_loss: 22009.2031\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 26833.7012 - val_loss: 22521.2930\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 26836.7676 - val_loss: 22289.7246\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 26816.1875 - val_loss: 21863.9316\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 26822.2988 - val_loss: 22113.7461\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 26800.7383 - val_loss: 22470.8828\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 26806.6152 - val_loss: 22130.1797\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 26786.6738 - val_loss: 21925.7305\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 26790.0117 - val_loss: 22292.3672\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 26775.0078 - val_loss: 22391.1328\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 26772.7461 - val_loss: 22019.1270\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 26764.9297 - val_loss: 22070.5801\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 26756.5605 - val_loss: 22397.3965\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 26754.9434 - val_loss: 22208.7070\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 26743.1367 - val_loss: 22010.4688\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 26743.2207 - val_loss: 22283.1035\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 26732.8867 - val_loss: 22347.1367\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 26729.3242 - val_loss: 22070.6035\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 26724.4688 - val_loss: 22171.2383\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 26716.2227 - val_loss: 22368.3105\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 26714.4395 - val_loss: 22145.4961\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 26706.1934 - val_loss: 22102.8145\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 26702.1426 - val_loss: 22329.0547\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 26697.7422 - val_loss: 22207.1133\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 26690.1621 - val_loss: 22088.3730\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 26687.0234 - val_loss: 22287.4805\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 26680.7559 - val_loss: 22237.4746\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 26674.5527 - val_loss: 22086.8770\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 26670.7988 - val_loss: 22246.4883\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 26664.0898 - val_loss: 22232.0664\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 26658.5352 - val_loss: 22084.2734\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 26654.1543 - val_loss: 22221.4023\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 26647.5254 - val_loss: 22212.9629\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 26641.9219 - val_loss: 22081.2637\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 26637.1602 - val_loss: 22205.6660\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 26630.7422 - val_loss: 22177.5488\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 26624.8027 - val_loss: 22073.0254\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 26619.7734 - val_loss: 22194.9121\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 26613.6387 - val_loss: 22138.4473\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 26607.2812 - val_loss: 22080.3418\n",
      "United Kingdom - new_deaths\n",
      "Test Data : [140.0, 121.0, 90.0, 92.0, 103.0, 45.0, 37.0, 146.0, 108.0, 95.0, 100.0, 94.0, 59.0, 27.0, 170.0, 111.0, 114.0, 114.0, 104.0, 49.0, 42.0, 174.0, 149.0, 142.0, 101.0, 133.0, 61.0, 48.0, 51.0, 207.0, 178.0, 121.0, 120.0, 68.0, 45.0, 210.0, 191.0, 167.0, 147.0, 156.0, 56.0, 62.0, 187.0, 201.0, 159.0, 180.0, 164.0, 61.0, 50.0, 204.0, 168.0, 195.0, 180.0, 129.0, 64.0, 40.0, 177.0, 160.0, 137.0, 128.0, 124.0, 43.0, 40.0, 166.0, 150.0, 124.0, 127.0, 156.0, 38.0, 28.0, 184.0, 136.0, 160.0, 145.0, 148.0, 57.0, 45.0, 223.0, 179.0, 118.0, 181.0, 135.0, 72.0, 40.0, 263.0, 209.0, 166.0, 187.0, 166.0, 74.0, 43.0, 292.0, 217.0, 219.0, 193.0, 155.0, 62.0, 57.0, 263.0, 216.0, 199.0, 145.0, 158.0, 63.0, 47.0, 214.0, 201.0, 200.0, 159.0, 150.0, 61.0, 45.0, 165.0, 149.0, 148.0, 160.0, 131.0, 51.0, 35.0, 161.0, 172.0, 142.0, 146.0, 127.0, 54.0, 46.0, 180.0, 163.0]\n",
      "Prediction : [55.141937255859375, 177.8686981201172, 172.05992126464844, 144.25314331054688, 145.91490173339844, 155.98593139648438, 91.66217803955078, 72.8475570678711, 185.99966430664062, 161.4352264404297, 149.8188018798828, 154.25485229492188, 148.85789489746094, 112.16045379638672, 57.78270721435547, 204.29054260253906, 163.3258514404297, 166.31222534179688, 166.31126403808594, 157.42840576171875, 98.1057357788086, 82.53961181640625, 212.24948120117188, 197.26055908203125, 191.29949951171875, 155.17201232910156, 183.6058349609375, 114.7515869140625, 94.58658599853516, 96.30194854736328, 244.2880859375, 222.47254943847656, 172.58230590820312, 172.22622680664062, 123.0335693359375, 90.80339050292969, 245.20082092285156, 234.0309295654297, 212.88864135742188, 195.53509521484375, 203.6641387939453, 108.17395782470703, 112.7350845336914, 228.8585968017578, 243.18582153320312, 205.72067260742188, 224.78213500976562, 210.37550354003906, 114.49996948242188, 97.59119415283203, 241.37503051757812, 213.67800903320312, 238.02833557128906, 224.366455078125, 179.6026611328125, 118.40876007080078, 82.0048599243164, 214.22085571289062, 206.926025390625, 186.7947235107422, 179.1229248046875, 175.67831420898438, 89.06746673583984, 77.51435852050781, 204.55291748046875, 198.21124267578125, 175.46395874023438, 178.31761169433594, 203.78738403320312, 80.29753875732422, 54.73738098144531, 216.9528045654297, 185.23768615722656, 206.71592712402344, 193.28128051757812, 196.0926055908203, 109.2165298461914, 89.22325897216797, 256.8404235839844, 223.20860290527344, 169.93478393554688, 225.9995880126953, 184.83583068847656, 127.32105255126953, 82.86471557617188, 291.1267395019531, 249.485595703125, 211.78610229492188, 230.8992462158203, 212.0326690673828, 129.13438415527344, 88.10106658935547, 318.5558166503906, 256.3148193359375, 258.95709228515625, 235.64927673339844, 202.28662109375, 115.79691314697266, 107.4666748046875, 296.2366638183594, 255.73313903808594, 241.05776977539062, 193.43624877929688, 205.46336364746094, 116.97631072998047, 93.34928894042969, 249.42298889160156, 242.90745544433594, 242.11077880859375, 205.7335662841797, 198.2682342529297, 114.62523651123047, 89.90217590332031, 205.32711791992188, 197.37210083007812, 196.5892791748047, 207.17137145996094, 181.55697631835938, 101.44149017333984, 70.80664825439453, 198.46368408203125, 217.6461181640625, 190.97564697265625, 194.8089599609375, 178.0918731689453, 105.63432312011719, 90.2072525024414, 218.864990234375]\n",
      "MAE : 78.24357724189758\n",
      "MSE : 7375.345890608527\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for v in single_lstm_model[:1]:\n",
    "    df = pd.read_csv(r\"{}/Model/{}/{}.csv\".format(path, v[0], v[1]))\n",
    "    for x in df.columns:\n",
    "        if len(df[x].unique()) == 1:\n",
    "            df.drop(x, 1, inplace=True)\n",
    "        elif x != v[0]:\n",
    "            df[x] = MinMaxScaler().fit_transform(df[[x]])\n",
    "\n",
    "    train_size = round(len(df) * 0.8)\n",
    "    train = df.iloc[:train_size, :]\n",
    "    test = df.iloc[train_size:, :]\n",
    "\n",
    "    X_train, Y_train = buildTrain(train, v[0], v[2], 1)\n",
    "\n",
    "    ### shuffle the data, and random seed is 10\n",
    "    X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "    ### split training data and validation data\n",
    "    X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "    Y_train = Y_train[:, np.newaxis]\n",
    "    Y_val = Y_val[:, np.newaxis]\n",
    "\n",
    "    model = buildOneToOneModel(X_train.shape)\n",
    "    model.fit(X_train, Y_train, epochs=v[3], batch_size=512, validation_data=(X_val, Y_val))\n",
    "\n",
    "    X_test, Y_test = buildTrain(test, v[0], v[2], 1)\n",
    "\n",
    "    predict_test = model.predict(X_test)\n",
    "\n",
    "    Y_test = Y_test.reshape(-1).tolist()\n",
    "    predict_test = predict_test.reshape(-1).tolist()\n",
    "\n",
    "    print(f'{v[1]} - {v[0]}')\n",
    "    print(f'Test Data : {Y_test}')\n",
    "    print(f'Prediction : {predict_test}')\n",
    "    print(f'MAE : {mean_absolute_error(Y_test, predict_test)}')\n",
    "    print(f'MSE : {mean_squared_error(Y_test, predict_test)}')\n",
    "    plt.plot(Y_test[:120], 'g-', label='real')\n",
    "    plt.plot(predict_test[:120], 'r-', label='predict')\n",
    "    plt.xlabel('days')\n",
    "    plt.ylabel(v[0])\n",
    "    plt.title(f'{v[1]} - {v[0]}')\n",
    "    plt.legend()\n",
    "    # plt.savefig(r\"{}/Model Result/{}_{}.png\".format(image_path, v[1], v[0]), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-3CflJZoUte"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMEYt3lRbn08SUoIgs4BjVh",
   "collapsed_sections": [],
   "mount_file_id": "1ArEho13b-W3KN9kiHm738VLNGQrHLjMH",
   "name": "ALL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 miniforge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
